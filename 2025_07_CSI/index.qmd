---
title: "CSI Presentation 2025"
author: "Wassim Kabalan"
footer: "CSI Presentation 2025"
format:
  revealjs:
    theme: [default, css/custom.scss]
    incremental: false   
    transition: slide
    background-transition: slide
    presentation-size: max-scale
    slide-number: true
    pdfSeparateFragments: true
    template-partials:
      - css/title-slide.html
output: revealjs

code-block-border-left: "#31BAE9"
title-slide-attributes:
  data-background-image: "assets/titles/llst_first_l.webp"
  data-background-size: fill
  data-background-opacity: "0.5"

logo1: '![](assets/Logos/AIM.png){fig-align="center"width=10%} ![](assets/Logos/APC.png){fig-align="center" width=10%} ![](assets/Logos/AstroDeep-2.png){fig-align="center" width=10%} ![](assets/Logos/scipol.jpeg){fig-align="center" width=10%}'


---



## Summary of projects {auto-animate=true}

<br/>

:::{data-id="Projects"}
![Projects](assets/Projects/ALL_Projects.svg){.nostretch fig-align="center" width="70%"}
:::


::: {.notes}


Mon sujet de recherche est à cheval entre deux domaines : **la séparation des composants du CMB** dans le cadre du projet **SciPol**, et **le lentillage gravitationnel faible** avec **AstroDeep**. Je vais vous présenter ces deux projets plus en détail dans les prochaines diapositives.

:::

# CMB Component Separation

## Cosmic Microwave Background - Scipol {auto-animate=true}

<br/>

:::{data-id="Projects"}
![Projects](assets/Projects/Scipol.svg){.nostretch fig-align="center" width="70%"}
:::

::: {.notes}

On commence par la separation de composants pour le fond diffus cosmologique

:::

## Cosmic Microwave Background 
:::: {.columns}

::: {.column width="50%"}

![](assets/CMB/CMB-Planck.png){.nostretch fig-align="center" width="75%"}

::: {.fragment fragment-index=1}
![](assets/CMB/Equal_symbol.svg){.nostretch fig-align="center" width="10%"}

![](assets/CMB/eb-modes.png){.nostretch fig-align="center" width="45%"}
::: 

:::

::: {.column width="50%"}

- The **Cosmic Microwave Background (CMB)** is the afterglow of the Big Bang, providing a snapshot of the early universe.

::: {.fragment fragment-index=1}
- The CMB is polarized, consisting of **E and B modes**. 
- **E modes** are curl-free, generated by density fluctuations.
- **B modes** could be evidence of primordial gravitational waves, indicating cosmic inflation.
- **The tensor-to-scalar** ratio $r$, which is the ratio of the tensor power spectrum to the scalar power spectrum
:::

:::

::::


::: {.notes}

Le fond diffus cosmologique, ou CMB, est le rayonnement fossile du Big Bang, qui nous offre une image de l'univers primitif.

Le CMB est polarisé et se compose de deux types de modes : les **E modes** et les **B modes**.

Les **E modes** ont un champ rotationnel nul et sont générés par les fluctuations de densité dans l'univers. En revanche, les **B modes**, qui possèdent un champ rotationnel, pourraient indiquer la présence d'ondes gravitationnelles primordiales.

Un paramètre clé lié à ces **B modes** est le **rapport tensor-surface** $r$, qui mesure la force relative des ondes gravitationnelles par rapport aux perturbations de densité, ce qui nous aide à mieux comprendre l'inflation cosmique.

:::

## Cosmic Microwave Background 

:::: {.columns}

::: {.column width="50%"}

![](assets/CMB/CMB-Planck.png){.nostretch fig-align="center" width="35%"}

::: {.fragment fragment-index=1}

![](assets/CMB/Plus_symbol.svg){.nostretch fig-align="center" width="10%"}

![](assets/CMB/dust_planck.png){.nostretch fig-align="center" width="35%"}

:::

::: {.fragment fragment-index=2}

![](assets/CMB/Plus_symbol.svg){.nostretch fig-align="center" width="10%"}

![](assets/CMB/synch-planck.png){.nostretch fig-align="center" width="35%"}

:::

:::

::: {.column width="50%"}

- The **Cosmic Microwave Background (CMB)** signal is obscured by various **foregrounds**, making it challenging to detect the true cosmological information.

::: {.fragment fragment-index=1}
- **Dust**: Emission from galactic dust adds significant noise to the CMB, particularly affecting polarization measurements.
:::

::: {.fragment fragment-index=2}
- **Synchrotron Radiation**: Electrons spiraling in the galaxy's magnetic fields produce synchrotron radiation, another major contaminant.
:::

<br />
<br />

::: {.fragment fragment-index=3}

### Component seperation methods

- **Blind Methods**: Like **SMICA** (Spectral Matching Independent Component Analysis)
- **Parametric Methods**: Like **FGbuster** (Foreground Buster)

:::


<br />
<br />




:::

::::

::: {.notes}

Le signal du fond diffus cosmologique, ou CMB, est en réalité obscurci par plusieurs avant-plans, ce qui rend difficile l'extraction des informations cosmologiques réelles.

L'un des contaminants principaux est la poussière galactique. Cette poussière émet du rayonnement qui ajoute un bruit significatif au CMB, affectant particulièrement les mesures de polarisation.

**NEXT**

Un autre contaminant majeur est la radiation synchrotron. Elle est produite par des électrons en spirale dans les champs magnétiques de notre galaxie, ce qui vient encore plus brouiller le signal cosmologique que l'on souhaite observer.

**AFTER**

Pour pouvoir extraire une valeur fiable du rapport $r$, il est crucial de séparer ou de "démixer" ces composants. Le signal du CMB est mêlé à diverses émissions parasites.

 Il existe différentes méthodes pour cela, principalement des méthodes aveugles comme SMICA, qui fonctionnent sans connaissances préalables des avant-plans, et des méthodes paramétriques comme FGbuster, qui reposent sur la modélisation explicite des avant-plans.

Dans cette présentation, nous allons nous concentrer sur les méthodes paramétriques. Celles-ci nous permettent d'utiliser des modèles pour les avant-plans et d'améliorer la précision du processus de séparation.

:::

## CMB Component Separation

:::: {.columns}

::: {.column width="50%"}

::: {.fragment fragment-index=1}

![](assets/CMB/dust_planck.png){.nostretch fig-align="center" width="45%"}

:::

::: {.fragment fragment-index=2}

![](assets/CMB/synch-planck.png){.nostretch fig-align="center" width="45%"}

:::

:::

::: {.column width="50%"}

::: {.fragment fragment-index=1}

**Modified Blackbody SED of Dust:**

$$
\boxed{s_{\mathrm{d}}(\nu) = A_{\mathrm{d}} \cdot \frac{\nu}{\exp\left(\frac{h\nu}{k \color{red}{T_{\mathrm{d}}}}\right) - 1} \cdot \frac{\exp\left(\frac{h\nu_{0}}{k \color{red}{T_{\mathrm{d}}}}\right) - 1}{\nu_{0}} \cdot \left(\frac{\nu}{\nu_{0}}\right)^{\color{blue}{\beta}}}
$$

:::

::: {.fragment fragment-index=2}

**Power Law of Synchrotron Emission:**

$$
\boxed{s_{\text{synch}}(\nu) = \left(\frac{\nu}{\nu_0}\right)^{\color{green}{\beta_{\text{pl}}}}}
$$

:::

:::

::::

::: {.fragment fragment-index=3}

#### Signal Representation

::: {.columns}

::: {.column width="50%"}

$$
\boxed{\mathbf{d} = \mathbf{A} \mathbf{s} + \mathbf{n}}
$$

:::

::: {.column width="50%"}

$$
\boxed{\mathbf{d} = \color{green}{A_{\text{synch}}} \cdot s_{\text{synch}} + \color{blue}{A_{\mathrm{d}}} \cdot s_{\mathrm{d}} + A_{\text{cmb}} \cdot s_{\text{cmb}} + \mathbf{n}}
$$

:::

:::

:::

::: {.fragment fragment-index=4}

#### Likelihood Function

$$
\boxed{
\begin{aligned}
-2 \ln \mathcal{L}_{\text{data}}(\mathbf{s}, \boldsymbol{\beta}) &= \text{const} \\
&\quad + \sum_{p} \left( \mathbf{d}_p - \mathbf{A}_p \mathbf{s}_p \right)^T \mathbf{N}_p^{-1} \left( \mathbf{d}_p - \mathbf{A}_p \mathbf{s}_p \right)
\end{aligned}
}
$$

:::

::: {.r-stack}

::: {.fragment fragment-index=5 .fade-in-then-out}

#### Minimization for Component Separation

$$
\boxed{\mathbf{s} = \left( \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d} \right)^T \left( \mathbf{A}^T \mathbf{N}^{-1} \mathbf{A} \right)^{-1} \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d}}
$$

:::

::: {.fragment fragm:ent-index=6}

#### Minimization for Component Separation

$$
\boxed{\mathcal{L}(\color{blue}{\beta_d}, \color{red}{T_d}, \color{green}{\beta_{\text{pl}}}) = \left( \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d} \right)^T  \left( \mathbf{A}^T \mathbf{N}^{-1} \mathbf{A} \right)^{-1} \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d}}
$$

:::


:::

::: {.notes}

Dans cette diapositive, je vais expliquer comment on modélise la séparation des composantes du CMB à partir des données observées.

Nous avons d'abord **deux composantes principales** qui contaminent le signal CMB pur : 
- **La poussière galactique**, modélisée par une loi de corps noir modifiée. Elle est une des sources d'émission la plus importante à haute fréquence.
- **Le rayonnement synchrotron**, produit par les électrons en spirale dans les champs magnétiques de la galaxie, qui est une source dominante à basse fréquence.

On représente ensuite le signal **d** par une combinaison linéaire des contributions de chaque composant multipliées par leur matrice de mélange respective, plus un bruit **n**. 

L'objectif de la séparation des composantes est de maximiser la vraisemblance de nos données modélisées par rapport aux données observées. Cela se fait par une **minimisation**, représentée par l'équation en bas de la diapositive.

La méthode que nous utilisons ici est **paramétrique**, où chaque composante a un modèle physique avec des paramètres spécifiques comme **$\beta$**, **$T_d$** pour la poussière et **$\beta_{pl}$** pour le synchrotron.

À la fin, nous obtenons la partie CMB de la matrice de mélange, à partir de laquelle nous allons pouvoir estimer le **ratio tenseur-spectral $r$**, un paramètre clé pour contraindre les modèles d'inflation cosmique.

:::

# FURAX Framework

## The FURAX Minimization Process  ![SO](assets/Logos/so.webp){.nostretch fig-valign="center" width="5%"} ![LiteBIRD](assets/Logos/litebird.png){.nostretch fig-valign="center" width="7%"}

We build a modular, physical model of the sky using linear operators, then solve for the components with a direct minimization of the likelihood.

<br/>

:::: {.columns}

::: {.column width="50%"}

### Building the Physical Model

Define each sky component as a `FURAX` operator:

<br/>

```{.python style="font-size: 1.1em;"}
# Define each physical component
cmb = CMBOperator(nu, ...)
dust = DustOperator(nu, temp=params["T_d"], 
                    beta=params["beta_d"], ...)
synchrotron = SynchrotronOperator(nu, 
                beta=params["beta_pl"], ...)

# Combine into a single mixing matrix
A = MixingMatrixOperator(cmb=cmb, dust=dust, 
                        synchrotron=synchrotron)
```

<br/>

**Key Features:**
- **Modular Design**: Each component is independent
- **Physical Models**: Based on emission mechanisms  
- **Differentiable**: Full JAX integration

:::

::: {.column width="50%"}

### Solving for the Signal

The likelihood minimization is solved analytically as:

<br/>

$$\mathbf{\hat{s}} = (\mathbf{A}^T \mathbf{N}^{-1} \mathbf{A})^{-1} \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d}$$

<br/>

In `FURAX`, this is implemented with linear operators for maximum efficiency and differentiability:

<br/>

```{.python style="font-size: 1.1em;"}
# Solve for the best-fit signal 's'
AND = (A.T @ N.I)(d)
s = (A.T @ N.I @ A).I(AND)
```

<br/>

**Advantages:**
- **Direct Solution**: No iterative fitting required
- **GPU Accelerated**: Efficient linear algebra
- **Stable**: Numerically robust implementation

:::

::::

::: {.notes}

FURAX represents a major advancement in CMB component separation methodology. Rather than working with explicit matrices, we build a modular, operator-based model of the sky where each physical component (CMB, dust, synchrotron) is represented as a differentiable linear operator.

The key innovation is the direct analytical solution of the likelihood minimization problem. Instead of iterative fitting, we solve the generalized least squares problem directly using the operator framework. This approach is both more efficient and more stable than traditional methods.

The linear operator implementation ensures that all computations remain differentiable and GPU-accelerated through JAX, enabling gradient-based optimization and large-scale analysis that would be computationally prohibitive with traditional matrix-based approaches.

::: 

## Our Innovation: Data-Driven Adaptive Clustering

We move beyond fixed sky patches by using K-means clustering to define optimal, data-driven regions for foreground modeling.

<br/>

:::: {.columns}

::: {.column width="60%"}

### Traditional Approach
- **Fixed, predefined sky regions**
- Often arbitrary boundaries
- Cannot adapt to local foreground complexity
- Limited by rigid HEALPix resolution constraints

<br/>

### Our Data-Driven Method
- **Spherical K-means clustering**
- Boundaries emerge from the data itself
- Adapts to spatial foreground variability
- Optimizes clustering configuration via grid search

:::

::: {.column width="40%"}

![An example sky partition into clusters optimized by K-means clustering. Each color represents a distinct region sharing common spectral parameters.](assets/CMB/kmeans_patch_layout.png){.nostretch fig-align="center" width="60%"}

:::

::::


## The Objective: Minimizing CMB Variance

We select the best clustering model by finding the one that minimizes the variance in the recovered CMB map.

![Distribution of CMB map variance for different clustering strategies. K-means clustering (blue) achieves the lowest variance, indicating optimal foreground modeling.](assets/CMB/variance_likelihood_distributions.png){.nostretch fig-align="center" width="100%"}

### Grid Search Strategy

**Configuration Space**: $\mathcal{G} = \{K_{\beta_d}\} \times \{K_{T_d}\} \times \{K_{\beta_s}\}$

For each clustering configuration:
1. **Fit spectral parameters** $\boldsymbol{\beta}$ via likelihood maximization
2. **Reconstruct CMB component** using optimal parameters  
3. **Compute variance** $\sigma^2_{\mathrm{CMB}}$ across noise realizations
4. **Select configuration** with minimum variance

$$\sigma^2_{\mathrm{CMB}} = \left\langle \mathrm{Var}_{i} \left[ \hat{s}^{(i)}_{\mathrm{CMB}} \right] \right\rangle_{\text{pixels}}$$



## The Engine: The FURAX Framework

This entire pipeline is implemented in FURAX, a JAX-powered, end-to-end differentiable framework for component separation.

:::: {.columns}

::: {.column width="60%"}

### Key Framework Features

**Modular Architecture**
- Linear operator-based design
- Differentiable spectral models
- GPU-accelerated computation

<br/>

**Scalable Optimization**
- L-BFGS with exact gradients
- Distributed grid search framework
- Millions of likelihood evaluations

<br/>

**End-to-End Pipeline**
- Multi-frequency input maps
- Adaptive clustering selection
- Component reconstruction
- Cosmological parameter inference

<br/>

**Performance**
- 1000+ GPU-hours for full study
- 2M+ component separation runs
- Robust convergence and stability

:::

::: {.column width="40%"}

<br/>

![Complete FURAX analysis pipeline from multi-frequency observations to cosmological parameter constraints. The framework integrates adaptive clustering, operator-based minimization, and distributed grid search.](assets/CMB/FURAX-CS-transparent.png){.nostretch fig-align="center" width="90%"}

:::

::::

::: {.notes}

The FURAX framework is the computational engine that makes this data-driven approach practically feasible. Built on JAX, it provides a fully differentiable, GPU-accelerated environment for parametric component separation at scale.

The framework integrates several key innovations: the modular, operator-based design allows us to build complex mixing matrices without explicit matrix construction; the automatic differentiation provides exact gradients for stable optimization; and the distributed computation framework enables exhaustive exploration of the clustering configuration space.

The pipeline diagram shows the complete workflow: starting from multi-frequency sky maps, the framework performs adaptive clustering, solves for the optimal spectral parameters using our operator-based minimization, reconstructs clean component maps, and finally computes cosmological parameter constraints. This end-to-end approach ensures that the clustering optimization directly targets the final science objectives.

:::

# Results and Validation

## Result 1: Reduced Systematic Residuals

Our adaptive method significantly reduces systematic foreground leakage, leading to cleaner CMB maps.

:::{.columns}


::: {.column width="50%"}

<br/>
<br/>

**Single-Patch Reference**
- Significantly higher residuals
- Large-scale systematic contamination
- **Biased cosmological inference**

**Key Insight**
*Minimizing CMB variance leads to better control of systematic residuals that matter for cosmological parameter estimation.*

:::

:::: {.column width="50%"}


![CMB reconstruction comparison showing residual maps for different spatial modeling approaches. Our K-means clustering method (bottom) achieves substantially cleaner reconstruction compared to traditional single-patch and multi-resolution methods.](assets/CMB/cmb_recon.png){.nostretch fig-align="center" width="100%"}

::::

::::

## Result 2: Tighter and Unbiased Constraints on *r*

### Scientific Impact

:::: {.columns}

::: {.column width="50%"}

<br/>
<br/>

**K-means Clustering (Our Method)**
- **Unbiased**: $\hat{r} = 4.55 \times 10^{-4}$ (true r = 0)
- **Precise**: Tightest credible interval
- **Robust**: Stable across noise realizations

<br/>

**Multi-Resolution Methods**
- Slight positive bias
- Broader uncertainty intervals
- Sensitivity to configuration choices

<br/>

**Single Global Patch**
- **Severely biased**: Significant overestimate of r
- Artificially tight constraints
- Dominated by systematic residuals

<br/>

**Bottom Line**
*Data-driven clustering directly translates to more reliable cosmological science, enabling robust constraints on primordial gravitational waves.*


:::

::: {.column width="50%"}

<br/>

![Tensor-to-scalar ratio likelihood distributions comparing different spatial modeling approaches. K-means clustering (blue) achieves the tightest and most unbiased constraints on r, centered on the true value r=0.](assets/CMB/r_likelihood_distribution.png){.nostretch fig-align="center" width="100%"}

:::

::::



## Summary of projects {auto-animate=true}

:::{data-id="Projects"}
![Projects](assets/Projects/ALL_Projects.svg){.nostretch fig-align="center" width="70%"}
:::

## Large Scale Structure - AstroDeep {auto-animate=true}

:::{data-id="Projects"}

![Projects](assets/Projects/WL.svg){.nostretch fig-align="center" width="70%"}

:::

## The Traditional Approach to Cosmological Inference {style="font-size: 22px;"}


:::{.columns}

:::: {.column width="50%"}

<br/>

::::: {.r-stack}


![](assets/latest/trad_cosmo.svg){fig-align="center" width="100%"}

:::::

::::

:::: {.column width="50%"}


<br/>
<br/>


- **cosmological parameters** (Ω): matter density, dark energy, etc.
- Predict observables: **CMB, galaxies, lensing**
- Extract **summary statistics**: $P(k)$, $C_\ell$ , 2PCF
- Compute **likelihood**: $L(\Omega \vert data)$
- Estimate $\hat{\Omega}$ via **maximization** ($\chi^2$ fitting)


::::

:::

:::::: {.fragment fragment-index=1 .fade-in}


:::{.solutionbox}

::: {.solutionbox-header style="font-size: 20px;"}

**Summary Statistics Based Inference**

:::

::::{.solutionbox-body style="font-size: 18px;"}

- Traditional inference uses **summary statistics** to compress data.
- Power spectrum fitting: $P(k)$, $C_\ell$ 
- It misses complex, non-linear structure in the data

::::

:::

:::::: 


::: {.notes}

"Most of modern cosmological inference pipelines rely on summary statistics — things like the power spectrum P(k)P(k) or angular power spectrum CℓCℓ​."

"These work well under the assumption that most of the information is encoded in second-order statistics — basically, the correlations between pairs of points."

"But this approach ignores all the higher-order structure — the full non-linear complexity that emerges in the formation of cosmic structure."

Even higher-order statistics (like bispectrum or 3-point correlations) still reduce the data, and fail to capture the entire structure or allow for fully Bayesian inference over the field.

"And that’s the motivation for moving beyond summary statistics..."

:::

---

## The Traditional Approach to Cosmological Inference {style="font-size: 22px;"}


<br/>

::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-out}
![Credit: Natalia Porqueres](assets/latest/summ_stat.png){fig-align="center" width="80%" text-align="right"}
::::::
:::::: {.fragment fragment-index=1 .fade-in}
![[Jeffrey et al. (2024)](https://arxiv.org/abs/2403.02314)](assets/latest/bad_posterior.png){fig-align="center" width="50%"}
::::::
:::

:::{.solutionbox}

:::: {.solutionbox-body style="font-size: 26px;"}
 - **Summary statistics (e.g. P(k)) discard the non-Gaussian features.**
 - **Gradient-based curve fitting does not recover the true posterior shape.**
::::

:::

#  How to maximize the information gain? 

---

## From Summary Statistics to Likelihood Free Inference {style="font-size: 19px;"}

<br/>

::: {.r-stack}
![](assets/latest/likelihood_free_cosmo.svg){fig-align="center" width="60%"}
:::


:::::: {.fragment fragment-index=1 .fade-in}

### Bayes’ Theorem

$$
p(\theta \mid x_0) \propto p(x_0 \mid \theta) \cdot p(\theta)
$$

* **Prior**: Encodes our assumptions about parameters $\theta$
* **Likelihood**: How likely the data $x_0$ is given $\theta$
* **Posterior**: What we want to learn — how data updates our belief about $\theta$

::::::

:::::: {.fragment fragment-index=2 .fade-in}



::: {.solutionbox}

:::: {.solutionbox-body style="font-size: 24px;"}

* **Simulators become the bridge between cosmological parameters and observables.**
* **How to use simulators allow us to go beyond summary statistics?**
::::

:::

::::::

---

## From Summary Statistics to Likelihood Free Inference {style="font-size: 19px;"}


<br/>

::: {.columns}


:::: {.column width="60%"}

#### **Implicit Inference**

* Treats the simulator as a **black box** — we only require the ability to simulate $(\theta, x)$ pairs.

* No need for an explicit likelihood — instead, use **simulation-based inference** (SBI) techniques

* Often relies on **compression** to summary statistics $t = f_\phi(x)$, then approximates $p(\theta \mid t)$.


<br/>

#### **Explicit Inference**

* Requires a **differentiable forward model** or simulator.

* Treat the simulator as a probabilistic model and perform inference over the joint posterior $p(x \mid \theta, z)$

* Computationally demanding — but provides **exact control over the statistical model**.

::::

:::: {.column width="40%"}



::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-out}
![Implicit Inference](assets/latest/likelihood_free_SBI.svg){fig-align="center" width="80%"}
::::::

:::::: {.fragment fragment-index=1 .fade-in}

![Explicit Inference](assets/latest/likelihood_free_ffi.svg){fig-align="center" width="90%"}

::::


:::

::::

:::

---



## Implicit inference {style="font-size: 20px;"}

#### Simulation-Based Inference Loop 

 - Sample parameters $\theta_i \sim p(\theta)$
 - Run simulator $x_i = p(x \vert \theta_i)$
 - Compress observables $t_i = f_\phi(x_i)$
 - Train a **density estimator** $\hat{p}_\Phi(\theta \mid f_\phi(x))$

::: {.r-stack}

![](assets/latest/illu_compressor.svg){fig-align="center" width="70%"}

:::::: {.fragment fragment-index=2 .fade-in-then-out}

![](assets/latest/neural_comp.png){fig-align="center" width="40%"}
::::::

:::::: {.fragment fragment-index=3 .fade-in}

![](assets/latest/normal_flows.png){fig-align="center" width="60%"}
::::::

:::

:::::: {.fragment fragment-index=1 .fade-in}

::: {.solutionbox}

:::: {.solutionbox-body style="font-size: 19px;"}

* **Neural Summarisation (Zeghal & Lanzieri et al 2025).**
* **Normalizing Flows (Zeghal et al. 2022).**
* **✅ Works with non-differentiable or stochastic simulators**
* **❌ Requires an optimal compression function $f_\phi$**
::::

:::

::::::

---

## Explicit inference {style="font-size: 20px;"}

::: {.columns}

:::: {.column width="60%"}

The goal is to reconstruct the **entire latent structure** of the Universe — not just compress it into summary statistics.
To do this, we jointly infer:

$$
p(\theta, z \mid x) \propto p(x \mid \theta, z) \, p(z \mid \theta) \, p(\theta)
$$

**Where:**

* $\theta$: cosmological parameters (e.g. matter density, dark energy, etc.)

* $z$: latent fields (e.g. initial conditions of the density field)

* $x$: observed data (e.g. convergence maps or galaxy fields)



:::::: {.fragment fragment-index=1 .fade-in}

::: {.solutionbox}

::: {.solutionbox-header style="font-size: 19px;"}
The challenge of explicit inference

:::

:::: {.solutionbox-body style="font-size: 16px;"}

* The latent variables $z$ typically live in **very high-dimensional spaces** — with millions of degrees of freedom.

* Sampling in this space is **intractable using traditional inference techniques**.

::::

:::

::::::


:::::: {.fragment fragment-index=1 .fade-in}

::: {.solutionbox}

:::: {.solutionbox-body style="font-size: 16px;"}

* We need samplers that can scale efficiently to high-dimensional latent spaces and Exploit **gradients** from differentiable simulators
* This makes **differentiable simulators** essential for modern cosmological inference.
* **Particle Mesh (PM)** simulations offer a scalable and differentiable solution.
::::

:::

::::::

::::

:::: {.column width="40%"}

::: {.r-stack}
![Explicit Inference](assets/latest/likelihood_free_ffi.svg){fig-align="center" width="90%"}
:::

::::

:::


---

## Particle Mesh Simulations {style="font-size: 19px;"}

::: {.columns}

:::: {.column width="40%"}


![](assets/latest/PM_forces.svg){fig-align="center" width="100%"}

::::: {.fragment fragment-index=1.fade-in}
![](assets/latest/PM_interpolate.svg){fig-align="center" width="100%"}
:::::




::::

:::: {.column width="60%"}


###  Compute Forces via PM method

- **Start with particles** $\mathbf{x}_i, \mathbf{p}_i$  
- Interpolate to mesh: $\rho(\mathbf{x})$  
- Solve Poisson’s Equation:
  $$
  \nabla^2 \phi = -4\pi G \rho
  $$
- In Fourier space:
  $$
  \mathbf{f}(\mathbf{k}) = i\mathbf{k}k^{-2}\rho(\mathbf{k})
  $$


::::: {.fragment fragment-index=1 .fade-in}

###  Time Evolution via ODE

- PM uses **Kick-Drift-Kick** (symplectic) scheme:
  - Drift: $\mathbf{x} \leftarrow \mathbf{x} + \Delta a \cdot \mathbf{v}$
  - Kick:  $\mathbf{v} \leftarrow \mathbf{v} + \Delta a \cdot \nabla \phi$

:::::
::::

:::

::::: {.fragment fragment-index=2 .fade-in}

:::{.solutionbox}

:::: {.solutionbox-body style="font-size: 18px; border: 3px solid;"}
 - Fast and scalable approximation to gravity.
 - A cycle of FFTs and interpolations.
 - Sacrifices small-scale accuracy for speed and differentiability.
 - Current implementations **JAXPM v0.1**, **PMWD** and **BORG**.
::::

:::

:::::


# Using Full-Field Inference with Weak Lensing

::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-out}
![](assets/bayes/FFI_full.svg){fig-align="center" width="100%"}
::::::

:::::: {.fragment fragment-index=1 .fade-in}

![](assets/FFI/FFI_wl_focus.svg){fig-align="center" width="100%"}

::::::

:::

---

## From 3D Structure to Lensing Observables {visibility="uncounted" style="font-size: 19px;"}

::: {.columns}
:::: {.column width="60%"}

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-out}
![](assets/latest/how_converge.svg){fig-align="center" width="80%"}
::::::



:::::: {.fragment fragment-index=1 .fade-in-then-out}
![](assets/latest/lightcone_1.svg){fig-align="center" width="100%"}
::::::

:::::: {.fragment fragment-index=2 .fade-in-then-out}
![](assets/latest/lightcone_2.svg){fig-align="center" width="100%"}
::::::

:::::: {.fragment fragment-index=3 .fade-in-then-out}
![](assets/latest/lightcone_3.svg){fig-align="center" width="100%"}
::::::

:::::

::::

:::: {.column width="40%"}

:::::: {.fragment fragment-index=1 .fade-in}
- Simulate structure formation over time, taking snapshots at key redshifts
- Stitch these snapshots into a **lightcone**, mimicking the observer’s view of the universe
- Combine contributions from all slabs to form convergence maps
- Use the Born approximation to simplify the lensing calculation
::::::

:::::: {.fragment fragment-index=3 .fade-in}

:::{.solutionbox}

::: {.solutionbox-header}
Born Approximation for Convergence
:::

:::: {.solutionbox-body style="font-size: 18px;"}

$$
\kappa(\boldsymbol{\theta}) = \int_0^{r_s} dr \, W(r, r_s) \, \delta(\boldsymbol{\theta}, r)
$$

Where the lensing weight is:

$$
W(r, r_s) = \frac{3}{2} \, \Omega_m \, \left( \frac{H_0}{c} \right)^2 \, \frac{r}{a(r)} \left(1 - \frac{r}{r_s} \right)
$$

::::

:::

::::::

::::

:::


# Can we start doing inference?

:::{.notes}

"So we’ve built a forward model — from cosmology to lensing maps.
Now comes the big question: can we start doing inference?" (click)

:::

---


#### The impact of resolution on simulation accuracy


::: {layout-ncol=3}
![$512^3$ Resolution mesh](assets/Fields/LPT_density_field_z0_512.png){fig-align="center" width="75%"}

![$256^3$ Resolution mesh](assets/Fields/LPT_density_field_z0_256.png){fig-align="center" width="75%"}

![$64^3$ Resolution mesh](assets/Fields/LPT_density_field_z0_64.png){fig-align="center" width="75%"}

:::

:::: {.fragment fragment-index=1 .fade-in}

::: {layout-ncol=2}

![-](assets/latest/power_spectrum_comparison.png){fig-align="center" width="80%"}

![Biased Posterior](assets/latest/biased_posterior_plot.png){fig-align="center" width="50%"}

:::

::::

<br/>

::: {.notes}

"Well… not yet. Not if we care about accuracy."
"Here’s what happens as we vary the resolution of the simulation mesh. Visually, the structure starts to break down. But more importantly, we lose power on small scales — and that directly biases the inferred cosmological parameters."
"So even if our inference machinery is technically running — the science is wrong."
“Low-resolution simulations introduce a significant systematic bias in the inferred parameters — even if the rest of the pipeline is perfect.”

:::


---

## When the Simulator Fails, the Model Fails


#### Inference is only as good as the simulator it depends on.


![](assets/latest/illu_compressor.svg){fig-align="center" width="80%"}


:::{.solutionbox}

:::: {.solutionbox-body style="font-size: 18px; border: 3px solid;"}
 - If we want to model complex phenomena like galaxy painting, baryonic feedback, or non-linear structure formation, our simulator must be not only fast, but also physically accurate.
 - A decent resolution for weak lensing requires about 1 grid cell per Mpc/h — both for angular resolution and structure fidelity.
::::

:::

---

## Scaling Up the simulation volume: The LSST Challenge {style="font-size: 20px;"}

::: {.columns}

:::: {.column width="50%"}

#### LSST Scan Range

-  Covers ~**18,000 deg²** (~44% of the sky)
-  Redshift reach: up to **z ≈ 3**
-  Arcminute-scale resolution
-  Requires simulations spanning thousands of Mpc in depth and width

::::

:::: {.column width="50%"}

![LSST Survey Footprint](assets/latest/lsst_footprint.png){fig-align="center" width="70%"}

::::

:::

- Simulating even a **(1 Gpc/h)³** subvolume at **1024³ mesh resolution** requires:
  - ~**54 GB** of memory for a simulation with a single snapshot
  - Gradient-based inference and multi-step evolution push that beyond **100–200 GB**


::: {.columns}

:::: {.column width="70%"}


:::{.solutionbox}

::: {.solutionbox-header style="font-size: 19px;"}
 **Takeaway**  
:::

:::: {.solutionbox-body style="font-size: 18px;"}

 - LSST-scale cosmological inference demands **multiple (Gpc/h)³ simulations** at high resolution.  
Modern high-end GPUs cap at **~80 GB**, so even a single box **requires multi-GPU distributed simulation** — both for memory and compute scalability.

::::

:::

::::

:::: {.column width="30%"}

![Jean Zay HPC - IDRIS](assets/HPC/jean_zay_photo.png){fig-align="center" width="90%"}

::::

:::



# We Need Scalable Distributed Simulations

---


## Distributed Particle Mesh Simulation {style="font-size: 20px;"}


::: {.columns}

:::: {.column width="50%"}

::::: {.r-stack}

::::::: {.fragment fragment-index=1 .fade-out}
![Particle Mesh Simulation](assets/latest/PM_pipeline.svg){fig-align="center" width="100%"}
:::::::

::::::: {.fragment fragment-index=1 .fade-in-then-out}
![Particle Mesh Simulation](assets/latest/pm_laplace_kernel.svg){fig-align="center" width="100%"}
:::::::

::::::: {.fragment fragment-index=2 .fade-in}
![Particle Mesh Simulation](assets/latest/pm_fft.svg){fig-align="center" width="100%"}
:::::::

:::::

::::

:::: {.column width="50%"}

:::::: {.fragment fragment-index=1 .fade-in}

#### Force Computation is Easy to Parallelize

- Poisson’s equation in Fourier space:  
  $$
  \nabla^2 \phi = -4\pi G \rho
  $$

- Gravitational force in Fourier space:  
  $$
  \mathbf{f}(\mathbf{k}) = i\mathbf{k}k^{-2}\rho(\mathbf{k})
  $$

- Each Fourier mode $\mathbf{k}$ can be computed independently using JAX  
- Perfect for large-scale, parallel GPU execution

![](assets/Logos/JaxLogo.png){fig-align="center" width="20%"}

::::::

:::::: {.fragment fragment-index=3 .fade-in}

:::{.solutionbox}

:::: {.solutionbox-body style="font-size: 26px;"}
**Fourier Transform requires global communication**
::::

:::


::::::

::::

:::


---

## jaxDecomp: Distributed 3D FFT and Halo Exchange {style="font-size: 20px;"}

[![](https://img.shields.io/badge/GitHub-jaxdecomp-blue?logo=github)](https://github.com/DifferentiableUniverseInitiative/jaxDecomp)

- **Distributed 3D FFT** using **domain decomposition**  
- **Fully differentiable**, runs on **multi-GPU and multi-node** setups  
- Designed as a **drop-in replacement** for `jax.numpy.fft.fftn`  
- Open source and available on **PyPI** $\Rightarrow$ `pip install jaxdecomp`
- **Halo exchange** for mass conservation across subdomains

:::{layout-ncol=2}

![FFT](assets/jaxDecomp/fft.svg){fig-align="center" width="90%"}


![Halo Exchange](assets/Fields/CIC/Halo_Exchange.svg){fig-align="center" width="75%"}

:::

---


## Distributed Particle Mesh Simulation {style="font-size: 20px;"}

::: {.r-stack}

![Particle Mesh Simulation](assets/latest/pm_cic.svg){fig-align="center" width="60%"}

:::

---

## Cloud In Cell (CIC) Interpolation {style="font-size: 19px;"}

::: {layout-ncol=2}

:::: {.r-stack}

::::: {.fragment fragment-index=1 .fade-out}

![](assets/latest/particles_animation.gif){fig-align="center" width="80%"}

:::::

::::: {.fragment fragment-index=1 .fade-in}

![](assets/latest/particles_last_frame.png){fig-align="center" width="55%"}

:::::

::::

:::: {.r-stack}

::::: {.fragment fragment-index=1 .fade-out}

![](assets/latest/field_animation.gif){fig-align="center" width="80%"}

:::::

::::: {.fragment fragment-index=1 .fade-in}

![](assets/latest/field_last_frame.png){fig-align="center" width="55%"}

:::::

::::

:::


### Mass Assignment and Readout {style="font-size: 19px;"}

The Cloud-In-Cell (CIC) scheme spreads particle mass to nearby grid points using a linear kernel:

::: {.columns}

:::: {.column width="50%"}

- **Paint to Grid** (mass deposition):
  $$
  g(\mathbf{j}) = \sum_i m_i \prod_{d=1}^{D} \left(1 - \left|p_i^d - j_d\right|\right)
  $$

::::

:::: {.column width="50%"}

- **Read from Grid** (force interpolation):
  $$
  v_i = \sum_{\mathbf{j}} g(\mathbf{j}) \prod_{d=1}^{D} \left(1 - \left|p_i^d - j_d\right|\right)
  $$

::::

:::
---

## Distributed Cloud In Cell (CIC) Interpolation {style="font-size: 19px;"}


* In distributed simulations, **each subdomain handles a portion of the global domain**
* **Boundary conditions** are crucial to ensure physical continuity across subdomain edges
* **CIC interpolation** assigns and reads mass from nearby grid cells — potentially crossing subdomain boundaries
* To avoid discontinuities or mass loss, we apply **halo exchange**:

  * Subdomains **share overlapping edge data** with neighbors
  * Ensures **correct mass assignment and gradient flow** across boundaries



::: {.r-stack}

::::: {.fragment fragment-index=1 .fade-in-then-out}

### Without Halo Exchange (Not Distributed)

::: {layout-ncol=3 align="center"}


![Sub Domain 1 (Particles)](assets/latest/D_CIC_Paint_0.svg){fig-align="center" width="65%"}

![Sub Domain 1 (Grid)](assets/latest/D_CIC_Paint_1.svg){fig-align="center" width="65%"}

![Sub Domain 1 (Read out)](assets/latest/D_CIC_Paint_2.svg){fig-align="center" width="65%"}


:::

:::::

::::: {.fragment fragment-index=2 .fade-in}

### With Halo Exchange (Distributed)


::: {layout-ncol=3 align="center"}

![Sub Domain 1 & Halo (Particles)](assets/latest/D_CIC_Paint_3.svg){fig-align="center" width="65%"}

![Sub Domain 1 & Halo (Grid)](assets/latest/D_CIC_Paint_4.svg){fig-align="center" width="65%"}

![Sub Domain 2 & Halo (Grid)](assets/latest/D_CIC_Paint_5.svg){fig-align="center" width="65%"}

:::

:::::

:::

---

## Why Halo Exchange Matters in Distributed Simulations {style="font-size: 20px;"}

::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/HPC/depict_split.png){.absolute top=30 right=20 width="20%"}

::: 

::: {.fragment fragment-index=2 }

![](assets/HPC/depict_gathered.png){.absolute top=30 right=20 width="20%"}

::: 



Without halo exchange, subdomain boundaries introduce visible artifacts in the final field.  
This breaks the smoothness of the result — **even when each local computation is correct**.


::: {.columns}

:::: {.column width="50%"}

:::{.r-stack}


:::{.fragment fragment-index=1 .fade-in-then-out}

:::{layout="[[1] , [1] , [1] , [1]]" layout-valign="center" layout-align="center"}

![First slice](assets/Fields/LPT_density_field_z0_0_no_halo.png){.nostretch fig-align="center" width="50%"}

![Second slice](assets/Fields/LPT_density_field_z0_1_no_halo.png){.nostretch fig-align="center" width="50%"}

![Third slice](assets/Fields/LPT_density_field_z0_2_no_halo.png){.nostretch fig-align="center" width="50%"}

![Fourth slice](assets/Fields/LPT_density_field_z0_3_no_halo.png){.nostretch fig-align="center" width="50%"}

:::

:::

:::{.fragment fragment-index=2 .fade-in}


![No Halo Artifacts](assets/Fields/LPT_density_field_z0_1024_no_halo.png){.nostretch fig-align="center" width="80%"}

:::

::::


:::


:::: {.column width="50%"}


:::{.r-stack}

:::{.fragment fragment-index=2 .fade-in-then-out   style="visibility: hidden;"}

:::{layout="[[1] , [1] , [1] , [1]]" layout-valign="center" layout-align="center"}

![First slice](assets/Fields/LPT_density_field_z0_0_no_halo.png){.nostretch fig-align="center" width="50%"}

![Second slice](assets/Fields/LPT_density_field_z0_1_no_halo.png){.nostretch fig-align="center" width="50%"}

![Third slice](assets/Fields/LPT_density_field_z0_2_no_halo.png){.nostretch fig-align="center" width="50%"}

![Fourth slice](assets/Fields/LPT_density_field_z0_3_no_halo.png){.nostretch fig-align="center" width="50%"}

:::

:::

:::{.fragment fragment-index=3 .fade-in}

![With Halo no Artifacts](assets/Fields/LPT_density_field_z0_1024.png){.nostretch fig-align="center" width="80%"}

:::

:::

::::

:::


# What About the Gradients?

---

## Backpropagating Through ODE Integration {style="font-size: 20px;"}


<br/>
<br/>

::: {.columns}

:::: {.column width="50%"}


### Why Gradients Are Costly

To compute gradients through a simulation, we need to track:

- All intermediate positions: $d_i$
- All velocities: $v_i$
- And their tangent vectors for backpropagation

Even though each update is simple, **autodiff requires storing the full history**.

::::

:::: {.column width="50%"}

::::: {.fragment fragment-index=2 .fade-in}

### Example: Kick-Drift Integration

A typical update step looks like:

$$
\begin{aligned}
d_{i+1} &= d_i + v_i \, \Delta t \\\\
v_{i+1} &= v_i + F(d_{i+1}) \, \Delta t
\end{aligned}
$$

Over many steps, the memory demand scales **linearly** — which becomes a bottleneck for large simulations.

:::::


::::

:::

::::: {.fragment fragment-index=3 .fade-in}


::: {.solutionbox}
::: {.solutionbox-header}
Why This Is a Problem in Practice
:::

::: {.solutionbox-body}
- Storing intermediate states for autodiff causes **memory to scale linearly** with the number of steps.
  
- Example costs at full resolution:
  - **(1 Gpc/h)³**, 10 steps → ~**500 GB**
  - **(2 Gpc/h)³**, 10 steps → ~**4.2 TB**

- This severely limits how many time steps or how large a volume we can afford — even with many GPUs.
:::
:::


:::::




---


## Reverse Adjoint: Gradient Propagation Without Trajectory Storage (**Preliminary**) {style="font-size: 19px;"} 

![](assets/Logos/JaxLogo.png){.absolute top=30 right=10 width="5%"}

::: {.columns}

:::: {.column width="50%"}


* Instead of storing the full trajectory...

* We use the **reverse adjoint method**:

  * Save only the **final state**
  * **Re-integrate backward in time** to compute gradients

::: {.mathbox}

**Forward Pass (Kick-Drift)**

$$
\begin{aligned}
d_{i+1} &= d_i + v_i \, \Delta t \\
v_{i+1} &= v_i + F(d_{i+1}) \, \Delta t
\end{aligned}
$$

**Reverse Pass (Adjoint Method)**

$$
\begin{aligned}
v_i &= v_{i+1} - F(d_{i+1}) \, \Delta t \\
d_i &= d_{i+1} - v_i \, \Delta t
\end{aligned}
$$

:::

::::

:::: {.column width="50%"}

::::: {.fragment fragment-index=1 .fade-in}

![Memory vs. Checkpoints](assets/latest/memory_vs_checkpoints.png){fig-align="center" width="100%"}

::: {.caption style="font-size: 14px; margin-top: 0.5rem;"}
-  **Checkpointing** saves intermediate simulation states periodically to reduce memory — but still grows with the number of steps.
-  **Reverse Adjoint** recomputes on demand, keeping memory constant.
:::

:::::

::::

:::


::::: {.fragment fragment-index=2 .fade-in}

::: {.solutionbox}

:::: {.solutionbox-header style="font-size: 19px;"}
Reverse Adjoint Method
::::

:::: {.solutionbox-body style="font-size: 18px;"}

* **Constant memory** regardless of number of steps
* **Requires a second simulation pass** for gradient computation
* In a 10-step 1024³ Lightcone simulation, reverse adjoint uses **5× less memory** than checkpointing (∼100 GB vs ∼500 GB)

::::


:::

::::


# Putting It All Together

---

## <span style="color:black">JAXPM v0.1.5: Differentiable, Scalable Simulations </span> {background-image="assets/Fields/LPT_density_field_z0_2048.png" background-opacity="0.2" style="font-size: 20px;"}

[![](https://img.shields.io/badge/GitHub-jaxpm-blue?logo=github)](https://github.com/DifferentiableUniverseInitiative/JaxPM)


<br/>
<br/>
<br/>

::: {.solutionbox}

:::: {.solutionbox-header}
What JAXPM v0.1.5 Supports
::::

:::: {.solutionbox-body}

* **Multi-GPU and Multi-Node** simulation with distributed domain decomposition (Successfully ran 2048³ on 256 GPUs)

* **End-to-end differentiability**, including force computation and interpolation

* Compatible with a custom JAX compatible Reverse **Adjoint solver** for memory-efficient gradients

* Supports full **PM Lightcone Weak Lensing**

* Available on **PyPI**: `pip install jaxpm`

* Built on top of `jaxdecomp` for distributed 3D FFT

::::

:::





## Attended Conferences 


:::{.solutionbox}

::::{.solutionbox-header}

French Conferences 


::::

::::{.solutionbox-body}

- **IAP** 2023 Machine Learning-
- **LSST France 2023** Lyon, France
- **LSST France 2024** Marseille, France **(Talk)**
- **IAP GDR CoPhy Tools 2024** Paris, France **(Talk)**
- **CMB France 2024** Paris, France 2024
- **COLOURS 2025** Orsay, France
- ***[Upcomming Conference]*** SO F2F **(Poster)**
- ***[Upcomming Hackathon]*** AISSAI Hackathon **(Hackathon + Workshop)**

::::

:::

:::{.solutionbox}

::::{.solutionbox-header}

International Conferences

::::

::::{.solutionbox-body}

- **Moriond Cosmology 2023** La Thuile, Italy **(Poster)**
- **LSST DESC 2023** Zurich, Switzerland
- **JAXTronomy 2024** Flatiron, New York, USA **(Hackathon)**



::::

:::

 
## Papers & Code

<br />
<br />

:::{.solutionbox}

::::{.solutionbox-header}

Current/Submitted papers

::::

::::{.solutionbox-body}

- Infrared Radiometric Image Classification and Segmentation of Cloud Structure Using ML <span style="color:darkblue;">**(Sommer et al. 2024, Published)**</span>
- **JaxDecomp**: A Distributed Fast Fourier Transform Library <span style="color:darkblue;">**(submitted)**</span>
- **Furax**: MultiPatch Component Separation <span style="color:darkblue;">**(to be submitted soon)**</span>
- **JaxPM**: A Differentiable Particle-Mesh Simulation <span style="color:darkblue;">**(Kabalan et al. in prep.)**</span>

::::

:::

:::{.solutionbox}

::::{.solutionbox-header}

Future papers

::::

::::{.solutionbox-body}

- **Cosmological Inference with Full-Field Inference (DES data)** <span style="color:darkblue;">**(Kabalan et al. in prep.)**</span>

::::

:::

:::{.solutionbox}

::::{.solutionbox-header}

Software

::::

::::{.solutionbox-body}

➢  &emsp;[jaxDecomp](https://github.com/DifferentiableUniverseInitiative/jaxDecomp) : Distributed FFT operations and halo exchange  **(Released)**\
➢  &emsp;[jaxPM](https://github.com/DifferentiableUniverseInitiative/jaxPM) : Distributed Particle-Mesh simulations **(Released)** \
➢  &emsp;[Furax](https://gitlab.in2p3.fr/scipol/furax) : Minimization of the spectral likelihood for CMB component separation **(Released)** \
➢  &emsp;[S2FFT](https://astro-informatics.github.io/s2fft/) : Accelerated spherical harmonics transforms **(Released)**\
➢  &emsp;[jax_healpy](https://github.com/CMBSciPol/jax-healpy) : Healpix port for JAX

::::

:::

## Formations


<div style="font-size: 285%">

:::{.solutionbox}

::::{.solutionbox-header}

divers

::::


::::{.solutionbox-body}

- Euclid summer school (2023)
- AISSAI AstroInfo Hackathon 2023, Frejus, France
- **Physics informed neural networks** with the IDRIS team (Jean-zay super computer) **(2024)**

::::

:::

:::{.solutionbox}

::::{.solutionbox-header}

Cours Ecole Doctorale

::::

::::{.solutionbox-body}

- **QCD** with Matteo Cacciari **(2023)**
- Statistic class with Prof Marco Bomben **(2025)**
- Cosmology with prof. Jim Bartlett **(2025)**

::::

:::

