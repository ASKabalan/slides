---
title: "Generative AI with JAX"
subtitle: "after CNNs"
author: "Wassim Kabalan"
footer: "AISSAI School 2025"
date: "October 2025"
format:
  revealjs:
    theme: [default, css/custom.scss]
    incremental: false
    transition: slide
    background-transition: slide
    presentation-size: max-scale
    highlight-style: github
    slide-number: true
    pdfSeparateFragments: true
    template-partials:
      - css/title-slide.html
output: revealjs

code-block-border-left: "#31BAE9"
title-slide-attributes:
  data-background-image: "assets/titles/bayes_title.png"
  data-background-size: fill
  data-background-opacity: "0.8"

logo1 : '
<div style="display: flex; justify-content: space-around; align-items: center; layout-valign="middle">
  <img src="assets/Logos/AstroDeep-2.png" style="width: 35%;"/>
  <img src="assets/Logos/APC.png" style="width: 20%;"/>
  <img src="assets/Logos/scipol.png" style="width: 35%;"/>
</div>
'

---

## Outline for This Presentation 

<br/>
<br/>
<br/>


::::::: {.solutionbox}

::::{.solutionbox-body style="font-size: 22px;"}


- <span style="color:#1a237e; font-size: 26px;">**Introduction to Generative AI**</span>
<br/>
<span style="color:#1a237e;">Understand the mathematical meaning of generating data</span>
<br/>
<br/>
- <span style="color:#1a237e; font-size: 26px;">**Deep dive into Generative Models**</span>
<br/>
<span style="color:#1a237e;">Explore the different framework and state of the art ways of generative AI</span>
<br/>
<br/>
- <span style="color:#1a237e; font-size: 26px;">**JAX eco system**</span>
<br/>
<span style="color:#1a237e;">Learn core JAX transforms (jit, vmap, pmap) and Flax tooling</span>
<br/>
<br/>
- <span style="color:#1a237e; font-size: 26px;">**Hands on tutorial**</span>
<br/>
<br/>
<span style="color:#1a237e;">Explore how generative AI can be used for cosmological inference</span>


::::

::::

::: {.notes}
This workshop covers three main sections: first, we'll explore different generative AI models and their mathematical properties. Then we'll introduce JAX and its ecosystem for high-performance computing. Finally, we'll apply these techniques to real cosmology problems with hands-on notebooks.
::::

# What is Generative AI?

## What does "generate" mean?

::: {.columns}
:::: {.column width="50%"}
:::: {.fragment .fade-in fragment-index=1}
![Generated faces example](assets/generated_faces.jpg){width=90% }
::::
:::

:::::: {.column width="50%"}
:::: {.r-stack}
::::: {.fragment .fade-in-then-out fragment-index=2}
![](assets/generated/discrete_select.png){width=100%}
:::::

::::: {.fragment .fade-in fragment-index=3}
![](assets/generated/continuos_generate.png){width=100%}
:::::

::::

:::

::::

:::{.solutionbox .fragment .fade-in fragment-index=4}
:::: {.solutionbox-header style="font-size: 20px;"}
Key Idea: Generative AI
::::
::::{.solutionbox-body style="font-size: 18px;"}
- Generative AI is about learning the underlying distribution $p(\text{data})$ and sampling from it to generate images.
- The output of a generative model is a new data point.
- Green point marks a selected/generated draw.
- <span style="color:#1a237e;"><strong>Generative</strong></span> is intrinsically related to <span style="color:#1a237e;"><strong>probability distributions</strong></span>.
::::
:::




::: {.notes}
Generative AI is about learning probability distributions over data. Unlike discrete lookup tables, generative models learn continuous distributions and can sample entirely new data points. The key is modeling $p(\text{data})$ accurately enough to generate realistic samples.
:::

# Generative Models

## Generative Adversarial Networks (GANs)

::: {.columns}
:::: {.column width="50%"}
::::: {.fragment fragment-index=1 .fade-in}

<br/>
<br/>

![GAN Architecture](assets/gan_diagram.png){width=100%; .img-border-2}
:::::
::::

:::: {.column width="50%"}
::::: {.fragment fragment-index=2 .fade-in}
![](assets/gan_title.png){width=95%}

::::: {.solutionbox}
::::{.solutionbox-body style="font-size: 18px;"}
$$\min_G \max_D \mathbb{E}_{x\sim p_\text{data}}[\log D(x)] + \mathbb{E}_{z\sim p(z)}[\log(1 - D(G(z)))]$$

- Loss function
  - Generator: produces samples G(z) to fool D
  - Discriminator: estimates real vs fake probability D(·)
::::
:::

:::::

::::


::::: {.fragment fragment-index=3 .fade-in}

:::{.solutionbox }
::: {.solutionbox-header style="font-size: 20px;"}
Advantages and Limitations
:::
::::{.solutionbox-body style="font-size: 18px;"}
- Advantages: sharp samples; flexible implicit modeling; no explicit likelihood.
- Limitations: unstable training; mode collapse; sensitive to architecture and tricks.
::::
:::

:::::

:::

## Limitations with GANs

::: {.columns}
:::: {.column width="50%"}

::::: {.fragment fragment-index=1 .fade-in}

### Unstable learning

![Nash equilibrium illustration](assets/gan/nash_equilibrium.png){width=100%}

:::::

::::: {.fragment fragment-index=2 .fade-in}

### Mode collapse

![Mode collapse](assets/gan/mode_collapse.png){width=100%}

:::::

::::

:::: {.column width="50%"}

::::: {.fragment fragment-index=3 .fade-in}
### Vanishing gradients
![Vanishing gradients](assets/gan/GAN_vanishing_gradient.png){width=65% .img-border-2}
:::::

::::: {.fragment fragment-index=4 .fade-in}
:::::: {.solutionbox}
::::::: {.solutionbox-header style="font-size: 20px;"}
Some possible solutions
:::::::
:::::::{.solutionbox-body style="font-size: 18px;"}
- Use WGAN for more stable training and stronger gradients.
- Add gradient penalty (WGAN-GP) to keep the critic smooth.
:::::::
::::::
:::::
::::
:::

---

## Variational Autoencoders (VAEs)

::: {.columns}
:::: {.column width="55%"}

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}
![Autoencoder architecture](../assets/vae/autoencoder-schema.png){width=65% .img-border-2}
::::::

:::::: {.fragment fragment-index=2 .fade-in}
![VAE architecture](../assets/vae_arch.png){width=95% .img-border-2}
::::::

:::::

:::::: {.fragment fragment-index=5 .fade-in}
:::{.solutionbox}

:::: {.solutionbox-header style="font-size: 20px;"}

VAE takeaways

::::

::::{.solutionbox-body style="font-size: 18px;"}
- VAE learn optimal compression into the latent space
- Represents a dataset by an easy to sample gaussian distribution
- Compared to GAN gives a better access to probability distribution
- Produces blurrier images than GANs
::::

:::
::::::

::::

:::: {.column width="45%"}

:::::: {.fragment fragment-index=1 .fade-in}
![VAE paper title](../assets/vae_title.png){width=100%}
::::::

:::::: {.fragment fragment-index=2 .fade-in}

Evidence Lower Bound (ELBO):

$$\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \text{KL}(q_\phi(z|x)\|p(z))$$

- **Reconstruction**: decoder quality
- **KL Divergence**: latent regularization

Reparameterization trick:
$$z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0,I)$$
::::::

::::

:::






::: {.notes}
VAEs use an encoder-decoder architecture with a probabilistic latent space. The encoder outputs mean and log-variance for a Gaussian posterior. The reparameterization trick allows backpropagation through the stochastic sampling. The ELBO balances reconstruction quality against matching a prior distribution, typically standard normal.
:::

## VAE — Why KL matters (β-VAE) {.smaller}

::::: {.columns}
:::::: {.column width="50%"}

:::: {.fragment fragment-index=1 .fade-in}
**Without KL regularization**:

![Unregularized](../assets/vae_no_kl.png){width=60% .img-border-2}

Latent space is unstructured
::::

:::

:::::: {.column width="50%"}

:::: {.fragment fragment-index=2 .fade-in}
**With KL regularization**:

![Regularized](../assets/vae_with_kl.png){width=60% .img-border-2}

Organized, continuous latent space
::::

:::
::::

:::: {.fragment fragment-index=3 .fade-in}

**β-VAE objective**:

$$\mathcal{L}_\beta = \mathbb{E}[\log p_\theta(x|z)] - \beta \cdot \text{KL}(q_\phi(z|x)\|p(z))$$

- $\beta > 1$: More disentanglement, less reconstruction
- $\beta < 1$: Better reconstruction, less structure
- **Limitation**: Gaussian prior can bias toward simpler shapes

::::

::: {.notes}
The KL term is crucial for organizing the latent space. Without it, the encoder can map inputs to arbitrary regions making interpolation meaningless. Beta-VAE introduces a hyperparameter β to control the tradeoff between reconstruction quality and latent space disentanglement. Higher β encourages more interpretable latent dimensions but may reduce reconstruction fidelity.
:::

## VAE in Cosmology (Deblending) {.smaller}

::: {.absolute bottom=20 right=20 width="12%"}
![Binh Nguyen](https://avatars.githubusercontent.com/u/93823951?v=4){width=100% .img-border-2}
:::

::::: {.columns}
:::::: {.column width="50%"}

![Deblender architecture](TASK/images/deblender_diagram.png){width=85% .img-border-2}

::: {.fragment fragment-index=2 .fade-in}

![Distribution e1/e2](TASK/images/distribution_e1_e2_recon_pred_no_filtered.png){width=95% .img-border-2}

:::

:::

:::::: {.column width="50%"}

::: {.fragment fragment-index=1 .fade-in}

![Reconstructions](TASK/images/recons_t2.png){width=95% .img-border-2}

:::

:::
::::


::: {.notes}
In cosmology, VAEs are used for galaxy deblending and morphology analysis. The latent representation can compress galaxy images efficiently. However, the Gaussian prior assumption can bias reconstructions toward rounder shapes, which is problematic for weak lensing studies that need accurate ellipticity measurements. This motivates using more flexible priors like normalizing flows.
:::

## Normalizing Flows (Flow-based Generative Models)

![Flow intuition (© Lilian Weng, Lil’Log)](../assets/flow_diagram.png){width=60% .img-border-2 fig-align="center"}

::: {.columns}
::: {.column width="60%"}

::: {.fragment .fade-in fragment-index=1}

Mappings: $f(x) \to y$, $g(y) = z$; $z = g_{\theta}(x) = g_K \circ \cdots \circ g_1(x)$

**Log-likelihood**

$$\log p_{\theta}(x) = \log p_Z\big(g_{\theta}(x)\big)
  + \sum_{k=1}^{K} \log \left|\det J_{g_k}\right|$$

:::

:::

::: {.column width="40%"}

::: {.fragment .fade-in fragment-index=1}

![](../assets/rezende_title.png){width=100% .img-border-2}
<p style="text-align:center; font-size: 12px; margin-top: 2px;">Rezende & Mohamed (2015)</p>

:::

:::
:::

::: {.fragment .fade-in fragment-index=2}

:::{.solutionbox}

:::: {.solutionbox-header style="font-size: 20px;"}
In short
::::

:::: {.solutionbox-body style="font-size: 16px;"}
- Gaussianize data: $z = g_{\theta}(x) \approx \mathcal{N}(0, I)$.
- Exact likelihood: $\log p_{\theta}(x) = \log p_Z(z) + \sum_k \log|\det J_{g_k}|$.
- Generate via inverse: $x = f_{\theta}(z)$, $z \sim p_Z$.
::::

:::

:::

::: {.notes}
Flows learn an invertible transform mapping a simple base $p_Z$ (e.g., $\mathcal{N}(0,I)$) to the data distribution. Training maximizes the exact likelihood via the change-of-variables formula with tractable log-determinants.
:::

## Coupling Layers : Forward vs Inverse 


![Coupling layer computation graph](../assets/coupling_flow.svg){width=60% .img-border-2 fig-align="center"}


::: {.columns}

:::: {.column width="55%"}

**Forward (x → y)**

1. Split $x=(x_a, x_b)$ by mask.
2. Conditioning network on kept half: $(s, t) = \text{NN}(x_a)$.
3. Affine update (element-wise):

$$
 y_a = x_a, \qquad
 y_b = x_b \odot e^{s(x_a)} + t(x_a).
$$


::::

:::: {.column width="45%"}

**Inverse (y → x)**

Given $(y_a, y_b)$:

1. Recompute $(s, t) = \text{NN}(y_a)$.
2. Invert the affine on the transformed half:

$$
 x_a = y_a, \qquad
 x_b = \big(y_b - t(y_a)\big) \odot e^{-s(y_a)}.
$$

::::
:::

## Masked Autoregressive Flow (MAF) vs Inverse Autoregressive Flow (IAF)

![MAF vs IAF comparison](assets/MAF-vs-IAF.png){width=100% .img-border-2 fig-align="center"} 

::: {.columns}
::: {.column width="40%"}

**MAF (Masked Autoregressive Flow)**

Autoregressive flow where density evaluation is **parallel** 
sampling is **sequential**.

$$
 z_i=\frac{x_i-\mu_i(x_{<i})}{\sigma_i(x_{<i})}
$$

:::

::: {.column width="20%"}
:::

::: {.column width="40%"}

**IAF (Inverse Autoregressive Flow) — fast sampling**

Same masked structure but reversed so sampling is **parallel** (given $z$), density is **sequential**.

$$
 x_i=\mu_i(z_{<i})+\sigma_i(z_{<i})\cdot z_i    
$$


:::
:::



## Diffusion Models

:::: {.columns}
::: {.column width="55%"}
![DDPM process](../assets/ddpm_box.png){width=100% .img-border-2}

![Diffusion visualization](../assets/diffusion_spiral.png){width=80% .img-border-2}
:::

::: {.column width="45%"}
![](../assets/ddpm_title.png){width=95% .img-border-2}

:::{.solutionbox}

:::: {.solutionbox-header style="font-size: 20px;"}

Characteristics of Diffusion Models

::::

::::{.solutionbox-body style="font-size: 18px;"}

Pros:

- **Best quality & coverage:** diffusion / flow-matching models (DiT / RFT) for images; rapidly improving for video.
- **Main trade-off:** sampling cost — increasingly mitigated by **consistency models** and fast-sampler **distillation**.

Cons:


- High computational cost for training and sampling.


::::
:::


:::
::::

::: {.notes}
Ultra-compact variant (if space is tight): Diffusion/flow-matching (DiT/RFT) lead in quality; video catching up. Cost = sampling, eased by consistency + distillation.
:::

## Generative models overview

![](../assets/generative-overview.png){width=75% .img-border-2 fig-align="center"}

---

## Generative models overview

| Model | Pros | Cons |
|-------|------|------|
| **VAE** | - Explicit likelihood<br>- Fast sampling<br>- Good global structure | - Blurry outputs<br>- Prior bias (Gaussian)<br>- Limited expressiveness |
| **GAN** | - Sharp, high-quality images<br>- Fast sampling | - Training instability<br>- Mode collapse<br>- No likelihood |
| **Flow** | - Exact likelihood<br>- Invertible<br>- Flexible distributions | - Computationally expensive<br>- Architecture constraints<br>- Difficult to scale |
| **Diffusion** | - SOTA quality<br>- Excellent mode coverage<br>- Stable training | - Slow sampling (many steps)<br>- Computationally intensive<br>- Careful tuning required |

::: {.notes}
Each generative model family has distinct tradeoffs. VAEs offer fast training and sampling but produce blurry outputs. GANs generate sharp images but are unstable to train. Flows provide exact likelihoods but are computationally expensive. Diffusion models achieve the best quality but require many sampling steps. Choose based on your application requirements.
:::

# JAX Ecosystem

## What is JAX? {.smaller}

![](../assets/jax_openxla.png){width=100% .img-border-2}

<div style="display:flex;justify-content:center;align-items:center;height:100%;">
    <h2 style="margin:0;">JAX and OpenXLA</h2>
</div>

::: {.notes}
JAX provides a NumPy-compatible API with powerful function transformations. grad computes derivatives, jit compiles to optimized XLA code, vmap vectorizes operations, and pmap parallelizes across devices. JAX enforces functional programming: functions must be pure without side effects. Randomness is explicit via PRNG key splitting. The same code runs efficiently on CPU, GPU, or TPU through XLA compilation.
:::

## Why JAX? tiny gradient demo

<br/>
<br/>

### Computing gradients with PyTorch

```python
import torch
x = torch.randn(10, 5)
w = torch.randn(5, 3, requires_grad=True)

y = x @ w
loss = y.sum()
loss.backward()
print(w.grad)              # gradient lives on tensor state
```

<br/>
<br/>


### Computing gradients with JAX


```python
import jax, jax.numpy as jnp

def loss(w, x):
    return (x @ w).sum()

x = jnp.ones((10, 5))
w = jnp.ones((5, 3))
grad_w = jax.grad(loss)(w, x)
print(grad_w)              # returns new array; no hidden state
```

## Implications of functional programming

::: {.columns}

:::: {.column width="50%"}

### No impure functions 

```python
import jax, jax.numpy as jnp
state = {"counter": 0}

@jax.jit
def bad(x):
    state["counter"] += 1   # ❌ side-effect breaks JIT/purity
    return x + 1

@jax.jit
def good(x, state):
    state = {"counter": state["counter"] + 1}  # ✅ pure function
    return x + 1, state
```

<br/>

### Explicit PRNG keys

#### Numpy RNG

```python
import numpy as np
np.random.seed(0)
x = np.random.normal(size=(100,))
y = np.random.uniform(size=(100,))
```

::::

:::: {.column width="50%"}

#### No inplace mutations

```python
import jax.numpy as jnp
x = jnp.array([1, 2, 3])

@jax.jit
def bad(x):
    x[0] = 10               # ❌ JAX arrays are immutable
    return x

@jax.jit
def good(x):
    x = x.at[0].set(10)     # ✅ returns new array
    return x
```

<br/>
<br/>
<br/>


#### JAX RNG

```python
import jax.random as jr
key = jr.PRNGKey(0)
k1 , k2 = jr.split(key)
x = jr.normal(k1, shape=(100,))
y = jr.uniform(k2, shape=(100,))
```

::::

:::

::: {.solutionbox}

:::: {.solutionbox-header style="font-size: 20px;"}

In general

::::

::::{.solutionbox-body style="font-size: 16px;"}

- All modified variables must be passed explicitly as function arguments and return values.
- All arrays are immutable; use `.at[].set()` or similar to create modified copies.

::::

::::

## JAX transforms

::: {.columns}
::: {.column width="50%"}

### JIT (Just in time compilation)

```python
import jax
import jax.numpy as jnp

@jax.jit
def f(x): 
    return jnp.sin(x**2) + jnp.cos(x)

y = f(jnp.linspace(0, 3, 1_000))    # compiled once, runs fast
```

<br/>

### grad (automatic differentiation)

```python
import jax
import jax.numpy as jnp

def loss(w, x, y):
     return ((x @ w - y)**2).mean()

dl_dw = jax.grad(loss)(w, x, y)     # grad w.r.t. first arg
dl_dl_dw = jax.grad(jax.grad(loss))(w, x, y)  # 2nd derivative
hessian_dw = jax.hessian(loss)(w, x, y)  # Hessian matrix
```

:::

::: {.column width="50%"}


### Auto-vectorization (vmap)

```python
import jax
import jax.numpy as jnp

def score(x, w): 
    return x @ w        # single example

batched_score = jax.vmap(score, in_axes=(0, None))
y = batched_score(X_batch, w)
```

### Multi-device parallelism (shard_map)

```python
import numpy as np
import jax
import jax.numpy as jnp

mesh = jax.sharding.Mesh(np.array(jax.devices()), ('data',))

def double(x):
     return x * 2
     
double_sharded = jax.shard_map(double, mesh=mesh,
        in_specs=jax.sharding.PartitionSpec('data', None),
        out_specs=jax.sharding.PartitionSpec('data', None))

# shard over axis 0
x = jnp.arange(8).reshape(len(jax.devices()), -1) 
y = double_sharded(x)
```

:::
:::

---

## Neural nets in JAX (Flax)

### Defining a CNN model

:::: {.columns}
::: {.column width="50%"}

```python
import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.c1 = nn.Conv2d(3, 32, 3); self.c2 = nn.Conv2d(32, 64, 3)
        self.fc = nn.Linear(64*6*6, 10)
    def forward(self, x):
        x = F.relu(self.c1(x)); x = F.relu(self.c2(x))
        x = x.view(x.size(0), -1); return self.fc(x)

model = CNN()
```

:::

::: {.column width="50%"}

```python
import jax, jax.numpy as jnp
import flax.linen as nn

class CNN(nn.Module):
    @nn.compact
    def __call__(self, x):
        x = nn.relu(nn.Conv(32, (3,3))(x))
        x = nn.relu(nn.Conv(64, (3,3))(x))
        x = x.reshape((x.shape[0], -1))
        return nn.Dense(10)(x)

model = CNN()
params = model.init(jax.random.PRNGKey(0), jnp.ones((1, 3, 8, 8)))
```

:::
::::


### Training step example

:::: {.columns}
::: {.column width="50%"}

```python
opt = torch.optim.Adam(model.parameters(), lr=1e-3)

def train_step(x, y):
    opt.zero_grad()
    logits = model(x)
    loss = F.cross_entropy(logits, y)
    loss.backward()
    opt.step()
    return loss.item()

loss = train_step(x_batch, y_batch)
```

:::

::: {.column width="50%"}

```python
import jax, jax.numpy as jnp
import optax
from flax.training import train_state

def loss_fn(params, batch):
    logits = model.apply(params, batch["x"])   # forward
    return optax.softmax_cross_entropy_with_integer_labels(
        logits, batch["y"]
    ).mean()

@jax.jit
def train_step(state, batch):
    grads = jax.grad(loss_fn)(state.params, batch)
    return state.apply_gradients(grads=grads)

state = train_state.TrainState.create(
    apply_fn=model.apply,
    params=params,
    tx=optax.adam(1e-3),
)
state = train_step(state, batch)  # one update step
```

:::
::::


## JAX Ecosystem: an overview

### FLAX ![Flax](assets/flax.webp){width=55 style="vertical-align:middle; margin-right: 8px;"} 

Neural network library for JAX with modules, layers, optimizers, training loops.

### Optax ![Optax](assets/optax_logo.svg){width=55 style="vertical-align:middle; margin-right: 8px;"}

Gradient processing and optimization library for JAX.

### BlackJAX ![BlackJAX](assets/blackjax.png){width=55 style="vertical-align:middle; margin-right: 8px;"}

MCMC sampling library for JAX with HMC, NUTS, SGLD algorithms.

### NumPyro ![NumPyro](assets/numpyro.png){width=55 style="vertical-align:middle; margin-right: 8px;"}

Probabilistic programming library for JAX with Bayesian modeling and inference.

### Diffrax 

Differential equation solver library for JAX with ODE, SDE, DDE solvers.

## Composability + Trade-offs

::: {.columns}
::: {.column width="60%"}

**Strengths**

- **NumPy-like** API; clean function transforms
- **Fast & differentiable** via XLA + `jit`
- Powerful **composition**: `jit(vmap(grad(...)))`, `shard_map` for multi-device
- Ecosystem: **Flax, Optax, BlackJAX, Distrax, Diffrax**

**Weaknesses**

- **Steep learning curve** (purity, PRNG keys, transformations)
- Fewer **off-the-shelf models** vs PyTorch
- Some APIs evolving (e.g., sharding tools) → more boilerplate at first

:::

::: {.column width="40%"}

![](../assets/Logos/JaxLogo.png){width=55%}

:::
:::

# Hands-On: Cosmology Applications

## GZ10 Dataset

<br/>
<br/>

**Galaxy Zoo 10** from MultimodalUniverse:

```python
from datasets import load_dataset
ds = load_dataset("MultimodalUniverse/gz10")
```

<br/>
<br/>

**Dataset statistics**:

- ~17,700 galaxy images
- RGB images (256×256 pixels)
- Fields: `gz10_label`, `redshift`, `object_id`

**For this workshop**: Downsize to 32×32 or 64×64 for 

::: {.solutionbox}

:::: {.solutionbox-header style="font-size: 20px;"}

Explore the dataset

::::

::::{.solutionbox-body style="font-size: 16px;"}

Notebook link : `Generative_AI_JAX_GZ10_VAE.ipynb`

::::

::::

---

## Example A: Variational Autoencoder for galaxies


![Variational Autoencoder](TASK/images/autoencoder-architecture.png){width=70% .img-border-2 fig-align="center"}

::: {.solutionbox}

:::: {.solutionbox-header style="font-size: 20px;"}

Train a VAE on GZ10 galaxies and analyze latent space

::::

::::{.solutionbox-body style="font-size: 16px;"}

Notebook link : `Generative_AI_JAX_GZ10_VAE.ipynb`

::::

:::

---

### Example B : Using a classifier from the latent space

![VAE latent space to redshift](TASK/images/deblender_diagram.png){width=70% .img-border-2 fig-align="center"}

::: {.solutionbox}

:::: {.solutionbox-header style="font-size: 20px;"}

Classify galaxy morphology from VAE latent space

::::

::::{.solutionbox-body style="font-size: 16px;"}

Notebook link : `Generative_AI_JAX_GZ10_VAE_Classifier.ipynb`

::::

:::


