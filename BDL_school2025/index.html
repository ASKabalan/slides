<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.7.31">

  <meta name="author" content="Wassim Kabalan">
  <meta name="author" content="Alexandre Boucaud, François Lanusse">
  <title>Bayesian Inference for Cosmology with JAX</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #24292e;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #24292e; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #6a737d; } /* Annotation */
    code span.at { color: #d73a49; } /* Attribute */
    code span.bn { color: #005cc5; } /* BaseN */
    code span.bu { color: #d73a49; } /* BuiltIn */
    code span.cf { color: #d73a49; } /* ControlFlow */
    code span.ch { color: #032f62; } /* Char */
    code span.cn { color: #005cc5; } /* Constant */
    code span.co { color: #6a737d; } /* Comment */
    code span.cv { color: #6a737d; } /* CommentVar */
    code span.do { color: #6a737d; } /* Documentation */
    code span.dt { color: #d73a49; } /* DataType */
    code span.dv { color: #005cc5; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #d73a49; font-weight: bold; } /* Extension */
    code span.fl { color: #005cc5; } /* Float */
    code span.fu { color: #6f42c1; } /* Function */
    code span.im { color: #032f62; } /* Import */
    code span.in { color: #6a737d; } /* Information */
    code span.kw { color: #d73a49; } /* Keyword */
    code span.op { color: #24292e; } /* Operator */
    code span.ot { color: #6f42c1; } /* Other */
    code span.pp { color: #d73a49; } /* Preprocessor */
    code span.re { color: #6a737d; } /* RegionMarker */
    code span.sc { color: #005cc5; } /* SpecialChar */
    code span.ss { color: #032f62; } /* SpecialString */
    code span.st { color: #032f62; } /* String */
    code span.va { color: #e36209; } /* Variable */
    code span.vs { color: #032f62; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-8b3c6da0781682310500f0ce9cbdf807.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="assets/titles/bayes_title_1.png" data-background-opacity="0.8" data-background-size="fill" class="center">
  <h1 class="title"><span style="color:#ffffff; font-size: largest;">Bayesian Inference for Cosmology with JAX</span></h1>
  <p class="author"><span style="color:#ffffff; font-size: larger;">Wassim Kabalan</span></p>
  <p class="author"><span style="color:#ffffff; font-size: Smaller;">Alexandre Boucaud, François Lanusse</span></p>
  <div style=""></div>
<div style="margin-top: 20px; text-align: center;align-items: bottom;">
  <div style="display: flex; justify-content: space-around; align-items: center; layout-valign=" middle"=""> <img src="assets/Logos/AstroDeep-2.png" style="width: 35%;"> <img src="assets/Logos/APC.png" style="width: 20%;"> <img src="assets/Logos/scipol.png" style="width: 35%;"> </div>
</div>

</section>
<section id="outline-for-this-presentation" class="slide level2">
<h2>Outline for This Presentation</h2>
<div class="solutionbox">
<div class="solutionbox-body" style="font-size: 22px; border-radius: 10px; border: 2px solid #3b0a68;">
<ul>
<li><span style="color:#3b0a68; font-size: 26px;"><strong>Understand Cosmological Inference</strong></span>: Learn how we go from observations to cosmological parameters. <br><br></li>
<li><span style="color:#3b0a68; font-size: 26px;"><strong>From χ² to Bayesian Inference</strong></span>: See how Bayesian modeling generalizes classical approaches. <br><br></li>
<li><span style="color:#3b0a68; font-size: 26px;"><strong>Learn Forward Modeling and Hierarchical Models</strong></span>: Understand generative models and field-level inference. <br><br></li>
<li><span style="color:#3b0a68; font-size: 26px;"><strong>Explore Modern Tools (JAX, NumPyro, BlackJAX)</strong></span>: Use practical libraries for scalable inference. <br><br></li>
<li><span style="color:#3b0a68; font-size: 26px;"><strong>Prepare for Hands-On Notebooks</strong></span>: Apply Bayesian techniques in real examples using JAX.</li>
</ul>
</div>
</div>
<aside class="notes">
<p>“Let me walk you through what we’re aiming to cover today.”</p>
<ul>
<li><p><strong>First</strong>, we’ll build an understanding of <strong>cosmological inference</strong> — how we move from raw observational data to constraints on cosmological parameters. This includes both the intuition and the mathematical machinery behind it.</p></li>
<li><p><strong>Second</strong>, we’ll see how <strong>Bayesian inference</strong> generalizes the classical approach. Instead of just optimizing a χ², we model uncertainty and latent structure more fully.</p></li>
<li><p><strong>Third</strong>, we’ll dive into <strong>forward modeling</strong> and <strong>hierarchical models</strong>. These are especially relevant in modern cosmology where we simulate the full data generation process and marginalize over latent variables.</p></li>
<li><p>Then we’ll explore some of the <strong>modern tools</strong> that make all of this practical: <strong>JAX</strong> for fast, differentiable computing; <strong>NumPyro</strong> for probabilistic programming; and <strong>BlackJAX</strong> for flexible sampling.</p></li>
<li><p>Finally, the goal is for you to leave prepared for the <strong>hands-on notebooks</strong>, where you’ll implement and explore real inference pipelines using JAX-based tools.</p></li>
</ul>
<p>The goal of this is to able to start doing bayesian inference .. so if you have any questions, please ask them during the presentation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section>
<section id="background-inference-in-cosmology-the-big-picture" class="title-slide slide level1 center">
<h1>Background : Inference in Cosmology: The Big Picture</h1>

</section>
<section id="inference-in-cosmology-the-frequentist-pipeline" class="slide level2" style="font-size: 21px;">
<h2>Inference in Cosmology: The Frequentist Pipeline</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><br></p>
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/freq_pipeline_0.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="2">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/freq_pipeline_1.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="3">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/freq_pipeline_2.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="4">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/freq_pipeline_3.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="5">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/freq_pipeline_4.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:30%;">
<div class="fragment fade-in" data-fragment-index="1">
<ul>
<li><strong>cosmological parameters</strong> (Ω): matter density, dark energy, etc.</li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<ul>
<li>Predict observables: <strong>CMB, galaxies, lensing</strong></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<ul>
<li>Extract <strong>summary statistics</strong>: <span class="math inline">\(P(k)\)</span>, <span class="math inline">\(C_\ell\)</span> , 2PCF</li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<ul>
<li>Compute <strong>likelihood</strong>: <span class="math inline">\(L(\Omega \vert data)\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="5">
<ul>
<li>Estimate <span class="math inline">\(\hat{\Omega}\)</span> via <strong>maximization</strong> (<span class="math inline">\(\chi^2\)</span> fitting)</li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="6">
<div class="solutionbox">
<div class="solutionbox-header" style="font-size: 20px;">
<p><strong>Frequentist Toolbox</strong></p>
</div>
<div class="solutionbox-body" style="font-size: 18px;">
<ul>
<li>Optimizers/Gradient descent</li>
<li>2-point correlation function (2PCF)</li>
<li>Power spectrum fitting: <span class="math inline">\(P(k)\)</span>, <span class="math inline">\(C_\ell\)</span></li>
</ul>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>“This is the traditional approach many of you are already familiar with — the frequentist pipeline.”</p>
<ul>
<li><p>We start with a set of <strong>cosmological parameters</strong>, denoted here as <strong>Ω</strong> — this could include things like the matter density, dark energy equation of state, etc.</p></li>
<li><p>These parameters are used to predict observable quantities, such as the <strong>CMB power spectrum</strong>, <strong>galaxy clustering</strong>, or <strong>weak lensing shear</strong>.</p></li>
<li><p>From the data, we extract <strong>summary statistics</strong> — typically things like <strong>2-point correlation functions</strong>, <strong>power spectra</strong> (P(k), C_ℓ), or other reduced forms of the data.</p></li>
<li><p>Then, we compute a <strong>likelihood</strong> — usually assuming a Gaussian form for the summary statistics — and estimate parameters by <strong>maximizing this likelihood</strong>. That gives us a point estimate <strong>Ω̂</strong>.</p></li>
<li><p>This pipeline works well when:</p>
<ul>
<li>You have a reliable analytic likelihood,</li>
<li>The summary statistics are informative,</li>
<li>And the assumptions (e.g.&nbsp;Gaussianity, linear regime) hold.</li>
</ul></li>
<li><p>The box in the lower right shows what’s typically in the frequentist toolbox — <strong>χ² fitting</strong>, <strong>2PCF</strong>, and so on.</p></li>
</ul>
<p>“This pipeline is efficient and interpretable — but as we’ll see, it has limitations when you move beyond simple, low-dimensional, or linear models.”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="inference-in-cosmology-the-bayesian-pipeline" class="slide level2" style="font-size: 20px;">
<h2>Inference in Cosmology: The Bayesian Pipeline</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><br></p>
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/freq_pipeline_2.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="2">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_pipeline_0.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="3">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_pipeline_1.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_pipeline_2.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:30%;">
<div class="fragment fade-in" data-fragment-index="1">
<ul>
<li>Start from <strong>summary statistics</strong>: <span class="math inline">\(P(k)\)</span>, <span class="math inline">\(C_\ell\)</span> , 2PCF</li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<ul>
<li>Sample from a <strong>Prior</strong> <span class="math inline">\(P(\Omega)\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<ul>
<li>Compute <strong>likelihood</strong>: <span class="math inline">\(L(Obs \vert \Omega)\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<ul>
<li>Sampler from the <strong>Posterior</strong> <span class="math inline">\(P(\Omega \vert Obs)\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="6">
<div class="solutionbox">
<div class="solutionbox-header" style="font-size: 20px;">
<p><strong>Bayesian Toolbox</strong></p>
</div>
<div class="solutionbox-body" style="font-size: 18px;">
<ul>
<li>Priors encode beliefs: <span class="math inline">\(P(\Omega)\)</span><br>
</li>
<li>Hierarchical Bayesian Modeling (HBM)</li>
<li>Probabilistic programming (e.g., <strong>NumPyro</strong>)</li>
<li>Gradient-based samplers: <strong>HMC</strong>, <strong>NUTS</strong></li>
</ul>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Start by emphasizing that, like the frequentist pipeline, the Bayesian version also begins with summary statistics like power spectra. But instead of finding a best-fit point estimate, the Bayesian approach defines a <strong>probabilistic model</strong> over cosmological parameters.</p>
<p>Now show image 2:</p>
<ul>
<li>We sample from a <strong>prior</strong> over parameters, combine it with a <strong>likelihood</strong> based on the observed data, and use this to compute the <strong>posterior</strong>.</li>
<li>This posterior captures all uncertainty, correlations, and degeneracies.</li>
<li>The Bayesian toolbox lets us extend models easily — hierarchical structure, latent fields, flexible priors — and perform inference using modern tools like NumPyro and gradient-based samplers (e.g.&nbsp;NUTS, HMC).</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="inference-in-cosmology-the-bayesian-pipeline-1" class="slide level2" style="font-size: 18px;" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Inference in Cosmology: The Bayesian Pipeline</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_pipeline_latent.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div><div class="column" style="width:30%;">
<div class="fragment fade-in" data-fragment-index="1">
<ul>
<li><strong>Prior</strong>: Theory-driven assumptions <span class="math inline">\(P(\Omega)\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<ul>
<li><strong>Latent variables</strong>: Hidden/unobserved <span class="math inline">\(z \sim P(z \mid \Omega)\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<ul>
<li><strong>Likelihood</strong>: Generates observables <span class="math inline">\(P(\text{Obs} \mid \Omega, z)\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<ul>
<li><strong>Posterior</strong>: infer <span class="math inline">\(P(\Omega \mid \text{Obs})\)</span></li>
</ul>
</div>
</div></div>
</section>
<section id="inference-in-cosmology-the-bayesian-pipeline-2" class="slide level2" style="font-size: 18px;" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Inference in Cosmology: The Bayesian Pipeline</h2>
<div class="columns">
<div class="column" style="width:10%;">
<p><br></p>
<div class="quarto-figure quarto-figure-middle">
<figure>
<p><img data-src="assets/bayes/bayes_pipeline_latent.svg" class="quarto-figure quarto-figure-middle" style="width:100.0%"></p>
</figure>
</div>
</div><div class="column" style="width:90%;">
<p><strong>Bayes’ Rule with all components:</strong></p>
<blockquote>
<p>Full decomposition of the posterior. The denominator marginalizes over all possible parameters.</p>
</blockquote>
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<p><span class="math display">\[
\underbrace{P(\Omega \mid \text{Obs})}_{\text{Posterior}}
= \frac{
    \underbrace{P(\text{Obs} \mid \Omega)}_{\text{Likelihood}}
    \cdot
    \underbrace{P(\Omega)}_{\text{Prior}}
}{
    \underbrace{
        \int P(\text{Obs} \mid \Omega) P(\Omega) \, d\Omega
    }_{\text{Evidence}}
}
\]</span></p>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<p><span class="math display">\[
\underbrace{P(\Omega \mid \text{Obs})}_{\text{Posterior}}
= \frac{
    \underbrace{\int P(\text{Obs} \mid \Omega, z)\, P(z \mid \Omega)\, dz}_{\text{Likelihood (marginalized over latent $z$)}}
    \cdot
    \underbrace{P(\Omega)}_{\text{Prior}}
}{
    \underbrace{
        \int \left[ \int P(\text{Obs} \mid \Omega, z)\, P(z \mid \Omega)\, dz \right] P(\Omega)\, d\Omega
    }_{\text{Evidence}}
}
\]</span></p>
</div>
</div>
</div></div>
<div class="columns">
<div class="column" style="width:50%;">
<div class="fragment fade-in" data-fragment-index="3">
<blockquote>
<p>In practice, we drop the evidence term when sampling — it’s a constant.</p>
</blockquote>
<p><span class="math display">\[
P(\Omega \mid \text{Obs})
\propto
\underbrace{\int P(\text{Obs} \mid \Omega, z)\, P(z \mid \Omega) \, dz}_{\text{Marginal Likelihood}}
\cdot
\underbrace{P(\Omega)}_{\text{Prior}}
\]</span></p>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<p><span class="math display">\[
\log P(\Omega \mid \text{Obs})
= \log P(\text{Obs} \mid \Omega) + \log P(\Omega)
\]</span></p>
</div>
</div><div class="column" style="width:50%;">
<div class="fragment fade-in" data-fragment-index="5">
<div class="solutionbox">
<div class="solutionbox-header" style="font-size: 17px;">
<p><strong>Bayes’ Rule in Practice</strong></p>
</div>
<div class="solutionbox-body" style="font-size: 15px;">
<ul>
<li><p>The <strong>posterior</strong> combines theory (prior) and data (likelihood) to infer cosmological parameters.</p></li>
<li><p><strong>Latent variables</strong> <span class="math inline">\(z\)</span> encode hidden structure (e.g., initial fields, nuisance parameters).</p></li>
<li><p>The <strong>evidence</strong> is often ignored during sampling (it’s constant).</p></li>
<li><p><strong>Model comparison</strong> via the Bayes Factor:</p>
<p><span class="math display">\[
\text{Bayes Factor} = \frac{P(\text{Obs} \mid \mathcal{M}_1)}{P(\text{Obs} \mid \mathcal{M}_2)}
\]</span></p></li>
</ul>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p><strong>Speaker Notes for This Sequence:</strong></p>
<hr>
<p><strong>STEP 1:</strong></p>
<p>We now extend the Bayesian pipeline by introducing <strong>latent variables</strong> — denoted <em>z</em>. These are hidden, unobserved quantities such as initial conditions, noise fields, or instrumental effects.</p>
<ul>
<li><p>The <strong>prior</strong> encodes our belief over cosmological parameters Ω. It’s <strong>unobserved</strong>, <strong>unknown</strong>, and is the <strong>target of inference</strong>.</p></li>
<li><p>The <strong>latent variables</strong> <em>z</em> are also <strong>unobserved</strong> and <strong>unknown</strong>, but they are <strong>conditional on the prior</strong> — they depend on Ω and are integrated out (marginalized) during inference.</p></li>
<li><p>The <strong>likelihood</strong> is a forward model that generates observables given both the prior and latent structure: <span class="math inline">\(\mathcal{L}(\text{Obs} \mid \Omega, z)\)</span></p></li>
<li><p>The <strong>posterior</strong> combines all of these: it tells us how probable different cosmological parameters are, given the data and the model structure: <span class="math inline">\(P(\Omega \mid \text{Obs}) \propto \int \mathcal{L}(\text{Obs} \mid \Omega, z) \, P(z \mid \Omega) \, dz \cdot P(\Omega)\)</span></p></li>
</ul>
<p>This hierarchical view is what powers modern cosmological inference.</p>
<p><strong>STEP 2:</strong></p>
<p>Here’s a polished version of your speaker notes for that section:</p>
<hr>
<p>We now move to the full <strong>Bayesian formula</strong> — starting without latent variables:</p>
<p>This gives us the <strong>posterior</strong> as the product of the <strong>likelihood</strong> and the <strong>prior</strong>, normalized by the <strong>evidence</strong>. The evidence <span class="math inline">\(P(\text{Obs})\)</span> ensures the posterior is a proper probability distribution.</p>
<p>Now, when we introduce <strong>latent variables</strong> <span class="math inline">\(z\)</span>, the likelihood itself becomes an integral over those:</p>
<p>This marginal likelihood accounts for the full hidden structure.</p>
<p>In practice, we <strong>ignore the evidence</strong> term when sampling:</p>
<ul>
<li>It’s <strong>constant for a given model</strong> — so it doesn’t affect posterior shape.</li>
<li>It’s <strong>computationally expensive</strong> to compute (requires full integration).</li>
<li>But: it’s <strong>very useful for comparing models</strong>.</li>
</ul>
<hr>
<p><strong>Summary</strong>:</p>
<ul>
<li>We sample directly from the <strong>posterior</strong>, which combines <strong>prior</strong> and <strong>likelihood</strong>.</li>
<li><strong>Latent variables</strong> model hidden or uncertain structure — like initial conditions.</li>
<li>The <strong>evidence</strong> is dropped during sampling, but becomes important in <strong>model comparison</strong> using the <strong>Bayes Factor</strong>:</li>
</ul>
<p>This tells us which model is better supported by the data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="two-roads-to-inference-frequentist-and-bayesian" class="slide level2" style="font-size: 20px;">
<h2>Two Roads to Inference: Frequentist and Bayesian</h2>
<p><img data-src="assets/bayes/freq_vs_bayes.png" class="absolute" style="top: -50px; left: 900px; width: 20%; "></p>

<div class="solutionbox">
<div class="solutionbox-header" style="font-size: 20px;">
<p><strong>Conceptual Differences</strong></p>
</div>
<div class="solutionbox-body" style="font-size: 18px;">
<table class="caption-top">
<colgroup>
<col style="width: 14%">
<col style="width: 43%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Concept</strong></th>
<th><strong>Frequentist</strong></th>
<th><strong>Bayesian</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Parameters</strong></td>
<td><strong>Fixed</strong> but unknown</td>
<td><strong>Random variables</strong> with a prior</td>
</tr>
<tr class="even">
<td><strong>Goal</strong></td>
<td><strong>Point estimate</strong> (e.g.&nbsp;MLE)</td>
<td><strong>Full distribution</strong> (posterior over parameters)</td>
</tr>
<tr class="odd">
<td><strong>Uncertainty</strong></td>
<td>From <strong>data variability</strong></td>
<td>From <strong>parameter uncertainty</strong> (posterior)</td>
</tr>
<tr class="even">
<td><strong>Prior Knowledge</strong></td>
<td><strong>Not used</strong></td>
<td><strong>Explicitly included</strong> via prior <span class="math inline">\(P(\Omega)\)</span></td>
</tr>
<tr class="odd">
<td><strong>Interval Meaning</strong></td>
<td><strong>Confidence interval</strong>: “95% of experiments contain truth”</td>
<td><strong>Credible interval</strong>: “95% chance truth is in this range”</td>
</tr>
<tr class="even">
<td><strong>Likelihood Role</strong></td>
<td>Central in <strong><span class="math inline">\(\chi^2\)</span> minimization</strong>, fits</td>
<td>Combined with <strong>prior</strong> to form posterior</td>
</tr>
<tr class="odd">
<td><strong>Inference Output</strong></td>
<td><strong>Best-fit estimate</strong> + error bars</td>
<td><strong>Posterior distribution</strong></td>
</tr>
<tr class="even">
<td><strong>Tooling</strong></td>
<td><strong>Optimization</strong> (e.g.&nbsp;χ², maximum likelihood)</td>
<td><strong>Sampling</strong> (e.g.&nbsp;MCMC, HMC, NUTS)</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="columns">
<div class="column" style="width:90%;">
<p>Although these approaches are often contrasted, <strong>they’re not mutually exclusive</strong>. Modern workflows — like <strong>causal inference</strong> in <a href="https://www.youtube.com/watch?v=FdnMWdICdRs"><em>Statistical Rethinking</em></a> — draw on both perspectives. Bayesian methods offer a formal way to <strong>combine theory and data</strong>, especially powerful when simulations are involved.</p>
</div><div class="column" style="width:10%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/stat_rethink.jpg" style="width:100.0%"></p>
<figcaption>Statistical Rethinking</figcaption>
</figure>
</div>
</div></div>
<p><br></p>
<aside class="notes">
<p>This slide lays out a direct comparison between the <strong>frequentist</strong> and <strong>Bayesian</strong> approaches.</p>
<p>We’re highlighting not just the philosophical differences, but also the <strong>practical consequences</strong>.</p>
<p>A few key contrasts to emphasize:</p>
<ul>
<li><p><strong>Parameters</strong>: In frequentist stats, parameters are fixed but unknown. In Bayesian stats, they’re <strong>random variables</strong> — we assign distributions to represent our uncertainty.</p></li>
<li><p><strong>Goal</strong>: Frequentists usually aim for a <strong>point estimate</strong> (like the MLE). Bayesians aim to recover the <strong>entire posterior distribution</strong>.</p></li>
<li><p><strong>Uncertainty</strong>: Frequentists focus on <strong>data variability</strong> — uncertainty from random samples. Bayesians focus on <strong>parameter uncertainty</strong> — how uncertain we are about the parameters given the data.</p></li>
<li><p><strong>Intervals</strong>: The interpretations are totally different. A frequentist says, “95% of the time, this interval contains the truth.” A Bayesian says, “There’s a 95% chance the truth is in this interval.”</p></li>
<li><p><strong>Tooling</strong>: Optimization vs.&nbsp;sampling. Frequentists often rely on <strong>curve fitting</strong>, minimization, etc. Bayesian workflows rely on <strong>sampling</strong> (MCMC, HMC, NUTS).</p></li>
</ul>
<p>Make sure the audience sees this isn’t about choosing sides. As the bottom note says — they’re <strong>not mutually exclusive</strong>. A lot of modern workflows combine both perspectives.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><span class="citation" data-cites="image">@image</span> credit:Wayne Stewart</p>
</div></aside></section></section>
<section>
<section id="the-mechanics-of-inference" class="title-slide slide level1 center">
<h1>🛠️ The Mechanics of Inference</h1>

</section>
<section id="sampling-the-posterior-the-core-loop" class="slide level2" style="font-size: 20px;">
<h2>Sampling the Posterior: The Core Loop</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/inference_loop.svg" class="quarto-figure quarto-figure-center" style="width:75.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p><strong>The Sampling Loop:</strong></p>
<div class="fragment fade-in" data-fragment-index="1">
<ul>
<li>Start from a sample <span class="math inline">\((\Omega^t, z^t)\)</span><br>
</li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<ul>
<li>Propose new sample <span class="math inline">\((\Omega', z')\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<ul>
<li>Compute <strong>acceptance probability</strong></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<ul>
<li>Accept or reject proposal</li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="5">
<ul>
<li>Repeat and store accepted samples ⟶ <strong>posterior</strong></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="6">
<p><strong>Goal:</strong> Explore the full shape of the posterior<br>
(even in high-dim, non-Gaussian spaces)</p>
</div>
<div class="fragment fade-in" data-fragment-index="7">
<div class="solutionbox">
<div class="solutionbox-header" style="font-size: 20px;">
<p><strong>Key Takeaways</strong></p>
</div>
<div class="solutionbox-body" style="font-size: 18px;">
<ul>
<li>Most samplers follow this <strong>accept/reject loop</strong></li>
<li>Differ by how they propose samples: – Random walk (e.g., MH) – Gradient-guided (e.g., HMC, NUTS)</li>
<li>Some skip rejection (e.g., Langevin, VI)</li>
</ul>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>This slide illustrates the core mechanism behind most MCMC samplers — the accept/reject loop.</p>
<p>We start with a current sample from the posterior, say <span class="math inline">\((\Omega^t, z^t)\)</span>. The sampler then proposes a new point <span class="math inline">\((\Omega', z')\)</span>, using some rule — it might be a random walk, or it might use gradients like in HMC or NUTS.</p>
<p>Next, we compute the <strong>acceptance probability</strong> — this depends on how likely the new sample is under the posterior compared to the current one.</p>
<p>Then we make a decision:</p>
<ul>
<li>If the new sample is more likely (or meets some acceptance threshold), we accept it and add it to the chain.</li>
<li>If not, we reject it and store the current one again.</li>
</ul>
<p>This process repeats to build a chain of samples. The accepted ones collectively approximate the posterior distribution.</p>
<p>On the right, the key takeaways:</p>
<ul>
<li>Most samplers use this loop.</li>
<li>The difference lies in how they generate proposals — basic methods use random walks, while advanced methods use gradients.</li>
<li>Some newer algorithms avoid rejection altogether — like variational inference or some Langevin-based flows — but the classic accept/reject structure remains fundamental to many MCMC approaches.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 style="font-size: 20px;" id="sampling-algorithms-at-a-glance">Sampling Algorithms at a Glance</h3>
<div class="columns">
<div class="column" style="width:65%;">
<div class="fragment fade-in" data-fragment-index="1">
<p><strong>Metropolis-Hastings (MCMC)</strong></p>
<ul>
<li><p><strong>Propose</strong>: Random walk <span class="math inline">\(\Omega' \sim \mathcal{N}(\Omega^t, \sigma^2)\)</span></p></li>
<li><p><strong>Accept</strong>:</p>
<p><span class="math display">\[
\alpha = \min\left(1, \frac{P(\text{Obs} \mid \Omega') P(\Omega')}{P(\text{Obs} \mid \Omega^t) P(\Omega^t)}\right)
\]</span></p></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<p><strong>Hamiltonian Monte Carlo (HMC)</strong></p>
<ul>
<li><strong>Propose</strong>: Simulate physics Trajectory via gradients <span class="math inline">\(\nabla\_\Omega \log P(\text{Obs} \mid \Omega)\)</span></li>
<li><strong>Accept</strong>: Based on Hamiltonian energy conservation. <span class="math inline">\(\alpha = \min(1, e^{\mathcal{H}(\Omega^t, p^t) - \mathcal{H}(\Omega', p')})\)</span></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<p><strong>NUTS (No-U-Turn Sampler)</strong> Same as HMC, but auto-tunes:</p>
<ul>
<li>Step size</li>
<li>Trajectory length (stops before looping back)</li>
</ul>
</div>
</div><div class="column" style="width:35%;">
<div class="fragment fade-in" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/MCMC.gif" class="quarto-figure quarto-figure-center" style="border: 2px solid black; padding: 2px;;width:80.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/HMC.gif" class="quarto-figure quarto-figure-center" style="border: 2px solid black; padding: 2px;;width:80.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/NUTS.gif" class="quarto-figure quarto-figure-center" style="border: 2px solid black; padding: 2px;;width:80.0%"></p>
</figure>
</div>
</div>
</div></div>
<p><br> <br></p>

<aside class="notes">
<p>This slide gives a quick overview of three core MCMC algorithms.</p>
<p>We start with <strong>Metropolis-Hastings (MH)</strong>: It proposes a new sample using a simple random walk — typically from a Normal centered on the current value. The acceptance probability is the ratio of posteriors — new over old — and we accept based on how much better the new sample fits. You can see on the diagram: it just takes a small step and checks whether to keep it.</p>
<p>Next, <strong>Hamiltonian Monte Carlo (HMC)</strong>: Rather than proposing random jumps, HMC uses gradients to simulate a physical trajectory through parameter space — like a particle rolling through a potential landscape. This allows it to make larger moves that still preserve the posterior distribution. Acceptance here is based on energy conservation, using a Hamiltonian formulation.</p>
<p>Finally, <strong>NUTS (No-U-Turn Sampler)</strong>: This builds on HMC, but it adds smart tuning:</p>
<ul>
<li>It automatically adjusts step size</li>
<li>It stops the trajectory before looping back on itself — hence “No-U-Turn” This makes NUTS a great default sampler in most PPLs — it avoids a lot of manual tuning and works well out of the box.</li>
</ul>
<p>Together, these illustrate a spectrum: from simple MH to advanced gradient-based samplers — and show how modern samplers make use of geometry to efficiently explore high-dimensional spaces.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<aside><div>
<p><span class="citation" data-cites="credit">@credit</span>: https://github.com/chi-feng/mcmc-demo</p>
</div></aside></section>
<section id="gradient-based-sampling-in-action" class="slide level2" style="font-size: 20px;" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Gradient-Based Sampling in Action</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/gaussian_hmc.gif" style="width:50.0%"></p>
<figcaption>HMC: Gaussian Posterior</figcaption>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/gaussian_hmc_density.png" style="width:50.0%"></p>
<figcaption>HMC: Gaussian Posterior</figcaption>
</figure>
</div>
</div>
</div>
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="3">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/banana_hmc.gif" style="width:50.0%"></p>
<figcaption>HMC: Banana Posterior</figcaption>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/banana_hmc_density.png" style="width:50.0%"></p>
<figcaption>HMC: Banana Posterior</figcaption>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/gaussian_mcmc.gif" style="width:50.0%"></p>
<figcaption>MCMC: Gaussian Posterior</figcaption>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/gaussian_mcmc_density.png" style="width:50.0%"></p>
<figcaption>MCMC: Gaussian Posterior</figcaption>
</figure>
</div>
</div>
</div>
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="3">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/banana_mcmc.gif" style="width:50.0%"></p>
<figcaption>MCMC: Banana Posterior</figcaption>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/banana_mcmc_density.png" style="width:50.0%"></p>
<figcaption>MCMC: Banana Posterior</figcaption>
</figure>
</div>
</div>
</div>
</div></div>
</section>
<section id="gradient-based-sampling-in-action-1" class="slide level2" style="font-size: 20px;" data-auto-animate="true">
<h2 data-id="quarto-animate-title">Gradient-Based Sampling in Action</h2>
<div class="columns">
<div class="column" style="width:10%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/gaussian_hmc_density.png" style="width:50.0%"></p>
<figcaption>HMC: Gaussian Posterior</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/banana_hmc_density.png" style="width:50.0%"></p>
<figcaption>HMC: Banana Posterior</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/gaussian_mcmc_density.png" style="width:50.0%"></p>
<figcaption>MCMC: Gaussian Posterior</figcaption>
</figure>
</div>
</div><div class="column" style="width:10%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/banana_mcmc_density.png" style="width:50.0%"></p>
<figcaption>MCMC: Banana Posterior</figcaption>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li>In high dimensions, <strong>random walk proposals</strong> (MCMC) often land in low-probability regions ⟶ low acceptance.</li>
<li>To maintain acceptance, step size must shrink like <strong><span class="math inline">\(1/\sqrt{d}\)</span></strong> ⟶ very slow exploration.</li>
<li><strong>HMC uses gradients</strong> to follow high-probability paths ⟶ <strong>better samples, fewer steps</strong>.</li>
</ul>
</div></div>
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/samples_only.png" style="width:50.0%"></p>
<figcaption>Sampling Without Gradients</figcaption>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Samplers/samples_with_gradients.png" style="width:50.0%"></p>
<figcaption>Sampling With Gradients</figcaption>
</figure>
</div>
</div>
</div>
<aside class="notes">
<p>In this slide we compare HMC and traditional MCMC in two scenarios:</p>
<hr>
<h3 id="top-row-gaussian-posterior">Top row: <strong>Gaussian posterior</strong></h3>
<ul>
<li>HMC (left) aligns well with the true density contours — samples are well spread.</li>
<li>MCMC (right) struggles a bit — it’s noisy, slightly distorted, and shows <strong>correlated samples</strong>.</li>
<li>That’s because MCMC does a random walk, which is inefficient even in simple geometries.</li>
</ul>
<hr>
<h3 id="bottom-row-banana-shaped-posterior">Bottom row: <strong>Banana-shaped posterior</strong></h3>
<ul>
<li>This is a <strong>nonlinear</strong>, curved posterior — a much harder target.</li>
<li>HMC (left) still tracks the true shape well using its gradient information.</li>
<li>MCMC (right) again struggles: it oversamples in wrong regions and can’t explore the full space.</li>
</ul>
<hr>
<h3 id="key-point">Key Point:</h3>
<p>HMC shines when the geometry is tricky. Its gradients guide proposals along the posterior, unlike MCMC’s aimless wandering.</p>
<p>This motivates why we use HMC or NUTS in high-dimensional, curved, or strongly correlated problems — like cosmology.</p>
<p><strong>Slide: Sampling Without Gradients</strong></p>
<ul>
<li>This shows how traditional MCMC struggles when sampling from complex distributions.</li>
<li>Here, proposals are based on <strong>random walks</strong>, which means they can easily jump into low-probability regions.</li>
<li>As a result, many proposals are rejected — leading to inefficient sampling.</li>
<li>To maintain high acceptance, samplers reduce step size — but this slows down exploration dramatically, especially in high dimensions.</li>
<li>You can see the samples (blue dots) under-sample the second peak and have poor coverage overall.</li>
</ul>
<hr>
<p><strong>Slide: Sampling With Gradients</strong></p>
<ul>
<li>Now we add <strong>gradient information</strong> — this is what HMC uses.</li>
<li>Gradients give the sampler a sense of “direction,” pointing it toward high-probability areas.</li>
<li>Instead of random jumps, we simulate <strong>trajectories</strong> that follow the shape of the distribution.</li>
<li>This enables much better exploration with fewer steps.</li>
<li>As a result, samples land more effectively across both peaks and better represent the target distribution.</li>
<li>This illustrates why gradient-based samplers like HMC or NUTS perform so well in high-dimensional, structured problems.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="differentiable-inference-with-jax" class="slide level2" style="font-size: 20px;">
<h2>Differentiable Inference with JAX</h2>
<p>When it comes to <strong>gradients</strong>, always think of <strong>JAX</strong>.</p>
<p><br></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>An Easy pythonic API</strong></p>
<div class="sourceCode" id="cb1" data-code-line-numbers="|11"><pre class="sourceCode numberSource Python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> jax</span>
<span id="cb1-2"><a></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-3"><a></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb1-4"><a></a></span>
<span id="cb1-5"><a></a><span class="kw">def</span> sample_prior(key):</span>
<span id="cb1-6"><a></a>    <span class="cf">return</span> random.normal(key, shape<span class="op">=</span>(<span class="dv">3</span>,))  <span class="co"># Ω ~ N(0, 1)</span></span>
<span id="cb1-7"><a></a></span>
<span id="cb1-8"><a></a><span class="kw">def</span> log_prob(omega):</span>
<span id="cb1-9"><a></a>    <span class="cf">return</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> jnp.<span class="bu">sum</span>(omega<span class="op">**</span><span class="dv">2</span>)  <span class="co"># log p(Ω) ∝ -Ω²</span></span>
<span id="cb1-10"><a></a></span>
<span id="cb1-11"><a></a>log_prob_jit <span class="op">=</span> jax.jit(log_prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="fragment fade-in" data-fragment-index="2">
<p><strong>Easily accessible gradients using GRAD</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a></a>omegas <span class="op">=</span> ... <span class="co"># Sampled Ω</span></span>
<span id="cb2-2"><a></a>gradients <span class="op">=</span> jax.grad(log_prob_jit)(omegas)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<p><strong>Supports vectorization using VMAP</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a></a><span class="kw">def</span> generate_samples(seeds):</span>
<span id="cb3-2"><a></a>    key <span class="op">=</span> jax.random.PRNGKey(seeds)</span>
<span id="cb3-3"><a></a>    omega <span class="op">=</span> sample_prior(key)</span>
<span id="cb3-4"><a></a>    <span class="cf">return</span> omega</span>
<span id="cb3-5"><a></a>seeds <span class="op">=</span> jnp.arange(<span class="dv">0</span>, <span class="dv">1000</span>)</span>
<span id="cb3-6"><a></a>omegas <span class="op">=</span> jax.vmap(generate_samples)(seeds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/Logos/JaxLogo.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
</div></div>
<aside class="notes">
<ul>
<li>This is why JAX is such a natural fit for inference: it’s fully differentiable and built around <strong>gradients</strong>.</li>
<li>On the left we show three core ideas that power modern inference.</li>
</ul>
<p><strong>First block</strong>: JAX gives you a familiar NumPy-like API, with tools like <code>jit</code> to compile and optimize code. You define a prior, a log-prob function, and wrap it in <code>jit</code> — easy and fast.</p>
<p><strong>Second block</strong>: With <code>jax.grad</code>, you can differentiate any function. That means you get gradients of the log-posterior “for free,” which is exactly what HMC or variational inference need.</p>
<p><strong>Third block</strong>: JAX scales easily — use <code>vmap</code> to vectorize your function across many seeds, particles, or chains. This is a huge win when doing amortized inference or simulation-based methods.</p>
<ul>
<li>Altogether, JAX provides the <strong>gradient plumbing</strong> for probabilistic inference — while remaining readable and fast.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="practical-bayesian-modeling-inference-with-jax" class="title-slide slide level1 center">
<h1>Practical Bayesian Modeling &amp; Inference with JAX</h1>

</section>
<section id="a-recipe-for-bayesian-inference" class="slide level2">
<h2>A Recipe for Bayesian Inference</h2>
<div class="columns">
<div class="column" style="width:60%;">
<div class="fragment fade-in" data-fragment-index="2">
<p><strong>1. Probabilistic Programming Language (PPL)</strong> <em>NumPyro</em>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a></a><span class="im">import</span> numpyro</span>
<span id="cb4-2"><a></a><span class="im">import</span> numpyro.distributions <span class="im">as</span> dist</span>
<span id="cb4-3"><a></a></span>
<span id="cb4-4"><a></a><span class="kw">def</span> model():</span>
<span id="cb4-5"><a></a>    omega_m <span class="op">=</span> numpyro.sample(<span class="st">"Ωₘ"</span>, dist.Uniform(<span class="fl">0.1</span>, <span class="fl">0.5</span>))</span>
<span id="cb4-6"><a></a>    sigma8 <span class="op">=</span> numpyro.sample(<span class="st">"σ₈"</span>, dist.Normal(<span class="fl">0.8</span>, <span class="fl">0.1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<p><strong>2. Computing Likelihoods</strong> <em>JAX-Cosmo</em>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a></a><span class="im">import</span> jax_cosmo <span class="im">as</span> jc</span>
<span id="cb5-2"><a></a><span class="kw">def</span> likelihood(cosmo_params):</span>
<span id="cb5-3"><a></a>    mu, cov <span class="op">=</span> jc.angular_cl.gaussian_cl_covariance_and_mean(</span>
<span id="cb5-4"><a></a>        cosmo_params, ell, probes</span>
<span id="cb5-5"><a></a>    )</span>
<span id="cb5-6"><a></a>    <span class="cf">return</span> jc.likelihood.gaussian_log_likelihood(data, mu, cov)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<p><strong>3. Sampling the Posterior</strong> <em>NumPyro &amp; Blackjax</em>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a></a><span class="im">from</span> numpyro.infer <span class="im">import</span> MCMC, NUTS</span>
<span id="cb6-2"><a></a></span>
<span id="cb6-3"><a></a>kernel <span class="op">=</span> NUTS(model)</span>
<span id="cb6-4"><a></a>mcmc <span class="op">=</span> MCMC(kernel, num_warmup<span class="op">=</span><span class="dv">500</span>, num_samples<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb6-5"><a></a>mcmc.run(random.PRNGKey(<span class="dv">0</span>))</span>
<span id="cb6-6"><a></a>samples <span class="op">=</span> mcmc.get_samples()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="5">
<p><strong>4. Visualizing the Posterior</strong> <em>ArviZ</em>:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb7-2"><a></a>samples <span class="op">=</span> mcmc.get_samples()</span>
<span id="cb7-3"><a></a>az.plot_pair(samples, marginals<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div><div class="column" style="width:40%;">
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/shopping_cart_0.svg" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="2">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/shopping_cart_1.svg" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="3">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/shopping_cart_2.svg" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="4">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/shopping_cart_3.svg" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="5">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/shopping_cart_4.svg" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
</div>
</div>
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_full.svg" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="2">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_prior.svg" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="3">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_likelihood.svg" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="4">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_sample.svg" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in-then-out" data-fragment-index="5">
<img data-src="assets/bayes/post_corner.png" class="quarto-figure quarto-figure-center" style="width:70.0%">
<div style="text-align: center; font-size: 12px;">
<span class="citation" data-cites="credit">@credit</span>: Zeghal et al.&nbsp;(2409.17975)
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p>Here are the speaker notes for the two slides:</p>
<hr>
<p><strong>Slide 1: “A Recipe for Bayesian Inference”</strong></p>
<ul>
<li><p>Now let’s break Bayesian inference into a practical workflow.</p></li>
<li><p>First, we define our model using a <strong>probabilistic programming language</strong> — here, NumPyro. This is where we encode the prior and the structure of the model.</p></li>
<li><p>Second, we use a tool like <strong>JAX-Cosmo</strong> to compute the likelihood. This connects cosmological parameters to observable predictions — such as angular power spectra.</p></li>
<li><p>Third, we use <strong>MCMC</strong> or <strong>BlackJAX</strong> to sample from the posterior.</p>
<ul>
<li>This is where sampling happens, and where gradient-based methods like NUTS come into play.</li>
</ul></li>
<li><p>Finally, we extract posterior samples to analyze uncertainty and parameter correlations.</p></li>
</ul>
<p>The graphic on the right summarizes this logic visually: from prior → likelihood → posterior → sample.</p>
<hr>
<p><strong>Slide 2: “A Recipe for Bayesian Inference (Full Loop)”</strong></p>
<ul>
<li>This slide extends the recipe with the <strong>final step: visualization</strong>.</li>
<li>After sampling, we can summarize and visualize the posterior with tools like <strong>ArviZ</strong>.</li>
<li>The corner plot on the right shows the joint and marginal distributions — a key diagnostic to assess whether inference worked and how parameters are correlated.</li>
<li>The dashed vs.&nbsp;solid lines show different inference strategies — possibly <strong>explicit</strong> (forward simulations) vs <strong>implicit</strong> likelihoods.</li>
</ul>
<p>The takeaway is that with JAX, NumPyro, and JAX-Cosmo, we have a <strong>modular pipeline</strong> for Bayesian inference — from model definition to visual diagnostics.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="a-minimal-bayesian-linear-model" class="slide level2" style="font-size: 18px;">
<h2>A Minimal Bayesian Linear Model</h2>
<p><strong>Define a simple linear model:</strong></p>
<div class="sourceCode" id="cb8" data-code-line-numbers="|1-8|10-16"><pre class="sourceCode numberSource Python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a></a>true_w <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb8-2"><a></a>true_b <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb8-3"><a></a>num_points <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-4"><a></a></span>
<span id="cb8-5"><a></a>rng_key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb8-6"><a></a>x_data <span class="op">=</span> jnp.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, num_points)</span>
<span id="cb8-7"><a></a>noise <span class="op">=</span> jax.random.normal(rng_key, shape<span class="op">=</span>(num_points,)) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb8-8"><a></a>y_data <span class="op">=</span> true_w <span class="op">*</span> x_data <span class="op">+</span> true_b <span class="op">+</span> noise</span>
<span id="cb8-9"><a></a></span>
<span id="cb8-10"><a></a><span class="kw">def</span> linear_regression(x, y<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb8-11"><a></a>    w <span class="op">=</span> numpyro.sample(<span class="st">"w"</span>, dist.Normal(<span class="fl">1.</span>, <span class="fl">2.</span>))</span>
<span id="cb8-12"><a></a>    b <span class="op">=</span> numpyro.sample(<span class="st">"b"</span>, dist.Normal(<span class="fl">0.</span>, <span class="fl">2.</span>))  <span class="co"># Fixed the second parameter</span></span>
<span id="cb8-13"><a></a>    sigma <span class="op">=</span> numpyro.sample(<span class="st">"sigma"</span>, dist.Exponential(<span class="fl">1.0</span>))</span>
<span id="cb8-14"><a></a></span>
<span id="cb8-15"><a></a>    mean <span class="op">=</span> w <span class="op">*</span> x <span class="op">+</span> b</span>
<span id="cb8-16"><a></a>    numpyro.sample(<span class="st">"obs"</span>, dist.Normal(mean, sigma), obs<span class="op">=</span>y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="fragment fade-in" data-fragment-index="1">
<p><strong>Run the model using NUTS:</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a></a>kernel <span class="op">=</span> numpyro.infer.NUTS(linear_regression)</span>
<span id="cb9-2"><a></a>mcmc <span class="op">=</span> numpyro.infer.MCMC(kernel, num_warmup<span class="op">=</span><span class="dv">500</span>, num_samples<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb9-3"><a></a>mcmc.run(rng_key, x<span class="op">=</span>x_data, y<span class="op">=</span>y_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<p><strong>Posterior corner plot using arviz + corner</strong></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a></a>idata <span class="op">=</span> az.from_numpyro(mcmc)</span>
<span id="cb10-2"><a></a>posterior_array <span class="op">=</span> az.extract(idata, var_names<span class="op">=</span>[<span class="st">"w"</span>, <span class="st">"b"</span>, <span class="st">"sigma"</span>]).to_array().values.T</span>
<span id="cb10-3"><a></a></span>
<span id="cb10-4"><a></a>fig <span class="op">=</span> corner.corner(</span>
<span id="cb10-5"><a></a>    posterior_array,</span>
<span id="cb10-6"><a></a>    labels<span class="op">=</span>[<span class="st">"w"</span>, <span class="st">"b"</span>, <span class="st">"σ"</span>],</span>
<span id="cb10-7"><a></a>    truths<span class="op">=</span>[true_w, true_b, <span class="va">None</span>],</span>
<span id="cb10-8"><a></a>    show_titles<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-9"><a></a>)</span>
<span id="cb10-10"><a></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<aside class="notes">
<p>This slide walks through the first notebook exercise — a minimal Bayesian linear regression model using NumPyro.</p>
<ul>
<li>First block defines synthetic data: we’re sampling noisy <code>y</code> values from a true line <code>y = 2x - 1</code>.</li>
<li>The <code>linear_regression</code> function defines a probabilistic model: priors on slope <code>w</code>, intercept <code>b</code>, and noise <code>sigma</code>.</li>
<li>We use NUTS from NumPyro to sample from the posterior — no likelihood math needed manually.</li>
<li>Finally, the bottom cell visualizes the posterior using <code>corner.py</code> and ArviZ. You’ll see how the true parameters compare to the inferred distributions.</li>
</ul>
<p>This is the foundation — you’ll implement and modify this directly during the hands-on.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<h3 id="numpyro-tips-tricks-for-bayesian-modeling">Numpyro: Tips &amp; Tricks for Bayesian Modeling</h3>
<p><strong><code>numpyro.handlers.seed</code>: Fix randomness for reproducibility</strong></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a></a><span class="im">from</span> numpyro.handlers <span class="im">import</span> seed</span>
<span id="cb11-2"><a></a>seeded_model <span class="op">=</span> seed(model, rng_key)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong><code>numpyro.handlers.trace</code>: Inspect internal execution and sample sites</strong></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a></a><span class="im">from</span> numpyro.handlers <span class="im">import</span> trace</span>
<span id="cb12-2"><a></a>tr <span class="op">=</span> trace(model).get_trace()</span>
<span id="cb12-3"><a></a><span class="bu">print</span>(tr[<span class="st">"omega"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong><code>numpyro.handlers.condition</code>: Clamp a variable to observed or fixed value</strong></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a></a><span class="im">from</span> numpyro.handlers <span class="im">import</span> condition</span>
<span id="cb13-2"><a></a>conditioned_model <span class="op">=</span> condition(model, data<span class="op">=</span>{<span class="st">"omega"</span>: <span class="fl">0.3</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong><code>numpyro.handlers.substitute</code>: Replace variables with fixed values (e.g., MAP estimates)</strong></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a></a><span class="im">from</span> numpyro.handlers <span class="im">import</span> substitute</span>
<span id="cb14-2"><a></a>subbed_model <span class="op">=</span> substitute(model, data<span class="op">=</span>{<span class="st">"omega"</span>: <span class="fl">0.3</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong><code>numpyro.handlers.reparam</code>: Reparameterize a site to improve geometry</strong></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a></a><span class="im">from</span> numpyro.infer.reparam <span class="im">import</span> LocScaleReparam</span>
<span id="cb15-2"><a></a><span class="im">from</span> numpyro.handlers <span class="im">import</span> reparam</span>
<span id="cb15-3"><a></a></span>
<span id="cb15-4"><a></a>reparammed_model <span class="op">=</span> reparam(model, config<span class="op">=</span>{<span class="st">"z"</span>: LocScaleReparam()})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>This slide gives a quick preview of useful <code>numpyro.handlers</code> you’ll see in the notebooks.</p>
<ul>
<li><strong><code>seed</code></strong> lets you fix randomness for reproducibility — critical for testing or debugging.</li>
<li><strong><code>trace</code></strong> inspects your model’s internal execution and samples — great for seeing what actually gets sampled.</li>
<li><strong><code>condition</code></strong> clamps a variable to a fixed value — useful for simulating data from a known model.</li>
<li><strong><code>substitute</code></strong> is similar, but lets you plug in values like MAP estimates.</li>
<li><strong><code>reparam</code></strong> lets you reparameterize tricky sample sites to improve posterior geometry and sampling.</li>
</ul>
<p>We’ll practice using all of these interactively in the notebooks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="using-blackjax-and-numpyro" class="slide level2" style="font-size: 18px;">
<h2>Using BlackJax and NumPyro</h2>
<blockquote>
<p>BlackJax is NOT a PPL, so you need to combine it with a PPL like NumPyro or PyMC.</p>
</blockquote>
<div class="fragment fade-in" data-fragment-index="1">
<p><strong>Initialize model and extract the log-probability function</strong></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a></a>rng_key, init_key <span class="op">=</span> jax.random.split(rng_key)</span>
<span id="cb16-2"><a></a>init_params, potential_fn, <span class="op">*</span>_ <span class="op">=</span> initialize_model(</span>
<span id="cb16-3"><a></a>    init_key, model, model_args<span class="op">=</span>(x_data,), model_kwargs<span class="op">=</span>{<span class="st">"y"</span>: y_data}, dynamic_args<span class="op">=</span><span class="va">True</span></span>
<span id="cb16-4"><a></a>)</span>
<span id="cb16-5"><a></a></span>
<span id="cb16-6"><a></a>logdensity_fn <span class="op">=</span> <span class="kw">lambda</span> position: <span class="op">-</span>potential_fn(x_data, y<span class="op">=</span>y_data)(position)</span>
<span id="cb16-7"><a></a>initial_position <span class="op">=</span> init_params.z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<p><strong>Run warm-up to adapt step size and mass matrix using BlackJAX’s window adaptation</strong></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a></a>num_warmup <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb17-2"><a></a>adapt <span class="op">=</span> blackjax.window_adaptation(blackjax.nuts, logdensity_fn, target_acceptance_rate<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb17-3"><a></a>rng_key, warmup_key <span class="op">=</span> jax.random.split(rng_key)</span>
<span id="cb17-4"><a></a>(last_state, parameters), _ <span class="op">=</span> adapt.run(warmup_key, initial_position, num_warmup)</span>
<span id="cb17-5"><a></a>kernel <span class="op">=</span> blackjax.nuts(logdensity_fn, <span class="op">**</span>parameters).step</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<p><strong>Run BlackJAX NUTS sampling using <code>lax.scan</code></strong></p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a></a><span class="kw">def</span> run_blackjax_sampling(rng_key, state, kernel, num_samples<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb18-2"><a></a>    <span class="kw">def</span> one_step(state, key):</span>
<span id="cb18-3"><a></a>        state, info <span class="op">=</span> kernel(key, state)</span>
<span id="cb18-4"><a></a>        <span class="cf">return</span> state, state</span>
<span id="cb18-5"><a></a></span>
<span id="cb18-6"><a></a>    keys <span class="op">=</span> jax.random.split(rng_key, num_samples)</span>
<span id="cb18-7"><a></a>    _, samples <span class="op">=</span> jax.lax.scan(one_step, state, keys)</span>
<span id="cb18-8"><a></a>    <span class="cf">return</span> samples</span>
<span id="cb18-9"><a></a></span>
<span id="cb18-10"><a></a>samples <span class="op">=</span> run_blackjax_sampling(rng_key, last_state, kernel)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<p><strong>Convert BlackJAX output to ArviZ InferenceData</strong></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a></a>idata <span class="op">=</span> az.from_dict(posterior<span class="op">=</span>samples.position)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<aside class="notes">
<p>This slide shows how to use BlackJAX for sampling when you already have a model defined in NumPyro.</p>
<ul>
<li>First, we extract the <strong>log-probability function</strong> from a NumPyro model using <code>initialize_model</code>. This lets us use BlackJAX with the same model.</li>
<li>We define a <code>logdensity_fn</code>, which BlackJAX expects — it just wraps the potential function with a negative sign.</li>
<li>Next, we run <strong>adaptive warm-up</strong> with <code>blackjax.window_adaptation</code>. This tunes step size and mass matrix like NumPyro does.</li>
<li>Then, we sample using <code>blackjax.nuts</code> with a loop written using <code>jax.lax.scan</code> for speed and JIT compatibility.</li>
<li>Finally, we convert the raw samples to an ArviZ-friendly format with <code>az.from_dict</code>.</li>
</ul>
<p>Key point: BlackJAX is super fast and modular, but not a full PPL — so you bring your own model definition.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="sampler-comparison-table" class="slide level2" style="font-size: 26px;">
<h2>Sampler Comparison Table</h2>
<div class="solutionbox">
<div class="solutionbox-body" style="font-size: 18px;">
<table class="caption-top">
<colgroup>
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 6%">
<col style="width: 24%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Sampler</th>
<th>Library</th>
<th>Uses Gradient</th>
<th>Auto-Tuning</th>
<th>Rejection</th>
<th>Best For</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>MCMC (SA)</strong></td>
<td>NumPyro</td>
<td>❌</td>
<td>❌</td>
<td>✅</td>
<td>Simple low-dim models</td>
<td>No gradients; slow mixing</td>
</tr>
<tr class="even">
<td><strong>HMC</strong></td>
<td>NumPyro / BlackJAX</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>High-dim continuous posteriors</td>
<td>Needs tuned step size &amp; trajectory</td>
</tr>
<tr class="odd">
<td><strong>NUTS</strong></td>
<td>NumPyro / BlackJAX</td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>General-purpose inference</td>
<td>Adaptive HMC</td>
</tr>
<tr class="even">
<td><strong>MALA</strong></td>
<td>BlackJAX</td>
<td>✅</td>
<td>❌</td>
<td>✅</td>
<td>Local proposals w/ gradients</td>
<td>Stochastic gradient steps</td>
</tr>
<tr class="odd">
<td><strong>MCLMC</strong></td>
<td>BlackJAX</td>
<td>✅</td>
<td>✅ (via L)</td>
<td>❌</td>
<td>Large latent spaces</td>
<td>Unadjusted Langevin dynamics</td>
</tr>
<tr class="even">
<td><strong>Adj. MCLMC</strong></td>
<td>BlackJAX</td>
<td>✅</td>
<td>Manual (L)</td>
<td>✅</td>
<td>Bias-controlled Langevin sampler</td>
<td>Includes MH step</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>For more information check Simons et al.&nbsp;(2025), <a href="https://arxiv.org/pdf/2504.20130">§2.2.3, arXiv:2504.20130</a></p>
<aside class="notes">
<p><strong>Speaker Notes – Sampler Comparison Table</strong></p>
<p>This table gives a high-level overview of common samplers available in NumPyro and BlackJAX, organized by key features.</p>
<ul>
<li><p><strong>MCMC (SA)</strong>: This is standard Metropolis-Hastings — no gradients, no tuning, but it’s simple and useful for low-dimensional problems. Slow mixing is a drawback.</p></li>
<li><p><strong>HMC</strong>: Hamiltonian Monte Carlo improves mixing by using gradients to simulate physics-based trajectories. It requires tuning for step size and path length, so it’s more manual.</p></li>
<li><p><strong>NUTS</strong>: No-U-Turn Sampler is HMC with built-in auto-tuning. It adapts step size and path length during warm-up, making it the default choice for general-purpose inference in NumPyro and BlackJAX.</p></li>
<li><p><strong>MALA</strong>: Metropolis-Adjusted Langevin Algorithm uses gradients for local proposals. It’s a good middle ground — cheaper than full HMC, but more efficient than MH.</p></li>
<li><p><strong>MCLMC</strong>: Stochastic Langevin dynamics with <strong>no rejection</strong> step. Useful in large latent spaces, often in simulator-based models, but biased due to lack of correction.</p></li>
<li><p><strong>Adjusted MCLMC</strong>: Same as MCLMC, but now with a Metropolis-Hastings correction step. This helps reduce bias, at the cost of a little more computation.</p></li>
</ul>
<p><strong>Main idea:</strong> Pick the right sampler based on:</p>
<ul>
<li>whether you can compute gradients</li>
<li>dimensionality of the posterior</li>
<li>whether you need auto-tuning or not</li>
<li>whether you want unbiased samples (with rejection) or not.</li>
</ul>
<p>Final note: This is a simplified summary. For real applications, benchmarking is always best.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="examples-bayesian-inference-for-cosmology" class="title-slide slide level1 center">
<h1>Examples: Bayesian Inference for Cosmology</h1>

</section>
<section id="power-spectrum-inference-with-jax-cosmo" class="slide level2" style="font-size: 18px;">
<h2>Power Spectrum Inference with jax-cosmo</h2>
<div class="fragment fade-in" data-fragment-index="1">
<h4 id="step-1-simulate-cosmological-data">Step 1: Simulate Cosmological Data</h4>
<p><strong>Define a fiducial cosmology to generate synthetic observations</strong></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a></a>fiducial_cosmo <span class="op">=</span> jc.Planck15()</span>
<span id="cb20-2"><a></a>ell <span class="op">=</span> jnp.logspace(<span class="dv">1</span>, <span class="dv">3</span>)  <span class="co"># Multipole range for power spectrum</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Set up two redshift bins for galaxy populations</strong></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a></a>nz1 <span class="op">=</span> jc.redshift.smail_nz(<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">1.</span>)</span>
<span id="cb21-2"><a></a>nz2 <span class="op">=</span> jc.redshift.smail_nz(<span class="fl">1.</span>, <span class="fl">2.</span>, <span class="fl">0.5</span>)</span>
<span id="cb21-3"><a></a>nzs <span class="op">=</span> [nz1, nz2]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Define observational probes: weak lensing and number counts</strong></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a></a>probes <span class="op">=</span> [</span>
<span id="cb22-2"><a></a>    jc.probes.WeakLensing(nzs, sigma_e<span class="op">=</span><span class="fl">0.26</span>),</span>
<span id="cb22-3"><a></a>    jc.probes.NumberCounts(nzs, jc.bias.constant_linear_bias(<span class="fl">1.</span>))</span>
<span id="cb22-4"><a></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Generate synthetic data using the fiducial cosmology</strong></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a></a>mu, cov <span class="op">=</span> jc.angular_cl.gaussian_cl_covariance_and_mean(fiducial_cosmo, ell, probes)</span>
<span id="cb23-2"><a></a>rng_key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb23-3"><a></a>noise <span class="op">=</span> jax.random.multivariate_normal(rng_key, jnp.zeros_like(mu), cov)</span>
<span id="cb23-4"><a></a>data <span class="op">=</span> mu <span class="op">+</span> noise  <span class="co"># Fake observations</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<h4 id="step-2-define-the-numpyro-model">Step 2: Define the NumPyro Model</h4>
<div class="sourceCode" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a></a><span class="co"># Define a NumPyro probabilistic model to infer cosmological parameters</span></span>
<span id="cb24-2"><a></a><span class="kw">def</span> model(data):</span>
<span id="cb24-3"><a></a>    Omega_c <span class="op">=</span> numpyro.sample(<span class="st">"Omega_c"</span>, dist.Uniform(<span class="fl">0.1</span>, <span class="fl">0.5</span>))</span>
<span id="cb24-4"><a></a>    sigma8 <span class="op">=</span> numpyro.sample(<span class="st">"sigma8"</span>, dist.Uniform(<span class="fl">0.6</span>, <span class="fl">1.0</span>))</span>
<span id="cb24-5"><a></a>    </span>
<span id="cb24-6"><a></a>    <span class="co"># Forward model: compute theoretical prediction given parameters</span></span>
<span id="cb24-7"><a></a>    cosmo <span class="op">=</span> jc.Planck15(Omega_c<span class="op">=</span>Omega_c, sigma8<span class="op">=</span>sigma8)</span>
<span id="cb24-8"><a></a>    mu, cov <span class="op">=</span> jc.angular_cl.gaussian_cl_covariance_and_mean(cosmo, ell, probes)</span>
<span id="cb24-9"><a></a>    </span>
<span id="cb24-10"><a></a>    <span class="co"># Likelihood: multivariate Gaussian over angular power spectra</span></span>
<span id="cb24-11"><a></a>    numpyro.sample(<span class="st">"obs"</span>, dist.MultivariateNormal(mu, cov), obs<span class="op">=</span>data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<aside class="notes">
<p>This slide introduces a basic end-to-end inference workflow using <code>jax-cosmo</code> and <code>NumPyro</code>.</p>
<p><strong>Step 1: Simulate synthetic cosmological data</strong></p>
<ul>
<li>We start with a fiducial cosmology using <code>jc.Planck15()</code>.</li>
<li>Define a multipole range (<code>ell</code>) and two redshift distributions for tomographic bins.</li>
<li>Create weak lensing and number count probes.</li>
<li>Then, using the angular power spectrum mean and covariance from <code>jax_cosmo</code>, we generate mock observations by adding Gaussian noise.</li>
</ul>
<p><strong>Step 2: Define the NumPyro model</strong></p>
<ul>
<li>The model samples two parameters: <code>Omega_c</code> and <code>sigma8</code>.</li>
<li>A new cosmology object is constructed with those parameters.</li>
<li>Then the power spectrum prediction is computed and matched to data with a multivariate Gaussian likelihood.</li>
</ul>
<p>This setup mirrors what we do in practice: forward-model observables, simulate noisy data, and recover parameters via inference.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="full-field-inference-with-forward-models" class="slide level2" style="font-size: 18px;">
<h2>Full Field Inference with Forward Models</h2>
<div class="columns">
<div class="column" style="width:60%;">
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<p><strong>Bayesian Inference using power spectrum data:</strong></p>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<p><strong>Bayesian Inference using full field data:</strong></p>
</div>
</div>
<div class="r-stack">
<div class="fragment fade-in-then-out" data-fragment-index="1">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/bayes_pipeline_latent.svg" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<div class="fragment fade-out" data-fragment-index="4">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/FFI_full.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="assets/bayes/FFI_full_tuto.svg" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="width:40%;">
<div class="fragment fade-in" data-fragment-index="1">
<ul>
<li><strong>Recap:</strong> Bayesian inference maps theory + data → posterior</li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="2">
<ul>
<li><strong>Cosmological Forward models</strong>
<ul>
<li>Start from cosmological + latent parameters</li>
<li>Sample initial conditions</li>
<li>Evolve using <strong>N-body simulations</strong></li>
<li><strong>Predict convergence maps</strong> in tomographic bins</li>
</ul></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="3">
<ul>
<li><strong>Simulation-Based Inference</strong>
<ul>
<li>Compare predictions to real <strong>survey maps</strong></li>
<li>Build a <strong>likelihood</strong> from the forward model</li>
<li>Infer cosmological parameters from <strong>full field data</strong></li>
</ul></li>
</ul>
</div>
<div class="fragment fade-in" data-fragment-index="4">
<div class="solutionbox">
<div class="solutionbox-header" style="font-size: 19px;">
<p><strong>Full Field vs.&nbsp;Summary Statistics</strong></p>
</div>
<div class="solutionbox-body" style="font-size: 16px;">
<ul>
<li>Preserves <strong>non-Gaussian</strong> structure lost in summaries</li>
<li>Enables tighter constraints in <strong>nonlinear regimes</strong></li>
<li>Especially useful in <strong>high-dimensional</strong> inference problems</li>
<li>See: <em>Zeghal et al.&nbsp;(2024), Leclercq et al.&nbsp;(2021)</em></li>
<li>🔜 a talk on this topic this Thursday</li>
</ul>
</div>
</div>
</div>
</div></div>
<aside class="notes">
<p><strong>Speaker Notes for Final Slide (Full Field Inference):</strong></p>
<ul>
<li><p>We now shift to the most general and flexible form of inference — using <strong>full field data</strong>.</p></li>
<li><p>This means we <strong>don’t extract summary statistics</strong> like <span class="math inline">\(C_\ell\)</span> — instead, we model the forward process end-to-end, including simulations.</p></li>
<li><p>The green box shows the <strong>core components</strong> of this forward model pipeline:</p>
<ul>
<li>Start by <strong>sampling cosmological + latent parameters</strong> (e.g., initial conditions)</li>
<li>Use an <strong>N-body simulation</strong> to evolve structure</li>
<li>Predict observables (e.g., weak lensing convergence maps)</li>
<li>Compare the simulated maps to real observations to construct a <strong>likelihood</strong></li>
</ul></li>
<li><p>This approach <strong>preserves non-Gaussian information</strong>, which is critical in the nonlinear regime.</p></li>
<li><p>We’ll focus specifically on the contents of the green box in the hands-on notebook:</p>
<ul>
<li>Sampling initial fields</li>
<li>Running a small-scale N-body simulation</li>
<li>Building a likelihood from simulation output</li>
</ul></li>
<li><p>It’s an extremely powerful method, and the frontier of cosmological inference.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="conclusion" class="title-slide slide level1 center">
<h1>Conclusion</h1>

</section>
<section id="conclusion-why-bayesian-inference" class="slide level2">
<h2>Conclusion: Why Bayesian Inference?</h2>
<p><br> <br></p>
<h3 id="key-takeaways">🔑 Key Takeaways</h3>
<ul>
<li><p><strong>Bayesian modeling</strong> lets you build flexible, extensible pipelines — from analytical likelihoods to full forward simulations.</p></li>
<li><p>The <strong>JAX ecosystem</strong> (NumPyro, BlackJAX, jax-cosmo…) lets you focus on <strong>modeling</strong>, not math details.</p></li>
<li><p><strong>Gradients + differentiable simulators</strong> scale inference to complex, high-dimensional problems — efficiently and transparently.</p></li>
<li><p>Bayesian tools are now mature, fast, and usable — even for large cosmological models.</p></li>
</ul>
<div class="solutionbox">
<div class="solutionbox-header" style="font-size: 20px;">
<p><strong>Hands-On Notebooks:</strong></p>
</div>
<div class="solutionbox-body" style="font-size: 18px;">
<ul>
<li>Beginner Bayesian Inference with NumPyro &amp; Blackjax <a href="https://github.com/ASKabalan/Tutorials/blob/main/BDL2025/Exercises/01_Beginner.ipynb">here</a></li>
<li>Intermediate Bayesian Inference with NumPyro &amp; Blackjax <a href="https://github.com/ASKabalan/Tutorials/blob/main/BDL2025/Exercises/02_Intermediate.ipynb">here</a></li>
<li>some of the animation were made using this <a href="https://github.com/ASKabalan/Tutorials/blob/main/BDL2025/illustrations/HMC_vs_MCMC.ipynb">notebook</a></li>
</ul>
</div>
</div>
<h4 id="thank-you-for-your-attention">Thank you for your attention!</h4>
<aside class="notes">
<p><strong>Speaker Notes – Conclusion Slide</strong></p>
<ul>
<li><p>Let’s wrap up with some key takeaways on <strong>why Bayesian inference matters</strong>, especially in cosmology:</p>
<ul>
<li><p>First, <strong>Bayesian modeling is modular and flexible</strong>. It gives us a way to go from simple analytic models to full simulator-based pipelines — all under one framework.</p></li>
<li><p>The <strong>JAX ecosystem</strong> — NumPyro, BlackJAX, jax-cosmo — gives you a complete toolchain. You don’t have to worry about math-heavy derivations or custom samplers. You can focus on the science and modeling.</p></li>
<li><p>A major strength is the ability to leverage <strong>gradients and differentiable simulators</strong>. That means you can scale inference even for models with thousands or millions of parameters — like field-level inference.</p></li>
<li><p>And importantly, the tools are <strong>mature and fast</strong>. These aren’t just research toys — they’re ready for real problems.</p></li>
</ul></li>
<li><p>Now it’s your turn. You’ll have access to <strong>two hands-on notebooks</strong>:</p>
<ul>
<li>One walks you through Bayesian regression and cosmological inference with NumPyro.</li>
<li>The other dives into field-level inference and simulation-based modeling.</li>
</ul></li>
<li><p>This is where things become tangible — you’ll code your own inference pipeline, simulate structure formation, and run real MCMC samplers.</p></li>
<li><p>That’s the power of combining <strong>Bayesian ideas</strong> with <strong>modern tools</strong> — and that’s where the future of cosmological inference is headed.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p>Bayesian Deep Learning Workshop , 2025</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'slide',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
            const codeEl = trigger.previousElementSibling.cloneNode(true);
            for (const childEl of codeEl.children) {
              if (isCodeAnnotation(childEl)) {
                childEl.remove();
              }
            }
            return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/ASKabalan\.github\.io\/slides\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>