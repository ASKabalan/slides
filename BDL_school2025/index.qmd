---
title: '<span style="color:#ffffff; font-size: largest;">Bayesian Inference for Cosmology with JAX</span>'

author: 
  - name: "<span style='color:#ffffff; font-size: larger;'>Wassim Kabalan</span>"
  - name : "<span style='color:#ffffff; font-size: Smaller;'>Alexandre Boucaud, Fran√ßois Lanusse</span>"
footer: "Bayesian Deep Learning Workshop , 2025"
format:
  revealjs:
    theme: [default, css/custom.scss]
    incremental: false   
    transition: slide
    background-transition: slide
    presentation-size: max-scale
    highlight-style: github
    slide-number: true
    template-partials:
      - css/title-slide.html
output: revealjs

title-slide-attributes:
  data-background-image: "assets/titles/bayes_title_1.png"
  data-background-size: fill
  data-background-opacity: "0.8"


logo1 : '
<div style="display: flex; justify-content: space-around; align-items: center; layout-valign="middle">
  <img src="assets/Logos/AstroDeep-2.png" style="width: 35%;"/>
  <img src="assets/Logos/APC.png" style="width: 20%;"/>
  <img src="assets/Logos/scipol.png" style="width: 35%;"/>
</div>
'
---

## Goals for This Presentation 

:::{.solutionbox}

::::{.solutionbox-body style="font-size: 22px; border-radius: 10px; border: 2px solid #3b0a68;"}

- <span style="color:#3b0a68; font-size: 26px;">**Understand Cosmological Inference**</span>: Learn how we go from observations to cosmological parameters.
<br /><br />
- <span style="color:#3b0a68; font-size: 26px;">**From œá¬≤ to Bayesian Inference**</span>: See how Bayesian modeling generalizes classical approaches.
<br /><br />
- <span style="color:#3b0a68; font-size: 26px;">**Learn Forward Modeling and Hierarchical Models**</span>: Understand generative models and field-level inference.
<br /><br />
- <span style="color:#3b0a68; font-size: 26px;">**Explore Modern Tools (JAX, NumPyro, BlackJAX)**</span>: Use practical libraries for scalable inference.
<br /><br />
- <span style="color:#3b0a68; font-size: 26px;">**Prepare for Hands-On Notebooks**</span>: Apply Bayesian techniques in real examples using JAX.

::::

:::


::: {.notes}



"Let me walk you through what we‚Äôre aiming to cover today."

* **First**, we'll build an understanding of **cosmological inference** ‚Äî how we move from raw observational data to constraints on cosmological parameters. This includes both the intuition and the mathematical machinery behind it.

* **Second**, we'll see how **Bayesian inference** generalizes the classical approach. Instead of just optimizing a œá¬≤, we model uncertainty and latent structure more fully.

* **Third**, we‚Äôll dive into **forward modeling** and **hierarchical models**. These are especially relevant in modern cosmology where we simulate the full data generation process and marginalize over latent variables.

* Then we‚Äôll explore some of the **modern tools** that make all of this practical: **JAX** for fast, differentiable computing; **NumPyro** for probabilistic programming; and **BlackJAX** for flexible sampling.

* Finally, the goal is for you to leave prepared for the **hands-on notebooks**, where you‚Äôll implement and explore real inference pipelines using JAX-based tools.

The goal of this is to able to start doing bayesian inference ..
so if you have any questions, please ask them during the presentation. 

:::


# Background :  Inference in Cosmology: The Big Picture 

---

## Inference in Cosmology: The Frequentist Pipeline {style="font-size: 21px;"}


:::{.columns}

:::: {.column width="70%"}

<br/>

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/bayes/freq_pipeline_0.svg){fig-align="center" width="100%"}

::::::

:::::: {.fragment fragment-index=2 .fade-in-then-out}

![](assets/bayes/freq_pipeline_1.svg){fig-align="center" width="100%"}

::::::

:::::: {.fragment fragment-index=3 .fade-in-then-out}

![](assets/bayes/freq_pipeline_2.svg){fig-align="center" width="100%"}

::::::

:::::: {.fragment fragment-index=4 .fade-in-then-out}

![](assets/bayes/freq_pipeline_3.svg){fig-align="center" width="100%"}

::::::

:::::: {.fragment fragment-index=5 .fade-in}

![](assets/bayes/freq_pipeline_4.svg){fig-align="center" width="100%"}

::::::

:::::

::::

:::: {.column width="30%"}


:::::: {.fragment fragment-index=1 .fade-in}
- **cosmological parameters** (Œ©): matter density, dark energy, etc.
::::::

:::::: {.fragment fragment-index=2 .fade-in}
- Predict observables: **CMB, galaxies, lensing**
::::::

:::::: {.fragment fragment-index=3 .fade-in}
- Extract **summary statistics**: $P(k)$, $C_\ell$ , 2PCF
::::::

:::::: {.fragment fragment-index=4 .fade-in}
- Compute **likelihood**: $L(\Omega \vert data)$
::::::

:::::: {.fragment fragment-index=5 .fade-in}
- Estimate $\hat{\Omega}$ via **maximization** ($\chi^2$ fitting)
::::::
:::::: {.fragment fragment-index=6 .fade-in}


:::{.solutionbox}

::: {.solutionbox-header style="font-size: 20px;"}

**Frequentist Toolbox**

:::

::::{.solutionbox-body style="font-size: 18px;"}

- $\chi^2$ analysis
- 2-point correlation function (2PCF)
- Power spectrum fitting: $P(k)$, $C_\ell$ 

::::

:::

:::::: 

::::

:::


:::{.notes}


"This is the traditional approach many of you are already familiar with ‚Äî the frequentist pipeline."

* We start with a set of **cosmological parameters**, denoted here as **Œ©** ‚Äî this could include things like the matter density, dark energy equation of state, etc.

* These parameters are used to predict observable quantities, such as the **CMB power spectrum**, **galaxy clustering**, or **weak lensing shear**.

* From the data, we extract **summary statistics** ‚Äî typically things like **2-point correlation functions**, **power spectra** (P(k), C\_‚Ñì), or other reduced forms of the data.

* Then, we compute a **likelihood** ‚Äî usually assuming a Gaussian form for the summary statistics ‚Äî and estimate parameters by **maximizing this likelihood**. That gives us a point estimate **Œ©ÃÇ**.

* This pipeline works well when:

  * You have a reliable analytic likelihood,
  * The summary statistics are informative,
  * And the assumptions (e.g. Gaussianity, linear regime) hold.

* The box in the lower right shows what‚Äôs typically in the frequentist toolbox ‚Äî **œá¬≤ fitting**, **2PCF**, and so on.

"This pipeline is efficient and interpretable ‚Äî but as we‚Äôll see, it has limitations when you move beyond simple, low-dimensional, or linear models."


:::

---

## Inference in Cosmology: The Bayesian Pipeline {style="font-size: 20px;"}


:::{.columns}

:::: {.column width="70%"}

<br/>

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/bayes/freq_pipeline_2.svg){fig-align="center" width="100%"}

::::::

:::::: {.fragment fragment-index=2 .fade-in-then-out}

![](assets/bayes/bayes_pipeline_0.svg){fig-align="center" width="100%"}

::::::

:::::: {.fragment fragment-index=3 .fade-in-then-out}

![](assets/bayes/bayes_pipeline_1.svg){fig-align="center" width="100%"}

::::::

:::::: {.fragment fragment-index=4 .fade-in}

![](assets/bayes/bayes_pipeline_2.svg){fig-align="center" width="100%"}

::::::

:::::

::::

:::: {.column width="30%"}


:::::: {.fragment fragment-index=1 .fade-in}
- Start from **summary statistics**: $P(k)$, $C_\ell$ , 2PCF
::::::

:::::: {.fragment fragment-index=2 .fade-in}
- Sample from a **Prior** $P(\Omega)$
::::::

:::::: {.fragment fragment-index=3 .fade-in}
- Compute **likelihood**: $L(Obs \vert \Omega)$
::::::

:::::: {.fragment fragment-index=4 .fade-in}
- Sampler from the **Posterior** $P(\Omega \vert Obs)$
::::::



:::::: {.fragment fragment-index=6 .fade-in}


:::{.solutionbox}

::: {.solutionbox-header style="font-size: 20px;"}

**Bayesian Toolbox**

:::

::::{.solutionbox-body style="font-size: 18px;"}

- Priors encode beliefs: $P(\Omega)$  
- Hierarchical Bayesian Modeling (HBM)
- Probabilistic programming (e.g., **NumPyro**)
- Gradient-based samplers: **HMC**, **NUTS**

::::

:::


:::::: 

::::

:::


:::{.notes}



Start by emphasizing that, like the frequentist pipeline, the Bayesian version also begins with summary statistics like power spectra.
But instead of finding a best-fit point estimate, the Bayesian approach defines a **probabilistic model** over cosmological parameters.

Now show image 2:

* We sample from a **prior** over parameters, combine it with a **likelihood** based on the observed data, and use this to compute the **posterior**.
* This posterior captures all uncertainty, correlations, and degeneracies.
* The Bayesian toolbox lets us extend models easily ‚Äî hierarchical structure, latent fields, flexible priors ‚Äî and perform inference using modern tools like NumPyro and gradient-based samplers (e.g. NUTS, HMC).


:::

---




## Inference in Cosmology: The Bayesian Pipeline {style="font-size: 18px;" auto-animate=true}


:::{.columns}

:::: {.column width="70%"}

<br/>

![](assets/bayes/bayes_pipeline_latent.svg){fig-align="center" width="100%"}

::::

:::: {.column width="30%"}


:::::: {.fragment fragment-index=1 .fade-in}
* **Prior**: Theory-driven assumptions $P(\Omega)$
::::::
:::::: {.fragment fragment-index=2 .fade-in}
* **Latent variables**: Hidden/unobserved $z \sim P(z \mid \Omega)$
::::::
:::::: {.fragment fragment-index=3 .fade-in}
* **Likelihood**: Generates observables $P(\text{Obs} \mid \Omega, z)$
::::::
:::::: {.fragment fragment-index=4 .fade-in}
* **Posterior**: infer $P(\Omega \mid \text{Obs})$
::::::

::::

:::


## Inference in Cosmology: The Bayesian Pipeline {style="font-size: 18px;" auto-animate=true}

:::{.columns}

:::: {.column width="10%"}

<br/>

![](assets/bayes/bayes_pipeline_latent.svg){fig-align="middle" width="100%"}

::::

:::: {.column width="90%"}




**Bayes‚Äô Rule with all components:**

> Full decomposition of the posterior. The denominator marginalizes over all possible parameters.

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}

$$
\underbrace{P(\Omega \mid \text{Obs})}_{\text{Posterior}}
= \frac{
    \underbrace{P(\text{Obs} \mid \Omega)}_{\text{Likelihood}} 
    \cdot 
    \underbrace{P(\Omega)}_{\text{Prior}}
}{
    \underbrace{
        \int P(\text{Obs} \mid \Omega) P(\Omega) \, d\Omega
    }_{\text{Evidence}}
}
$$

::::::
:::::: {.fragment fragment-index=2 .fade-in}

$$
\underbrace{P(\Omega \mid \text{Obs})}_{\text{Posterior}}
= \frac{
    \underbrace{\int P(\text{Obs} \mid \Omega, z)\, P(z \mid \Omega)\, dz}_{\text{Likelihood (marginalized over latent $z$)}} 
    \cdot 
    \underbrace{P(\Omega)}_{\text{Prior}}
}{
    \underbrace{
        \int \left[ \int P(\text{Obs} \mid \Omega, z)\, P(z \mid \Omega)\, dz \right] P(\Omega)\, d\Omega
    }_{\text{Evidence}}
}
$$

::::::
::::

::::

:::




::: {.columns}

:::: {.column width="50%"}

:::::: {.fragment fragment-index=3 .fade-in}

> In practice, we drop the evidence term when sampling ‚Äî it‚Äôs a constant.

$$
P(\Omega \mid \text{Obs}) 
\propto 
\underbrace{\int P(\text{Obs} \mid \Omega, z)\, P(z \mid \Omega) \, dz}_{\text{Marginal Likelihood}} 
\cdot 
\underbrace{P(\Omega)}_{\text{Prior}}
$$

::::::

:::::: {.fragment fragment-index=4 .fade-in}

$$
\log P(\Omega \mid \text{Obs}) 
= \log P(\text{Obs} \mid \Omega) + \log P(\Omega)
$$

::::::

::::

::::: {.column width="50%"}

:::::: {.fragment fragment-index=5 .fade-in}

:::{.solutionbox}

::: {.solutionbox-header style="font-size: 17px;"}

**Bayes‚Äô Rule in Practice**

:::

::::{.solutionbox-body style="font-size: 15px;"}

* The **posterior** combines theory (prior) and data (likelihood) to infer cosmological parameters.
* **Latent variables** $z$ encode hidden structure (e.g., initial fields, nuisance parameters).
* The **evidence** is often ignored during sampling (it‚Äôs constant).
* **Model comparison** via the Bayes Factor:

  $$
  \text{Bayes Factor} = \frac{P(\text{Obs} \mid \mathcal{M}_1)}{P(\text{Obs} \mid \mathcal{M}_2)}
  $$

::::

:::

::::::

::::

:::


::: {.notes}

**Speaker Notes for This Sequence:**

---

**STEP 1:**



We now extend the Bayesian pipeline by introducing **latent variables** ‚Äî denoted *z*.
These are hidden, unobserved quantities such as initial conditions, noise fields, or instrumental effects.

* The **prior** encodes our belief over cosmological parameters Œ©.
  It's **unobserved**, **unknown**, and is the **target of inference**.

* The **latent variables** *z* are also **unobserved** and **unknown**, but they are **conditional on the prior** ‚Äî
  they depend on Œ© and are integrated out (marginalized) during inference.

* The **likelihood** is a forward model that generates observables given both the prior and latent structure:
  $\mathcal{L}(\text{Obs} \mid \Omega, z)$

* The **posterior** combines all of these:
  it tells us how probable different cosmological parameters are, given the data and the model structure:
  $P(\Omega \mid \text{Obs}) \propto \int \mathcal{L}(\text{Obs} \mid \Omega, z) \, P(z \mid \Omega) \, dz \cdot P(\Omega)$

This hierarchical view is what powers modern cosmological inference.




**STEP 2:**

Here's a polished version of your speaker notes for that section:

---

We now move to the full **Bayesian formula** ‚Äî starting without latent variables:

This gives us the **posterior** as the product of the **likelihood** and the **prior**, normalized by the **evidence**.
The evidence $P(\text{Obs})$ ensures the posterior is a proper probability distribution.

Now, when we introduce **latent variables** $z$, the likelihood itself becomes an integral over those:

This marginal likelihood accounts for the full hidden structure.


In practice, we **ignore the evidence** term when sampling:

* It‚Äôs **constant for a given model** ‚Äî so it doesn‚Äôt affect posterior shape.
* It‚Äôs **computationally expensive** to compute (requires full integration).
* But: it‚Äôs **very useful for comparing models**.

---

**Summary**:

* We sample directly from the **posterior**, which combines **prior** and **likelihood**.
* **Latent variables** model hidden or uncertain structure ‚Äî like initial conditions.
* The **evidence** is dropped during sampling, but becomes important in **model comparison** using the **Bayes Factor**:

This tells us which model is better supported by the data.



:::

---

## Two Roads to Inference: Frequentist and Bayesian {style="font-size: 20px;" }


![](assets/bayes/freq_vs_bayes.png){.absolute top=-50 left=900 width="20%"}

::: aside
@image credit:Wayne Stewart 
:::

:::{.solutionbox}

::::{.solutionbox-header style="font-size: 20px;"}
**Conceptual Differences**
::::

::: {.solutionbox-body style="font-size: 18px;"}



| **Concept**          | **Frequentist**                                             | **Bayesian**                                               |
| -------------------- | ----------------------------------------------------------- | ---------------------------------------------------------- |
| **Parameters**       | **Fixed** but unknown                                       | **Random variables** with a prior                          |
| **Goal**             | **Point estimate** (e.g. MLE)                               | **Full distribution** (posterior over parameters)          |
| **Uncertainty**      | From **data variability**                                   | From **parameter uncertainty** (posterior)                 |
| **Prior Knowledge**  | **Not used**                                                | **Explicitly included** via prior $P(\Omega)$            |
| **Interval Meaning** | **Confidence interval**: ‚Äú95% of experiments contain truth‚Äù | **Credible interval**: ‚Äú95% chance truth is in this range‚Äù |
| **Likelihood Role**  | Central in **$\chi^2$ minimization**, fits                  | Combined with **prior** to form posterior                  |
| **Inference Output** | **Best-fit estimate** + error bars                          | **Posterior distribution**                                 |
| **Tooling**          | **Optimization** (e.g. œá¬≤, maximum likelihood)              | **Sampling** (e.g. MCMC, HMC, NUTS)                        |

:::

:::

::: {.columns}

:::: {.column width="90%"}

Although these approaches are often contrasted, **they‚Äôre not mutually exclusive**.
Modern workflows ‚Äî like **causal inference** in [*Statistical Rethinking*](https://www.youtube.com/watch?v=FdnMWdICdRs) ‚Äî draw on both perspectives.
Bayesian methods offer a formal way to **combine theory and data**, especially powerful when simulations are involved.

::::

:::: {.column width="10%"}

![Statistical Rethinking](assets/bayes/stat_rethink.jpg){fig-align="center" width="100%"}

::::

::::

<br/>

::: {.notes}

This slide lays out a direct comparison between the **frequentist** and **Bayesian** approaches.

We're highlighting not just the philosophical differences, but also the **practical consequences**.

A few key contrasts to emphasize:

* **Parameters**: In frequentist stats, parameters are fixed but unknown. In Bayesian stats, they‚Äôre **random variables** ‚Äî we assign distributions to represent our uncertainty.

* **Goal**: Frequentists usually aim for a **point estimate** (like the MLE). Bayesians aim to recover the **entire posterior distribution**.

* **Uncertainty**: Frequentists focus on **data variability** ‚Äî uncertainty from random samples. Bayesians focus on **parameter uncertainty** ‚Äî how uncertain we are about the parameters given the data.

* **Intervals**: The interpretations are totally different. A frequentist says, ‚Äú95% of the time, this interval contains the truth.‚Äù A Bayesian says, ‚ÄúThere‚Äôs a 95% chance the truth is in this interval.‚Äù

* **Tooling**: Optimization vs. sampling. Frequentists often rely on **curve fitting**, minimization, etc. Bayesian workflows rely on **sampling** (MCMC, HMC, NUTS).

Make sure the audience sees this isn‚Äôt about choosing sides. As the bottom note says ‚Äî they‚Äôre **not mutually exclusive**. A lot of modern workflows combine both perspectives.


:::


# üõ†Ô∏è The Mechanics of Inference  

## Sampling the Posterior: The Core Loop {style="font-size: 20px;" }

:::{.columns}

:::: {.column width="50%"}

![](assets/bayes/inference_loop.svg){fig-align="center" width="75%"}
::::


:::: {.column width="50%"}

**The Sampling Loop:**

:::::: {.fragment fragment-index=1 .fade-in}
- Start from a sample $(\Omega^t, z^t)$  
::::::
:::::: {.fragment fragment-index=2 .fade-in}
- Propose new sample $(\Omega', z')$
::::::
:::::: {.fragment fragment-index=3 .fade-in}
- Compute **acceptance probability**
::::::
:::::: {.fragment fragment-index=4 .fade-in}
- Accept or reject proposal
::::::
:::::: {.fragment fragment-index=5 .fade-in}
- Repeat and store accepted samples ‚ü∂ **posterior**
::::::

:::::: {.fragment fragment-index=6 .fade-in}
**Goal:** Explore the full shape of the posterior  
(even in high-dim, non-Gaussian spaces)
::::::


:::::: {.fragment fragment-index=7 .fade-in}
::: {.solutionbox}

::::{.solutionbox-header style="font-size: 20px;"}
**Key Takeaways**
::::

::::{.solutionbox-body style="font-size: 18px;"}

* Most samplers follow this **accept/reject loop**
* Differ by how they propose samples:
  ‚Äì Random walk (e.g., MH)
  ‚Äì Gradient-guided (e.g., HMC, NUTS)
* Some skip rejection (e.g., Langevin, VI)

::::

:::

::::::


::::
:::

::: {.notes}
This slide illustrates the core mechanism behind most MCMC samplers ‚Äî the accept/reject loop.

We start with a current sample from the posterior, say $(\Omega^t, z^t)$.
The sampler then proposes a new point $(\Omega', z')$, using some rule ‚Äî it might be a random walk, or it might use gradients like in HMC or NUTS.

Next, we compute the **acceptance probability** ‚Äî this depends on how likely the new sample is under the posterior compared to the current one.

Then we make a decision:

* If the new sample is more likely (or meets some acceptance threshold), we accept it and add it to the chain.
* If not, we reject it and store the current one again.

This process repeats to build a chain of samples. The accepted ones collectively approximate the posterior distribution.

On the right, the key takeaways:

* Most samplers use this loop.
* The difference lies in how they generate proposals ‚Äî basic methods use random walks, while advanced methods use gradients.
* Some newer algorithms avoid rejection altogether ‚Äî like variational inference or some Langevin-based flows ‚Äî but the classic accept/reject structure remains fundamental to many MCMC approaches.


:::

---

### Sampling Algorithms at a Glance {style="font-size: 20px;" }

:::{.columns}

:::: {.column width="65%"}

:::::: {.fragment fragment-index=1 .fade-in}


**Metropolis-Hastings (MCMC)**

* **Propose**: Random walk
  $\Omega' \sim \mathcal{N}(\Omega^t, \sigma^2)$
* **Accept**:

  $$
  \alpha = \min\left(1, \frac{P(\text{Obs} \mid \Omega') P(\Omega')}{P(\text{Obs} \mid \Omega^t) P(\Omega^t)}\right)
  $$
::::::
:::::: {.fragment fragment-index=2 .fade-in}


**Hamiltonian Monte Carlo (HMC)**

* **Propose**: Simulate physics
  Trajectory via gradients $\nabla\_\Omega \log P(\text{Obs} \mid \Omega)$
* **Accept**:
  Based on Hamiltonian energy conservation.
  $\alpha = \min(1, e^{\mathcal{H}(\Omega^t, p^t) - \mathcal{H}(\Omega', p')})$
::::::
:::::: {.fragment fragment-index=3 .fade-in}


**NUTS (No-U-Turn Sampler)**
  Same as HMC, but auto-tunes:

  * Step size
  * Trajectory length (stops before looping back)
::::::

::::

:::: {.column width="35%"}

:::::: {.fragment fragment-index=1 .fade-in}
![](assets/Samplers/MCMC.gif){fig-align="center" width="80%" style="border: 2px solid black; padding: 2px;"}
::::::
:::::: {.fragment fragment-index=2 .fade-in}
![](assets/Samplers/HMC.gif){fig-align="center" width="80%" style="border: 2px solid black; padding: 2px;"}
::::::

:::::: {.fragment fragment-index=3 .fade-in}
![](assets/Samplers/NUTS.gif){fig-align="center" width="80%" style="border: 2px solid black; padding: 2px;"}
::::::


::::

:::

<br/>
<br/>

::: aside
@credit: https://github.com/chi-feng/mcmc-demo
:::

::: {.notes}


This slide gives a quick overview of three core MCMC algorithms.

We start with **Metropolis-Hastings (MH)**:
It proposes a new sample using a simple random walk ‚Äî typically from a Normal centered on the current value.
The acceptance probability is the ratio of posteriors ‚Äî new over old ‚Äî and we accept based on how much better the new sample fits.
You can see on the diagram: it just takes a small step and checks whether to keep it.

Next, **Hamiltonian Monte Carlo (HMC)**:
Rather than proposing random jumps, HMC uses gradients to simulate a physical trajectory through parameter space ‚Äî like a particle rolling through a potential landscape.
This allows it to make larger moves that still preserve the posterior distribution.
Acceptance here is based on energy conservation, using a Hamiltonian formulation.

Finally, **NUTS (No-U-Turn Sampler)**:
This builds on HMC, but it adds smart tuning:

* It automatically adjusts step size
* It stops the trajectory before looping back on itself ‚Äî hence ‚ÄúNo-U-Turn‚Äù
  This makes NUTS a great default sampler in most PPLs ‚Äî it avoids a lot of manual tuning and works well out of the box.

Together, these illustrate a spectrum: from simple MH to advanced gradient-based samplers ‚Äî and show how modern samplers make use of geometry to efficiently explore high-dimensional spaces.

:::



## Gradient-Based Sampling in Action  {style="font-size: 20px;" auto-animate=true}


:::{.columns}

:::: {.column width="50%"}

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}

![HMC: Gaussian Posterior](assets/Samplers/gaussian_hmc.gif){fig-align="center" width="50%"}

::::::

:::::: {.fragment fragment-index=2 .fade-in}
![HMC: Gaussian Posterior](assets/Samplers/gaussian_hmc_density.png){fig-align="center" width="50%"}
::::::
:::::

::::: {.r-stack}

:::::: {.fragment fragment-index=3 .fade-in-then-out}

![HMC: Banana Posterior](assets/Samplers/banana_hmc.gif){fig-align="center" width="50%"}

::::::

:::::: {.fragment fragment-index=4 .fade-in}
![HMC: Banana Posterior](assets/Samplers/banana_hmc_density.png){fig-align="center" width="50%"}
::::::
:::::


::::

:::: {.column width="50%"}

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}

![MCMC: Gaussian Posterior](assets/Samplers/gaussian_mcmc.gif){fig-align="center" width="50%"}

::::::

:::::: {.fragment fragment-index=2 .fade-in}
![MCMC: Gaussian Posterior](assets/Samplers/gaussian_mcmc_density.png){fig-align="center" width="50%"}
::::::
:::::

::::: {.r-stack}

:::::: {.fragment fragment-index=3 .fade-in-then-out}

![MCMC: Banana Posterior](assets/Samplers/banana_mcmc.gif){fig-align="center" width="50%"}

::::::

:::::: {.fragment fragment-index=4 .fade-in}
![MCMC: Banana Posterior](assets/Samplers/banana_mcmc_density.png){fig-align="center" width="50%"}
::::::
:::::


::::
:::


## Gradient-Based Sampling in Action    {style="font-size: 20px;" auto-animate=true}


:::{.columns}

:::: {.column width="10%"}
![HMC: Gaussian Posterior](assets/Samplers/gaussian_hmc_density.png){fig-align="center" width="50%"}
::::
:::: {.column width="10%"}
![HMC: Banana Posterior](assets/Samplers/banana_hmc_density.png){fig-align="center" width="50%"}
::::

:::: {.column width="10%"}
![MCMC: Gaussian Posterior](assets/Samplers/gaussian_mcmc_density.png){fig-align="center" width="50%"}
::::
:::: {.column width="10%"}
![MCMC: Banana Posterior](assets/Samplers/banana_mcmc_density.png){fig-align="center" width="50%"}
::::

:::: {.column width="60%"}

* In high dimensions, **random walk proposals** (MCMC) often land in low-probability regions ‚ü∂ low acceptance.
* To maintain acceptance, step size must shrink like **$1/\sqrt{d}$** ‚ü∂ very slow exploration.
* **HMC uses gradients** to follow high-probability paths ‚ü∂ **better samples, fewer steps**.

::::

:::


::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}

![Sampling Without Gradients ](assets/Samplers/samples_only.png){fig-align="center" width="50%"}

::::::

:::::: {.fragment fragment-index=2 .fade-in}

![Sampling With Gradients ](assets/Samplers/samples_with_gradients.png){fig-align="center" width="50%"}

::::::

:::::

::: {.notes}

In this slide we compare HMC and traditional MCMC in two scenarios:

---

### Top row: **Gaussian posterior**

* HMC (left) aligns well with the true density contours ‚Äî samples are well spread.
* MCMC (right) struggles a bit ‚Äî it‚Äôs noisy, slightly distorted, and shows **correlated samples**.
* That‚Äôs because MCMC does a random walk, which is inefficient even in simple geometries.

---

### Bottom row: **Banana-shaped posterior**

* This is a **nonlinear**, curved posterior ‚Äî a much harder target.
* HMC (left) still tracks the true shape well using its gradient information.
* MCMC (right) again struggles: it oversamples in wrong regions and can‚Äôt explore the full space.

---

### Key Point:

HMC shines when the geometry is tricky. Its gradients guide proposals along the posterior, unlike MCMC‚Äôs aimless wandering.

This motivates why we use HMC or NUTS in high-dimensional, curved, or strongly correlated problems ‚Äî like cosmology.


**Slide: Sampling Without Gradients**

* This shows how traditional MCMC struggles when sampling from complex distributions.
* Here, proposals are based on **random walks**, which means they can easily jump into low-probability regions.
* As a result, many proposals are rejected ‚Äî leading to inefficient sampling.
* To maintain high acceptance, samplers reduce step size ‚Äî but this slows down exploration dramatically, especially in high dimensions.
* You can see the samples (blue dots) under-sample the second peak and have poor coverage overall.

---

**Slide: Sampling With Gradients**

* Now we add **gradient information** ‚Äî this is what HMC uses.
* Gradients give the sampler a sense of ‚Äúdirection,‚Äù pointing it toward high-probability areas.
* Instead of random jumps, we simulate **trajectories** that follow the shape of the distribution.
* This enables much better exploration with fewer steps.
* As a result, samples land more effectively across both peaks and better represent the target distribution.
* This illustrates why gradient-based samplers like HMC or NUTS perform so well in high-dimensional, structured problems.



:::



## Differentiable Inference with JAX {style="font-size: 20px;"}

When it comes to **gradients**, always think of **JAX**.

<br/>

::: {.columns}
:::: {.column width="50%"}


**An Easy pythonic API**

```{.Python code-line-numbers="|11"}
import jax
import jax.numpy as jnp
from jax import random

def sample_prior(key):
    return random.normal(key, shape=(3,))  # Œ© ~ N(0, 1)

def log_prob(omega):
    return -0.5 * jnp.sum(omega**2)  # log p(Œ©) ‚àù -Œ©¬≤

log_prob_jit = jax.jit(log_prob)
```

:::::: {.fragment fragment-index=2 .fade-in}

**Easily accessible gradients using GRAD**

```python
omegas = ... # Sampled Œ©
gradients = jax.grad(log_prob_jit)(omegas)
```

::::::

:::::: {.fragment fragment-index=3 .fade-in}

**Supports vectorization using VMAP**

```python
def generate_samples(seeds):
    key = jax.random.PRNGKey(seeds)
    omega = sample_prior(key)
    return omega
seeds = jnp.arange(0, 1000)
omegas = jax.vmap(generate_samples)(seeds)
```

::::::

::::
:::: {.column width="50%"}

![](assets/Logos/JaxLogo.png){fig-align="center" width="80%"}

::::

:::

::: {.notes}

* This is why JAX is such a natural fit for inference: it‚Äôs fully differentiable and built around **gradients**.
* On the left we show three core ideas that power modern inference.

**First block**: JAX gives you a familiar NumPy-like API, with tools like `jit` to compile and optimize code.
You define a prior, a log-prob function, and wrap it in `jit` ‚Äî easy and fast.

**Second block**: With `jax.grad`, you can differentiate any function.
That means you get gradients of the log-posterior ‚Äúfor free,‚Äù which is exactly what HMC or variational inference need.

**Third block**: JAX scales easily ‚Äî use `vmap` to vectorize your function across many seeds, particles, or chains.
This is a huge win when doing amortized inference or simulation-based methods.

* Altogether, JAX provides the **gradient plumbing** for probabilistic inference ‚Äî while remaining readable and fast.


:::


# Practical Bayesian Modeling & Inference with JAX



## A Recipe for Bayesian Inference


::::{.columns}

:::: {.column width="60%"}


:::::: {.fragment fragment-index=2 .fade-in}
**1. Probabilistic Programming Language (PPL)** *NumPyro*:

```python
import numpyro
import numpyro.distributions as dist

def model():
    omega_m = numpyro.sample("Œ©‚Çò", dist.Uniform(0.1, 0.5))
    sigma8 = numpyro.sample("œÉ‚Çà", dist.Normal(0.8, 0.1))
```

::::::
:::::: {.fragment fragment-index=3 .fade-in}
**2. Computing Likelihoods** *JAX-Cosmo*:
```python
import jax_cosmo as jc
def likelihood(cosmo_params):
    mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(
        cosmo_params, ell, probes
    )
    return jc.likelihood.gaussian_log_likelihood(data, mu, cov)
```


::::::
:::::: {.fragment fragment-index=4 .fade-in}
**3. Sampling the Posterior** *NumPyro & Blackjax*:

```python
from numpyro.infer import MCMC, NUTS

kernel = NUTS(model)
mcmc = MCMC(kernel, num_warmup=500, num_samples=1000)
mcmc.run(random.PRNGKey(0))
samples = mcmc.get_samples()
```

::::::
:::::: {.fragment fragment-index=5 .fade-in}
**4. Visualizing the Posterior** *ArviZ*:
```python
import arviz as az
samples = mcmc.get_samples()
az.plot_pair(samples, marginals=True)
```
::::::

::::


:::: {.column width="40%"}

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}
![](assets/bayes/shopping_cart_0.svg){fig-align="center" width="40%"}
::::::
:::::: {.fragment fragment-index=2 .fade-in-then-out}
![](assets/bayes/shopping_cart_1.svg){fig-align="center" width="40%"}
::::::
:::::: {.fragment fragment-index=3 .fade-in-then-out}
![](assets/bayes/shopping_cart_2.svg){fig-align="center" width="40%"}
::::::
:::::: {.fragment fragment-index=4 .fade-in-then-out}
![](assets/bayes/shopping_cart_3.svg){fig-align="center" width="40%"}
::::::
:::::: {.fragment fragment-index=5 .fade-in-then-out}
![](assets/bayes/shopping_cart_4.svg){fig-align="center" width="40%"}
::::::

::::

::::: {.r-stack}

:::::: {.fragment fragment-index=1 .fade-in-then-out}
![](assets/bayes/bayes_full.svg){fig-align="center" width="70%"}
::::::
:::::: {.fragment fragment-index=2 .fade-in-then-out}
![](assets/bayes/bayes_prior.svg){fig-align="center" width="70%"}
::::::
:::::: {.fragment fragment-index=3 .fade-in-then-out}
![](assets/bayes/bayes_likelihood.svg){fig-align="center" width="70%"}
::::::
:::::: {.fragment fragment-index=4 .fade-in-then-out}
![](assets/bayes/bayes_sample.svg){fig-align="center" width="70%"}
::::::
:::::: {.fragment fragment-index=5 .fade-in-then-out}
![](assets/bayes/post_corner.png){fig-align="center" width="70%"}
<div style="text-align: center; font-size: 12px;">@credit: Zeghal et al. (2409.17975)</div>
::::::
:::::

::::

:::

::: {.notes}

Here are the speaker notes for the two slides:

---

**Slide 1: ‚ÄúA Recipe for Bayesian Inference‚Äù**

* Now let‚Äôs break Bayesian inference into a practical workflow.
* First, we define our model using a **probabilistic programming language** ‚Äî here, NumPyro.
  This is where we encode the prior and the structure of the model.
* Second, we use a tool like **JAX-Cosmo** to compute the likelihood. This connects cosmological parameters to observable predictions ‚Äî such as angular power spectra.
* Third, we use **MCMC** or **BlackJAX** to sample from the posterior.

  * This is where sampling happens, and where gradient-based methods like NUTS come into play.
* Finally, we extract posterior samples to analyze uncertainty and parameter correlations.

The graphic on the right summarizes this logic visually:
from prior ‚Üí likelihood ‚Üí posterior ‚Üí sample.

---

**Slide 2: ‚ÄúA Recipe for Bayesian Inference (Full Loop)‚Äù**

* This slide extends the recipe with the **final step: visualization**.
* After sampling, we can summarize and visualize the posterior with tools like **ArviZ**.
* The corner plot on the right shows the joint and marginal distributions ‚Äî a key diagnostic to assess whether inference worked and how parameters are correlated.
* The dashed vs. solid lines show different inference strategies ‚Äî possibly **explicit** (forward simulations) vs **implicit** likelihoods.

The takeaway is that with JAX, NumPyro, and JAX-Cosmo, we have a **modular pipeline** for Bayesian inference ‚Äî from model definition to visual diagnostics.


:::

---

##  Sampler Comparison Table {style="font-size: 26px;"}

:::{.solutionbox}

::: {.solutionbox-body style="font-size: 18px;"}

| Sampler        | Library            | Uses Gradient | Auto-Tuning | Rejection | Best For                         | Notes                              |
| -------------- | ------------------ | ------------- | ----------- | --------- | -------------------------------- | ---------------------------------- |
| **MCMC (SA)**  | NumPyro            | ‚ùå             | ‚ùå           | ‚úÖ         | Simple low-dim models            | No gradients; slow mixing          |
| **HMC**        | NumPyro / BlackJAX | ‚úÖ             | ‚ùå           | ‚úÖ         | High-dim continuous posteriors   | Needs tuned step size & trajectory |
| **NUTS**       | NumPyro / BlackJAX | ‚úÖ             | ‚úÖ           | ‚úÖ         | General-purpose inference        | Adaptive HMC                       |
| **MALA**       | BlackJAX           | ‚úÖ             | ‚ùå           | ‚úÖ         | Local proposals w/ gradients     | Stochastic gradient steps          |
| **MCLMC**      | BlackJAX           | ‚úÖ             | ‚úÖ (via L)   | ‚ùå         | Large latent spaces              | Unadjusted Langevin dynamics       |
| **Adj. MCLMC** | BlackJAX           | ‚úÖ             | Manual (L)  | ‚úÖ         | Bias-controlled Langevin sampler | Includes MH step     


:::

:::

For more information check Simons et al. (2025), [¬ß2.2.3, arXiv:2504.20130](https://arxiv.org/pdf/2504.20130)


::: {.notes}

**Speaker Notes ‚Äì Sampler Comparison Table**

This table gives a high-level overview of common samplers available in NumPyro and BlackJAX, organized by key features.

* **MCMC (SA)**: This is standard Metropolis-Hastings ‚Äî no gradients, no tuning, but it‚Äôs simple and useful for low-dimensional problems. Slow mixing is a drawback.

* **HMC**: Hamiltonian Monte Carlo improves mixing by using gradients to simulate physics-based trajectories. It requires tuning for step size and path length, so it‚Äôs more manual.

* **NUTS**: No-U-Turn Sampler is HMC with built-in auto-tuning. It adapts step size and path length during warm-up, making it the default choice for general-purpose inference in NumPyro and BlackJAX.

* **MALA**: Metropolis-Adjusted Langevin Algorithm uses gradients for local proposals. It's a good middle ground ‚Äî cheaper than full HMC, but more efficient than MH.

* **MCLMC**: Stochastic Langevin dynamics with **no rejection** step. Useful in large latent spaces, often in simulator-based models, but biased due to lack of correction.

* **Adjusted MCLMC**: Same as MCLMC, but now with a Metropolis-Hastings correction step. This helps reduce bias, at the cost of a little more computation.

**Main idea:** Pick the right sampler based on:

* whether you can compute gradients
* dimensionality of the posterior
* whether you need auto-tuning or not
* whether you want unbiased samples (with rejection) or not.

Final note: This is a simplified summary. For real applications, benchmarking is always best.


:::

---

### Numpyro: Tips & Tricks for Bayesian Modeling


**`numpyro.handlers.seed`: Fix randomness for reproducibility**

```python
from numpyro.handlers import seed
seeded_model = seed(model, rng_key)
```

**`numpyro.handlers.trace`: Inspect internal execution and sample sites**

```python
from numpyro.handlers import trace
tr = trace(model).get_trace()
print(tr["omega"])
```


**`numpyro.handlers.condition`: Clamp a variable to observed or fixed value**

```python
from numpyro.handlers import condition
conditioned_model = condition(model, data={"omega": 0.3})
```

**`numpyro.handlers.substitute`: Replace variables with fixed values (e.g., MAP estimates)**

```python
from numpyro.handlers import substitute
subbed_model = substitute(model, data={"omega": 0.3})
```

**`numpyro.handlers.reparam`: Reparameterize a site to improve geometry**

```python
from numpyro.infer.reparam import LocScaleReparam
from numpyro.handlers import reparam

reparammed_model = reparam(model, config={"z": LocScaleReparam()})
```

::: {.notes}

This slide gives a quick preview of useful `numpyro.handlers` you‚Äôll see in the notebooks.

* **`seed`** lets you fix randomness for reproducibility ‚Äî critical for testing or debugging.
* **`trace`** inspects your model's internal execution and samples ‚Äî great for seeing what actually gets sampled.
* **`condition`** clamps a variable to a fixed value ‚Äî useful for simulating data from a known model.
* **`substitute`** is similar, but lets you plug in values like MAP estimates.
* **`reparam`** lets you reparameterize tricky sample sites to improve posterior geometry and sampling.

We‚Äôll practice using all of these interactively in the notebooks.


:::

## A Minimal Bayesian Linear Model {style="font-size: 18px;"}

**Define a simple linear model:**
```{.Python code-line-numbers="|1-8|10-16"}
true_w = 2.0
true_b = -1.0
num_points = 100

rng_key = jax.random.PRNGKey(0)
x_data = jnp.linspace(-3, 3, num_points)
noise = jax.random.normal(rng_key, shape=(num_points,)) * 0.3
y_data = true_w * x_data + true_b + noise

def linear_regression(x, y=None):
    w = numpyro.sample("w", dist.Normal(0, 1))
    b = numpyro.sample("b", dist.Normal(0, 1))
    sigma = numpyro.sample("sigma", dist.Exponential(1.0))

    mean = w * x + b
    numpyro.sample("obs", dist.Normal(mean, sigma), obs=y)
```

:::: {.fragment fragment-index=1 .fade-in}
**Run the model using NUTS:**
```python
kernel = numpyro.infer.NUTS(linear_regression)
mcmc = numpyro.infer.MCMC(kernel, num_warmup=500, num_samples=1000)
mcmc.run(rng_key, x=x_data, y=y_data)
```
::::
:::: {.fragment fragment-index=2 .fade-in}
**Posterior corner plot using arviz + corner**
```python
idata = az.from_numpyro(mcmc)
posterior_array = az.extract(idata, var_names=["w", "b", "sigma"]).to_array().values.T

fig = corner.corner(
    posterior_array,
    labels=["w", "b", "œÉ"],
    truths=[true_w, true_b, None],
    show_titles=True
)
plt.show()
```
::::

::: {.notes}

This slide walks through the first notebook exercise ‚Äî a minimal Bayesian linear regression model using NumPyro.

* First block defines synthetic data: we're sampling noisy `y` values from a true line `y = 2x - 1`.
* The `linear_regression` function defines a probabilistic model: priors on slope `w`, intercept `b`, and noise `sigma`.
* We use NUTS from NumPyro to sample from the posterior ‚Äî no likelihood math needed manually.
* Finally, the bottom cell visualizes the posterior using `corner.py` and ArviZ. You‚Äôll see how the true parameters compare to the inferred distributions.

This is the foundation ‚Äî you‚Äôll implement and modify this directly during the hands-on.


:::

--- 

## Using BlackJax and NumPyro {style="font-size: 18px;"}

> BlackJax is NOT a PPL, so you need to combine it with a PPL like NumPyro or PyMC.

:::: {.fragment fragment-index=1 .fade-in}

**Initialize model and extract the log-probability function**

```python
rng_key, init_key = jax.random.split(rng_key)
init_params, potential_fn, *_ = initialize_model(
    init_key, model, model_args=(x_data,), model_kwargs={"y": y_data}, dynamic_args=True
)

logdensity_fn = lambda position: -potential_fn(x_data, y=y_data)(position)
initial_position = init_params.z
```
::::


:::: {.fragment fragment-index=2 .fade-in}

**Run warm-up to adapt step size and mass matrix using BlackJAX's window adaptation**

```python
num_warmup = 2000
adapt = blackjax.window_adaptation(blackjax.nuts, logdensity_fn, target_acceptance_rate=0.8)
rng_key, warmup_key = jax.random.split(rng_key)
(last_state, parameters), _ = adapt.run(warmup_key, initial_position, num_warmup)
kernel = blackjax.nuts(logdensity_fn, **parameters).step
```
::::
:::: {.fragment fragment-index=3 .fade-in}

**Run BlackJAX NUTS sampling using `lax.scan`**

```python
def run_blackjax_sampling(rng_key, state, kernel, num_samples=1000):
    def one_step(state, key):
        state, info = kernel(key, state)
        return state, state

    keys = jax.random.split(rng_key, num_samples)
    _, samples = jax.lax.scan(one_step, state, keys)
    return samples

samples = run_blackjax_sampling(rng_key, last_state, kernel)
```
::::
:::: {.fragment fragment-index=4 .fade-in}

**Convert BlackJAX output to ArviZ InferenceData**

```python
idata = az.from_dict(posterior=samples.position)
```
::::

::: {.notes}

This slide shows how to use BlackJAX for sampling when you already have a model defined in NumPyro.

* First, we extract the **log-probability function** from a NumPyro model using `initialize_model`. This lets us use BlackJAX with the same model.
* We define a `logdensity_fn`, which BlackJAX expects ‚Äî it just wraps the potential function with a negative sign.
* Next, we run **adaptive warm-up** with `blackjax.window_adaptation`. This tunes step size and mass matrix like NumPyro does.
* Then, we sample using `blackjax.nuts` with a loop written using `jax.lax.scan` for speed and JIT compatibility.
* Finally, we convert the raw samples to an ArviZ-friendly format with `az.from_dict`.

Key point: BlackJAX is super fast and modular, but not a full PPL ‚Äî so you bring your own model definition.


:::

# Examples: Bayesian Inference for Cosmology

## Power Spectrum Inference with jax-cosmo {style="font-size: 18px;"}

:::: {.fragment fragment-index=1 .fade-in}

#### Step 1: Simulate Cosmological Data

**Define a fiducial cosmology to generate synthetic observations**

```python
fiducial_cosmo = jc.Planck15()
ell = jnp.logspace(1, 3)  # Multipole range for power spectrum
```
**Set up two redshift bins for galaxy populations**

```python
nz1 = jc.redshift.smail_nz(1., 2., 1.)
nz2 = jc.redshift.smail_nz(1., 2., 0.5)
nzs = [nz1, nz2]
```
**Define observational probes: weak lensing and number counts**

```python
probes = [
    jc.probes.WeakLensing(nzs, sigma_e=0.26),
    jc.probes.NumberCounts(nzs, jc.bias.constant_linear_bias(1.))
]
```

**Generate synthetic data using the fiducial cosmology**

```python
mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(fiducial_cosmo, ell, probes)
rng_key = jax.random.PRNGKey(0)
noise = jax.random.multivariate_normal(rng_key, jnp.zeros_like(mu), cov)
data = mu + noise  # Fake observations
```

::::

:::: {.fragment fragment-index=2 .fade-in}

#### Step 2: Define the NumPyro Model

```python
# Define a NumPyro probabilistic model to infer cosmological parameters
def model(data):
    Omega_c = numpyro.sample("Omega_c", dist.Uniform(0.1, 0.5))
    sigma8 = numpyro.sample("sigma8", dist.Uniform(0.6, 1.0))
    
    # Forward model: compute theoretical prediction given parameters
    cosmo = jc.Planck15(Omega_c=Omega_c, sigma8=sigma8)
    mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(cosmo, ell, probes)
    
    # Likelihood: multivariate Gaussian over angular power spectra
    numpyro.sample("obs", dist.MultivariateNormal(mu, cov), obs=data)
```

::::

::: {.notes}

This slide introduces a basic end-to-end inference workflow using `jax-cosmo` and `NumPyro`.

**Step 1: Simulate synthetic cosmological data**

* We start with a fiducial cosmology using `jc.Planck15()`.
* Define a multipole range (`ell`) and two redshift distributions for tomographic bins.
* Create weak lensing and number count probes.
* Then, using the angular power spectrum mean and covariance from `jax_cosmo`, we generate mock observations by adding Gaussian noise.

**Step 2: Define the NumPyro model**

* The model samples two parameters: `Omega_c` and `sigma8`.
* A new cosmology object is constructed with those parameters.
* Then the power spectrum prediction is computed and matched to data with a multivariate Gaussian likelihood.

This setup mirrors what we do in practice: forward-model observables, simulate noisy data, and recover parameters via inference.

:::


---

## Simulation-Based Inference with Forward Models {style="font-size: 18px;"}


::: {.columns}

:::: {.column width="60%"}

::: {.r-stack}

:::: {.fragment fragment-index=1 .fade-in-then-out}
**Bayesian Inference using power spectrum data:**
::::

:::: {.fragment fragment-index=2 .fade-in}
**Bayesian Inference using full field data:**
::::

:::

::: {.r-stack}

:::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/bayes/bayes_pipeline_latent.svg){fig-align="center" width="80%"}

::::

:::: {.fragment fragment-index=2 .fade-in-then-out}


![](assets/bayes/FFI_full.svg){fig-align="center" width="100%"}


::::
:::: {.fragment fragment-index=3 .fade-int}


![](assets/bayes/FFI_full_tuto.svg){fig-align="center" width="100%"}


::::


:::

::::

:::: {.column width="40%"}

:::: {.fragment fragment-index=1 .fade-in}

  - **Recap:** Bayesian inference maps theory + data ‚Üí posterior

::::

:::: {.fragment fragment-index=2 .fade-in}

  - **Cosmological Forward models**
    - Start from cosmological + latent parameters
    - Sample initial conditions
    - Evolve using **N-body simulations**
    - **Predict convergence maps** in tomographic bins

::::
:::: {.fragment fragment-index=3 .fade-in}
  - **Simulation-Based Inference**
    * Compare predictions to real **survey maps**
    * Build a **likelihood** from the forward model
    * Infer cosmological parameters from **full field data**
::::


:::: {.fragment fragment-index=4 .fade-in}


::: {.solutionbox}

::::{.solutionbox-header style="font-size: 19px;"}

**Full Field vs. Summary Statistics**
::::

::::{.solutionbox-body style="font-size: 16px;"}


* Preserves **non-Gaussian** structure lost in summaries
* Enables tighter constraints in **nonlinear regimes**
* Especially useful in **high-dimensional** inference problems
* See: *Zeghal et al. (2024), Leclercq et al. (2021)*
* üîú Two talks on this topic this Thursday

::::

:::

::::

::::

:::


::: {.notes}

**Speaker Notes for Final Slide (Full Field Inference):**

* We now shift to the most general and flexible form of inference ‚Äî using **full field data**.
* This means we **don‚Äôt extract summary statistics** like $C_\ell$ ‚Äî instead, we model the forward process end-to-end, including simulations.
* The green box shows the **core components** of this forward model pipeline:

  * Start by **sampling cosmological + latent parameters** (e.g., initial conditions)
  * Use an **N-body simulation** to evolve structure
  * Predict observables (e.g., weak lensing convergence maps)
  * Compare the simulated maps to real observations to construct a **likelihood**
* This approach **preserves non-Gaussian information**, which is critical in the nonlinear regime.
* We'll focus specifically on the contents of the green box in the hands-on notebook:

  * Sampling initial fields
  * Running a small-scale N-body simulation
  * Building a likelihood from simulation output
* It‚Äôs an extremely powerful method, and the frontier of cosmological inference.


:::

# Conclusion

##  Conclusion: Why Bayesian Inference?

<br/>
<br/>

### üîë Key Takeaways

* **Bayesian modeling** lets you build flexible, extensible pipelines
  ‚Äî from analytical likelihoods to full forward simulations.

* The **JAX ecosystem** (NumPyro, BlackJAX, jax-cosmo...)
  lets you focus on **modeling**, not math details.

* **Gradients + differentiable simulators** scale inference to
  complex, high-dimensional problems ‚Äî efficiently and transparently.

* Bayesian tools are now mature, fast, and usable
  ‚Äî even for large cosmological models.

:::{.solutionbox}

::: {.solutionbox-header style="font-size: 20px;"}
  **Hands-On Notebooks:**
:::

::::{.solutionbox-body style="font-size: 18px;"}

  - Beginner Bayesian Inference with NumPyro & Blackjax [here](https://github.com/ASKabalan/Tutorials/blob/main/BDL2025/Exercises/01_Beginner.ipynb)
  - Intermediate Bayesian Inference with NumPyro & Blackjax [here](https://github.com/ASKabalan/Tutorials/blob/main/BDL2025/Exercises/02_Intermediate.ipynb)
  - some of the animation were made using this [notebook](https://github.com/ASKabalan/Tutorials/blob/main/BDL2025/illustrations/HMC_vs_MCMC.ipynb)

::::

:::

#### Thank you for your attention!


::: {.notes}

**Speaker Notes ‚Äì Conclusion Slide**

* Let‚Äôs wrap up with some key takeaways on **why Bayesian inference matters**, especially in cosmology:

  * First, **Bayesian modeling is modular and flexible**. It gives us a  way to go from simple analytic models to full simulator-based pipelines ‚Äî all under one framework.

  * The **JAX ecosystem** ‚Äî NumPyro, BlackJAX, jax-cosmo ‚Äî gives you a complete toolchain. You don‚Äôt have to worry about math-heavy derivations or custom samplers. You can focus on the science and modeling.

  * A major strength is the ability to leverage **gradients and differentiable simulators**. That means you can scale inference even for models with thousands or millions of parameters ‚Äî like field-level inference.

  * And importantly, the tools are **mature and fast**. These aren‚Äôt just research toys ‚Äî they‚Äôre ready for real problems.

* Now it‚Äôs your turn. You‚Äôll have access to **two hands-on notebooks**:

  * One walks you through Bayesian regression and cosmological inference with NumPyro.
  * The other dives into field-level inference and simulation-based modeling.

* This is where things become tangible ‚Äî you‚Äôll code your own inference pipeline, simulate structure formation, and run real MCMC samplers.

* That‚Äôs the power of combining **Bayesian ideas** with **modern tools** ‚Äî and that‚Äôs where the future of cosmological inference is headed.


:::
