---
title: "CSI Presentation 2024"
author: "Wassim Kabalan"
footer: "CSI Presentation 2024"
format:
  revealjs:
    theme: [default, css/custom.scss]
    incremental: false   
    transition: slide
    background-transition: slide
    presentation-size: max-scale
    slide-number: true
    template-partials:
      - css/title-slide.html
output: revealjs

code-block-border-left: "#31BAE9"
title-slide-attributes:
  data-background-image: "assets/lsst_bg.jpg"
  data-background-size: fill
  data-background-opacity: "0.5"

logo1: '![](assets/Logos/AIM.png){fig-align="center"width=10%} ![](assets/Logos/APC.png){fig-align="center" width=10%} ![](assets/Logos/AstroDeep-2.png){fig-align="center" width=10%} ![](assets/Logos/scipol.jpeg){fig-align="center" width=10%}'


---


## Accelerating Bayesian Inference in Cosmology

<br />
<br />

### Working Framework

➢  &emsp; Using advanced software and tools to constrain cosmological parameters through Bayesian inference

➢  &emsp; Leveraging the Cosmic Microwave Background (CMB) as a tracer to constrain the tensor-to-scalar ratio, $r$

➢  &emsp; Utilizing weak lensing to constrain key **cosmological parameters** like $\Omega_m$ and $\sigma_8$

<br />
<br />

::: {.fragment fragment-index=1}

### Employing Cutting-Edge Tools

➢  &emsp; Transitioning from CPU-based **NumPy** to GPU-accelerated **JAX** for faster computation

➢  &emsp; Writing optimized **CUDA** code for cosmology-specific tools

➢  &emsp; Scaling cosmological simulations to run on multiple GPUs and HPC nodes for enhanced performance

:::


::: {.notes}


Le titre de ma thèse est **« Accélérer les pipelines bayésiens pour la cosmologie »**. Ce travail vise à accélérer les processus de calcul pour extraire les paramètres cosmologiques à l'aide de l'inférence bayésienne.

L'inférence bayésienne, en utilisant le Fond diffus cosmologique (CMB) comme principal traceur, nous aide à contraindre des paramètres importants comme le rapport tenseur-surface $r$, fournissant des informations sur l'inflation et les conditions de l'univers primitif.

À mesure que l'univers évolue et que des structures à grande échelle se forment, nous utilisons le lentillage gravitationnel faible pour étudier ces structures et contraindre des paramètres comme $\Omega_m$ et $\sigma_8$.

**NEXT**

Sur le plan technique, nous passons des calculs basés sur le CPU à des workflows accélérés par GPU en utilisant JAX, ce qui améliore considérablement la vitesse de nos simulations.

Nous optimisons également les outils cosmologiques avec CUDA et mettons à l'échelle nos simulations pour les exécuter sur plusieurs GPU et nœuds de clusters de calcul haute performance (HPC) afin de traiter efficacement de grandes quantités de données.

:::

## Summary of projects {auto-animate=true}

<br/>

:::{data-id="Projects"}
![Projects](assets/Projects/ALL_Projects.svg){.nostretch fig-align="center" width="70%"}
:::


::: {.notes}


Mon sujet de recherche est à cheval entre deux domaines : **la séparation des composants du CMB** dans le cadre du projet **SciPol**, et **le lentillage gravitationnel faible** avec **AstroDeep**. Je vais vous présenter ces deux projets plus en détail dans les prochaines diapositives.

:::

## Cosmic Microwave Background - Scipol {auto-animate=true}

<br/>

:::{data-id="Projects"}
![Projects](assets/Projects/Scipol.svg){.nostretch fig-align="center" width="70%"}
:::

::: {.notes}

On commence par la separation de composants pour le fond diffus cosmologique

:::

## Cosmic Microwave Background 
:::: {.columns}

::: {.column width="50%"}

![](assets/CMB/CMB-Planck.png){.nostretch fig-align="center" width="75%"}

::: {.fragment fragment-index=1}
![](assets/CMB/Equal_symbol.svg){.nostretch fig-align="center" width="10%"}

![](assets/CMB/eb-modes.png){.nostretch fig-align="center" width="45%"}
::: 

:::

::: {.column width="50%"}

- The **Cosmic Microwave Background (CMB)** is the afterglow of the Big Bang, providing a snapshot of the early universe.

::: {.fragment fragment-index=1}
- The CMB is polarized, consisting of **E and B modes**. 
- **E modes** are curl-free, generated by density fluctuations.
- **B modes** could be evidence of primordial gravitational waves, indicating cosmic inflation.
- **The tensor-to-scalar** ratio $r$, which is the ratio of the tensor power spectrum to the scalar power spectrum
:::

:::

::::


::: {.notes}

Le fond diffus cosmologique, ou CMB, est le rayonnement fossile du Big Bang, qui nous offre une image de l'univers primitif.

Le CMB est polarisé et se compose de deux types de modes : les **E modes** et les **B modes**.

Les **E modes** ont un champ rotationnel nul et sont générés par les fluctuations de densité dans l'univers. En revanche, les **B modes**, qui possèdent un champ rotationnel, pourraient indiquer la présence d'ondes gravitationnelles primordiales.

Un paramètre clé lié à ces **B modes** est le **rapport tensor-surface** $r$, qui mesure la force relative des ondes gravitationnelles par rapport aux perturbations de densité, ce qui nous aide à mieux comprendre l'inflation cosmique.

:::

## Cosmic Microwave Background 

:::: {.columns}

::: {.column width="50%"}

![](assets/CMB/CMB-Planck.png){.nostretch fig-align="center" width="35%"}

::: {.fragment fragment-index=1}

![](assets/CMB/Plus_symbol.svg){.nostretch fig-align="center" width="10%"}

![](assets/CMB/dust_planck.png){.nostretch fig-align="center" width="35%"}

:::

::: {.fragment fragment-index=2}

![](assets/CMB/Plus_symbol.svg){.nostretch fig-align="center" width="10%"}

![](assets/CMB/synch-planck.png){.nostretch fig-align="center" width="35%"}

:::

:::

::: {.column width="50%"}

- The **Cosmic Microwave Background (CMB)** signal is obscured by various **foregrounds**, making it challenging to detect the true cosmological information.

::: {.fragment fragment-index=1}
- **Dust**: Emission from galactic dust adds significant noise to the CMB, particularly affecting polarization measurements.
:::

::: {.fragment fragment-index=2}
- **Synchrotron Radiation**: Electrons spiraling in the galaxy's magnetic fields produce synchrotron radiation, another major contaminant.
:::

<br />
<br />

::: {.fragment fragment-index=3}

### Component seperation methods

- **Blind Methods**: Like **SMICA** (Spectral Matching Independent Component Analysis)
- **Parametric Methods**: Like **FGbuster** (Foreground Buster)

:::


<br />
<br />




:::

::::

::: {.notes}

Le signal du fond diffus cosmologique, ou CMB, est en réalité obscurci par plusieurs avant-plans, ce qui rend difficile l'extraction des informations cosmologiques réelles.

L'un des contaminants principaux est la poussière galactique. Cette poussière émet du rayonnement qui ajoute un bruit significatif au CMB, affectant particulièrement les mesures de polarisation.

**NEXT**

Un autre contaminant majeur est la radiation synchrotron. Elle est produite par des électrons en spirale dans les champs magnétiques de notre galaxie, ce qui vient encore plus brouiller le signal cosmologique que l'on souhaite observer.

**AFTER**

Pour pouvoir extraire une valeur fiable du rapport $r$, il est crucial de séparer ou de "démixer" ces composants. Le signal du CMB est mêlé à diverses émissions parasites.

 Il existe différentes méthodes pour cela, principalement des méthodes aveugles comme SMICA, qui fonctionnent sans connaissances préalables des avant-plans, et des méthodes paramétriques comme FGbuster, qui reposent sur la modélisation explicite des avant-plans.

Dans cette présentation, nous allons nous concentrer sur les méthodes paramétriques. Celles-ci nous permettent d'utiliser des modèles pour les avant-plans et d'améliorer la précision du processus de séparation.

:::

## CMB Component Separation

:::: {.columns}

::: {.column width="50%"}

::: {.fragment fragment-index=1}

![](assets/CMB/dust_planck.png){.nostretch fig-align="center" width="45%"}

:::

::: {.fragment fragment-index=2}

![](assets/CMB/synch-planck.png){.nostretch fig-align="center" width="45%"}

:::

:::

::: {.column width="50%"}

::: {.fragment fragment-index=1}

**Modified Blackbody SED of Dust:**

$$
\boxed{s_{\mathrm{d}}(\nu) = A_{\mathrm{d}} \cdot \frac{\nu}{\exp\left(\frac{h\nu}{k \color{red}{T_{\mathrm{d}}}}\right) - 1} \cdot \frac{\exp\left(\frac{h\nu_{0}}{k \color{red}{T_{\mathrm{d}}}}\right) - 1}{\nu_{0}} \cdot \left(\frac{\nu}{\nu_{0}}\right)^{\color{blue}{\beta}}}
$$

:::

::: {.fragment fragment-index=2}

**Power Law of Synchrotron Emission:**

$$
\boxed{s_{\text{synch}}(\nu) = \left(\frac{\nu}{\nu_0}\right)^{\color{green}{\beta_{\text{pl}}}}}
$$

:::

:::

::::

::: {.fragment fragment-index=3}

#### Signal Representation

::: {.columns}

::: {.column width="50%"}

$$
\boxed{\mathbf{d} = \mathbf{A} \mathbf{s} + \mathbf{n}}
$$

:::

::: {.column width="50%"}

$$
\boxed{\mathbf{d} = \color{green}{A_{\text{synch}}} \cdot s_{\text{synch}} + \color{blue}{A_{\mathrm{d}}} \cdot s_{\mathrm{d}} + A_{\text{cmb}} \cdot s_{\text{cmb}} + \mathbf{n}}
$$

:::

:::

:::

::: {.fragment fragment-index=4}

#### Likelihood Function

$$
\boxed{-2 \ln \mathcal{L}_{\text{data}}(\mathbf{s}, \boldsymbol{\beta}) = \text{const} + \sum_{p} \left( \mathbf{d}_p - \mathbf{A}_p \mathbf{s}_p \right)^T \mathbf{N}_p^{-1} \left( \mathbf{d}_p - \mathbf{A}_p \mathbf{s}_p \right)}
$$

:::

::: {.r-stack}

::: {.fragment fragment-index=5 .fade-in-then-out}

#### Minimization for Component Separation

$$
\boxed{\mathbf{s} = \left( \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d} \right)^T \left( \mathbf{A}^T \mathbf{N}^{-1} \mathbf{A} \right)^{-1} \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d}}
$$

:::

::: {.fragment fragment-index=6}

#### Minimization for Component Separation

$$
\boxed{\mathcal{L}(\color{blue}{\beta_d}, \color{red}{T_d}, \color{green}{\beta_{\text{pl}}}) = \left( \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d} \right)^T  \left( \mathbf{A}^T \mathbf{N}^{-1} \mathbf{A} \right)^{-1} \mathbf{A}^T \mathbf{N}^{-1} \mathbf{d}}
$$

:::


:::

::: {.notes}

Dans cette diapositive, je vais expliquer comment on modélise la séparation des composantes du CMB à partir des données observées.

Nous avons d'abord **deux composantes principales** qui contaminent le signal CMB pur : 
- **La poussière galactique**, modélisée par une loi de corps noir modifiée. Elle est une des sources d'émission la plus importante à haute fréquence.
- **Le rayonnement synchrotron**, produit par les électrons en spirale dans les champs magnétiques de la galaxie, qui est une source dominante à basse fréquence.

On représente ensuite le signal **d** par une combinaison linéaire des contributions de chaque composant multipliées par leur matrice de mélange respective, plus un bruit **n**. 

L'objectif de la séparation des composantes est de maximiser la vraisemblance de nos données modélisées par rapport aux données observées. Cela se fait par une **minimisation**, représentée par l'équation en bas de la diapositive.

La méthode que nous utilisons ici est **paramétrique**, où chaque composante a un modèle physique avec des paramètres spécifiques comme **$\beta$**, **$T_d$** pour la poussière et **$\beta_{pl}$** pour le synchrotron.

À la fin, nous obtenons la partie CMB de la matrice de mélange, à partir de laquelle nous allons pouvoir estimer le **ratio tenseur-spectral $r$**, un paramètre clé pour contraindre les modèles d'inflation cosmique.

:::


## Minimization Process in CMB Component Separation  ![SO](assets/Logos/so.webp){.nostretch fig-valign="center" width="5%"} ![LiteBIRD](assets/Logos/litebird.png){.nostretch fig-valign="center" width="7%"}


### Using Scipols's  `Furax` Library (Chanial et al. in prep.)

<br />
<br />

```python
blocks = jnp.arange(24).reshape(3, 2, 4)
p = DenseBlockDiagonalOperator(blocks, jax.ShapeDtypeStruct((3, 4), jnp.int32), 'imn,in->im')
op.as_matrix()
Array([[ 0,  1,  2,  3,  0,  0,  0,  0,  0,  0,  0,  0],
       [ 4,  5,  6,  7,  0,  0,  0,  0,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  8,  9, 10, 11,  0,  0,  0,  0],
       [ 0,  0,  0,  0, 12, 13, 14, 15,  0,  0,  0,  0],
       [ 0,  0,  0,  0,  0,  0,  0,  0, 16, 17, 18, 19],
       [ 0,  0,  0,  0,  0,  0,  0,  0, 20, 21, 22, 23]], dtype=int32)

```
<br />
<br />

::: {.fragment fragment-index=1}

::: {.solutionbox}

::::{.solutionbox-header}

My contributions (in the context of SO and LiteBIRD)

::::

::::{.solutionbox-body}

- **Use JAX tools** to evaluate the spectral likelihood.
- **Apply gradient descent methods** to minimize the likelihood function.
- **Next steps**
  - Adapt code to handle **multi-resolution** data.
  - Implement support for **multi-patch** analysis.
  - Write a paper about GPU-accelerated component seperation

::::

:::

:::

::: {.notes}

Cette matrice bloc-diagonale que nous utilisons pour la séparation des composantes du CMB a une taille de l'ordre de (fréquence, stokes, composante, npix), et peut rapidement atteindre plusieurs gigaoctets en mémoire. Avec des résolutions élevées et de multiples fréquences, la gestion efficace de cette matrice devient critique. C’est pourquoi l’utilisation d’outils comme **JAX** est indispensable pour optimiser les calculs, en exploitant l'accélération par **GPU** tout en minimisant la consommation de mémoire et les temps de calcul.

Mon travail consiste à utiliser les outils de JAX pour évaluer la fonction de vraisemblance spectrale, puis appliquer des méthodes de descente de gradient pour minimiser cette fonction et optimiser la séparation des composantes du CMB. 

À l'avenir, je prévois d'adapter ce code pour gérer des données multi-résolution et permettre une analyse multi-patch, afin de traiter plus efficacement des régions distinctes du ciel avec des résolutions variées.

::: 

## Summary of projects {auto-animate=true}

:::{data-id="Projects"}
![Projects](assets/Projects/ALL_Projects.svg){.nostretch fig-align="center" width="70%"}
:::

## Large Scale Structure - AstroDeep {auto-animate=true}

:::{data-id="Projects"}

![Projects](assets/Projects/WL.svg){.nostretch fig-align="center" width="70%"}

:::

## Large Scale Structure - Statistical Tools {auto-animate=true}

:::: {.columns}

::: {.column width="50%"}

::: {.r-stack}

::: {.fragment fragment-index=1 .fade-out}

![Hubble eXtreme Deep Field](assets/CSI-Hubble.svg){.nostretch fig-align="center" width="55%"}

:::

::: {.fragment fragment-index=1 .fade-in-then-out}

![Hubble eXtreme Deep Field](assets/CSI-Hubble Annotated.svg){.nostretch fig-align="center" width="55%"}

:::

::: {.fragment fragment-index=2}

$$
-2 \underbrace{\log P(g, \boldsymbol{\beta} \mid \mathbf{d})}_{\text{Posterior}} = 
\sum_{\vec{k}} \left[\underbrace{\frac{\left|\mathbf{d} - f(g \mid \boldsymbol{\beta}, z)\right|^2}{N}}_{\text{Likelihood}} + \underbrace{\frac{|g|^2}{\mathcal{P}(\boldsymbol{\beta})}}_{\text{Prior}}\right]_{\vec{k}}
$$

:::

:::

::: {.fragment fragment-index=4}

![Prediction](assets/Symboles/Right-Arrow.svg){.nostretch fig-align="center" width="70%"}

:::

::: {.fragment fragment-index=3}


::: {layout="[[30,-4,30,-4,34], [100]]"}

![Gaussian field](assets/Fields/initial_conditions_1024.png){.nostretch fig-align="center" width="75%"}


![LPT Field](assets/Fields/LPT_density_field_z0_1024.png){.nostretch fig-align="center" width="75%"}


![Galaxies](assets/CSI-Hubble.svg){.nostretch fig-align="center" width="75%"}

:::

::: {.fragment fragment-index=4}

![Inference](assets/Symboles/Left-Arrow.svg){.nostretch fig-align="center" width="70%"}

:::

:::

:::

::: {.column width="50%"}

::: {.r-stack}

::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/FFI/PowerSpec.svg){.nostretch fig-align="center" width="80%"}

:::

::: {.fragment fragment-index=2}

![](assets/FFI/CSI-FFI.svg){.nostretch fig-align="center" width="80%"}

:::

:::


:::

::::

::: {.notes}

Dans le cadre de mon travail avec **AstroDeep**, je me concentre sur le lentillage gravitationnel faible.

Le lentillage gravitationnel faible est un tracer important pour mettre une contrante sur les parameters cosmologique liée à la densité de matiere dans l'univers et la formation des sctructure à grande echelle.

traditionellement, on utilise une fonction de correlation à deux points pour résumer les données des champs de convergence. typiquiment on utlisé le spectre de puissance .. très concreement il s'agit de encoder les distancess entre chaque pair de galaxy (ce qui assume des données gaussienne)

Cependant, cette approche est limitée car le champs observées à basse red shift est tout sauf gaussian.

**NEXT**

Une autre famille de methodes existent, notammement l'inference basé sur les simulation ou SBI. Cette approche consiste à utiliser des simulations pour générer des données et des champs de convergence, et ça remplace l'utilisation d'une vraisemblance analytique.

La vraisemblance devient l'ecart entre la sortie de la simulation et les données observées, On ajoute aussi un apriori gaussian sur les parameters.

**NEXT**

Un exemple de ce genre de processus est illustré ici. On commence par générer un champ gaussien, puis on le fait évoluer en utilisant une Simulation cosmologique. On utilise d'autre methodes specifique pour introduire les galaxies dans les amas de matière noire puis on compare avec les données

**NEXT**

En bref, on appelle le processus de générer les données à partir de simulations **Forward Modeling** ou la prédiction, et le processus de comparer les données observées avec les simulations **Inference**.

mon travail consist à faire un ce forward model

:::

## Hearchical Bayesian Modeling


:::{.columns}

:::{.column width="50%"}

![Probabilistic Graphical Model](assets/FFI/HBM.png){.nostretch fig-align="center" width="70%"}

:::

:::{.column width="50%"}

➕  &emsp;No longer need to compute the likelihood analytically
<br />
<br />

:::{.fragment fragment-index=1}

➖ &emsp;We need to infer the joint posterior $p(\beta, g | z)$ before marginalization to get $p(\beta | g) = \int p(\beta, z | g) \, dz$

:::{.fragment fragment-index=2}

:::{.solutionbox}


::::{.solutionbox-header}
Possible solutions
::::

::::{.solutionbox-body}
- *Hamiltonian Monte Carlo* (NUTS)
- **Variational Inference**
- **Dimensionality reduction using Fisher Information Matrix**
::::

:::

:::


:::

:::

:::{.fragment fragment-index=3}


:::{.solutionbox}


::::{.solutionbox-header}
Differentiable forward model for differentiable sampling
::::

::::{.solutionbox-body}
- Full NBody simulation are too slow to be used in an iterative sampler
- Dynamic grid-based simulations can be fast but hard to differentiate
- Fast Particle-Mesh simulations are fast and differentiable
::::

:::

:::

:::


::: {.notes}

Dans le cadre de l'inférence bayésienne hiérarchique, on utilise un modèle graphique probabiliste pour représenter les relations entre les variables aléatoires.

typiquiment ici j'ai un exemple d'un model graphique probabiliste ou les cercle blanches sont les parameters echantillonable et les grise represent les parametes latents.

L'avatage c'est que on a plus besoin de calculer la vraisemblance analytique, 

**Next**

Ce genre de simulateur dit explicit, ou les parameters intermediaire ou latents sont pas directement observables mais utilise, contrainerement à un simulateur implicite ou aveugle ou les parameters ne sont plus interpretable, implique une augmentation majeur de temps passé à marginaliser .

**Next**

Pour résoudre ce problème, on peut utiliser des méthodes d'échantillonnage comme le **NUTS** ou le **HMC**, ce genre de methodes peuvent être significativement accélérées en utilisant un modèle forward différentiable.
En ayant accès aux gradient, l'echantillonnage necessite moins de pas pour converger.

**Next**

Pour ce faire, des simulation N Corps classique sont trop lentes pour être utilisées dans un échantillonneur itératif. Les simulations basées sur des grilles dynamiques peuvent être rapides mais difficiles à différencier. Les simulations de particules-réseau sont rapides et différentiables.

:::

## Fast Particle-mesh simulations

<br />
<br />

:::: {.columns}

::: {.column width="50%"}

#### Numerical scheme

::: {.fragment fragment-index=1}

➢  &emsp;Interpolate particles on a grid to estimate mass density

:::

::: {.fragment fragment-index=2}

➢  &emsp;Estimate gravitational force on grid points by FFT

:::

::: {.fragment fragment-index=3}

➢  &emsp;Interpolate forces back on particles

:::

::: {.fragment fragment-index=4}

➢  &emsp;Update particle velocity and positions, and iterate 

:::

::: 

::: {.column width="50%"}

:::{r-stack}


::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/FastPM_Init.gif){.absolute top=50 left=400 width="800"}

:::


::: {.fragment fragment-index=2 .fade-in-then-out}

![](assets/FastPM_LPT.gif){.absolute top=50 left=400 width="800"}


[$\begin{array}{c}{{\nabla^{2}\phi=-4\pi G\rho}}\\\\ {{f(\vec{k})=i\vec{k}k^{-2}\rho(\vec{k})}}\end{array}$]{.absolute top=500 right=160}

::: 

::: {.fragment fragment-index=3}

![](assets/FastPM_ODE.gif){.absolute top=50 left=400 width="800"}



:::

:::

::: 

:::: 

::: {.fragment fragment-index=5 }

<br />
<br />
<br />
<br />
<br />
<br />


:::{.solutionbox}

::::{.solutionbox-body}

- Fast and simple, at the cost of approximating short range interactions. 
- It is essentially a series of FFTs and interpolations
- It is differentiable and can run on GPUs

::::

:::

:::

::: {.notes}

Les simulations de particules-mesh sont une méthode rapide et simple pour simuler l'évolution des structures cosmologiques.

Le schéma numérique est assez simple :

on commence par interpoler les particules sur une grille pour estimer la densité de masse,

puis on estime la force gravitationnelle sur les points de la grille en utilisant une transformée de Fourier rapide (FFT). 

On interpole ensuite les forces sur les particules, on met à jour les vitesses et les positions des particules, et on répète le processus.

On peut demarrer d'un red shift 10 par exemple et faire evoluer le systeme jusqu'à un red shift 0.

**NEXT**

Cette méthode est rapide et simple, mais elle approxime les interactions à courte portée. C'est essentiellement une série de FFT et d'interpolations. Elle est différentiable et peut être exécutée sur des GPU.

:::

<!-- <p style="display: flex; align-items: center; position: absolute; top: 10px; left: 700px;"> -->
<!--   <img src="assets/Logos/github_logo.png" alt="GitHub Logo" style="width: 5%; margin-right: 10px;"> -->
<!--   <a href="https://github.com/DifferentiableUniverseInitiative/JaxPM">DifferentiableUniverseInitiative/JaxPM</a> -->
<!-- </p> -->
<!---->

## Fast Particle-mesh scaling  ![LSST Desc](assets/Logos/lsst_desc.png){.nostretch fig-valign="center" width="5%"}

➢  &emsp;(Poqueres et al. 2021) : $64^3$ mesh size, on a 1000 Mpc/h box

➢  &emsp;(Li et al. 2022) : $512^3$ mesh size,  using [pmwd](https://github.com/eelregit/pmwd)

➢  &emsp;(Lanusse et al.) :  [JaxPM](https://github.com/DifferentiableUniverseInitiative/JaxPM) similaire à pmwd.

➢  &emsp;[FastPM](https://github.com/fastpm/fastpm) : distributed but CPU-based
 

::: {.fragment fragment-index=1 }

![](assets/HPC/depict_gathered.png){.absolute top=10 right=0 width="15%"}

:::  

:::: {.columns}

::: {.column width="50%"}

::: {.r-stack}

::: {.fragment fragment-index=1 .fade-in-then-out}

![Initial Conditions with a 1024 mesh](assets/Fields/initial_conditions_1024.png){.nostretch fig-align="center" width="400px"}

::: 

::: {.fragment fragment-index=2 .fade-in-then-out}

![Initial Conditions with a 64 mesh](assets/Fields/initial_conditions_64.png){.nostretch fig-align="center" width="400px"}

::: 

::: {.fragment fragment-index=5 .fade-out}
::: {.fragment fragment-index=3}

![Power spectrum comparison](assets/Fields/power_spec.png){.nostretch fig-align="center" width="100%}

::: 
:::


::: {.fragment fragment-index=5}

![Muti Node ( $\infty$ )](assets/HPC/jax-multi-node.png){width="100%"}

:::


:::

::: 

::: {.column width="50%"}

::: {.r-stack}

::: {.fragment fragment-index=1 .fade-in-then-out}

![Final field with a 1024 mesh](assets/Fields/LPT_density_field_z0_1024.png){.nostretch fig-align="center" width="400px"}

::: 

::: {.fragment fragment-index=2 .fade-in-then-out}

![Final field with a 64 mesh](assets/Fields/LPT_density_field_z0_64.png){.nostretch fig-align="center" width="400px"}

:::

::: {.fragment fragment-index=4}

:::{.solutionbox}

::::{.solutionbox-header}

My contributions (in the context of ISSC and LSST DESC)

::::

::::{.solutionbox-body}

We need a fast, differentiable and <span style="color:violet;">Scalable</span> Particle-Mesh simulation that can run on multiple GPUs.

**Multi-GPU Particle mesh** requires : 

  - **Distributed FFT** operations\
  - **Interpolation scheme** to handle boundary conditions in a distributed manner

::::

:::  

:::


::: 

:::: 



:::


::: {.notes}

Il existe déja quelque implementation de simulation de particules-mesh qui sont capable de simuler des boites de 1000 Mpc/h avec une résolution de 64^3 ou 512^3.

Les deux exemples faites dans un papier de poqueres et l'autre par le package pmwd basé sur JAX.

Le papier de poqueres à utilisé une résolution de 64^3 pour simuler une boite de 1000 Mpc/h, et le package pmwd peut aller jusq'à une résolution de 512^3.

JaxPM est un package similaire à pmwd, qui est capable de simuler des résolutions similaires à celles de pmwd et toujours sur un seul GPU.

**NEXT**

Un exemple de deux simulation faites sur une boite de 1 Gpc/h avec une résolution de 64^3 et 1024^3. On peut voir que la résolution plus élevée permet de capturer plus de détails dans le champ de densité.

**NEXT**

Si on visualise le spectre de puissance de ces deux champs, on peut voir que la résolution plus basse sous-estime la densité de matière et les interactions à petite échelle.
En fonction du type du but cosmologique , ce qui peut dire que ce type d'inference ne sera pas capable de mettre une contrainte mielleure voir meme pire par rapport à une méthode basé sur des fonction de correlation à deux points ou spectre de puissance.

La taille de mémoire etant un facteur limitant, il est important de pouvoir mettre à l'échelle ces simulations sur plusieurs GPU et nœuds de calcul haute performance.

Le passage à une simulation multi-GPU nécessite des opérations de FFT distribuées, des interpolations distribuées et des conditions aux limites.

**NEXT**

Notre but serait de pouvoir faires simulation qui peuvent être distribuées sur plusieur GPU et nœuds de calcul haute performance.
Tout en restant diffirentiable et rapide (quelque secondes pour chaque simulation).

pas été fait

:::


## Distributed Fast Fourier Transform {auto-animate="true"}

➢  &emsp;only operation that requires communication is the FFT

<br/>

#### Jaxdecomp

:::: {.columns}

::: {.column width="50%"}

```python
import jax
import jax.numpy as jnp

field = jax.random.normal(jax.random.PRNGKey(0), (1024, 1024, 1024))
k_field = jnp.fft.fftn(field)
```

:::

::: {.column width="50%"}

:::

::::

:::{.notes}

L'opération qui nécessite le plus de communication dans une simulation distribuée est la transformée de Fourier rapide (FFT).

L'utilisation sur un seul GPU est triviale

:::

---

## Distributed Fast Fourier Transform {auto-animate="true"}

➢  &emsp;only operation that requires communication is the FFT

<br/>

#### Jaxdecomp

<p style="display: flex; align-items: center; position: absolute; top: 10px; left: 600px;">
  <img src="assets/Logos/github_logo.png" alt="GitHub Logo" style="width: 5%; margin-right: 10px;">
  <a href="https://github.com/DifferentiableUniverseInitiative/jaxDecomp">DifferentiableUniverseInitiative/jaxDecomp</a>
</p>

:::: {.columns}

::: {.column width="50%"}


::: {.r-stack}

```python
import jax
import jax.numpy as jnp
import jaxdecomp

devices = mesh_utils.create_device_mesh((2, 2))
mesh = jax.sharding.Mesh(devices, axis_names=('x', 'y'))
sharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))

# Create gaussian field distributed across the mesh
field = jax.make_array_from_single_device_arrays(
        shape=mesh_shape,
        sharding=sharding,
        arrays=[jax.random.normal(jax.random.PRNGKey(rank), (512, 512, 1024))])

k_field = jaxdecomp.fft.pfft3d(field)


```

<!-- ::: {.fragment fragment-index=3} -->
<!---->
<!-- ![](assets/Papers/joss-jaxdecomp.png) -->
<!---->
<!-- ::: -->

:::

:::{.fragment fragment-index=1}


:::{.solutionbox}

::::{.solutionbox-header}

JaxDecomp features

::::

::::{.solutionbox-body}

➢  &emsp;jaxDecomp supports 2D and 1D decompositions

➢  &emsp;Works for multi-node FFTs

➢  &emsp;is differentiable

➢  &emsp;The package is also provided as a standalone library


::::

:::

:::

:::

::: {.column width="50%"}

:::{.fragment fragment-index=2}

![](assets/Fields/CIC/FFT-Op.svg){.nostretch fig-align="center" width="100%"}

:::

::: 

::::


::: {.notes}

avec JaxDecomp c'est aussi trivial faire des FFT distribuées sur plusieurs GPU et nœuds de calcul.

L'utilisation est aussi trivial que faire une FFT sur un seul GPU, il suffit de décrire la grille de distribution et la bibliothèque s'occupe du reste.

**NEXT**

ça supporte les décompositions 2D et 1D, fonctionne pour les FFT multi-nœuds, tout en restant différentiable.

**NEXT**

Côté implémentation, j'effectue à une série de FFT locales sur chaque GPU, en travaillant à chaque fois sur un axe non distribué. Ensuite, j’effectue une transposition de la grille de distribution pour redistribuer les données et traiter l’axe suivant. Cela permet de répartir la charge de calcul efficacement entre plusieurs GPU tout en s'assurant que chaque transformation de Fourier est correctement alignée avec l'axe de calcul.
:::

## Scaling of Distributed FFT operations

![](assets/benchmarks/single_precision_gpus.png){.absolute top=50 left=0 width="1000"}

::: {.notes}

Dans cette figure, on peut voir les performances de la bibliothèque JaxDecomp pour les FFT distribuées sur plusieurs GPU.

On est capable des faires des FFTs sur des grille de 4096^3 en quelque centaine de millisecondes (ce qui est très rapide).

Et on voit deja qu'on peut aller jusqu'à une grille de 4096^3 en utilisant 64 GPU.
En comparaison, Les simulation actuelles avec un seul GPU ne dépassent pas 1024^3.

:::

## Interpolation function for Particle-Mesh simulations (Cloud-in-Cell)

:::: {.columns}

::: {.column width="50%"}

::: {.fragment fragment-index=2}

**CIC Paint Function:**

$$
\begin{array}{l c r}
g({\bf j})=\sum_{i=1}^{N}m_{i}\times\prod_{d=1}^{D}\left(1-\left|p_{i}^{d}-j_{d}\right|\right)
\end{array}
$$

:::

::: {.fragment fragment-index=3}

**Forces Functions:**

$$
\nabla^{2}\phi = -4\pi G\kappa\rho
$$

$$
f(\vec{k}) = \dot{\vec{k}}k^{-2}\rho(\vec{k})
$$

:::

::: {.fragment fragment-index=4}

**CIC Read Function:**

$$
v_{i} = \sum_{{\bf j}}g({\bf j})\times\prod_{d=1}^{D}\left(1-|p_{i}^{d}-j_{d}|\right)
$$

:::

::: {.fragment fragment-index=5}

:::{.solutionbox}

::::{.solutionbox-body}

➢  &emsp;Periodic boundary conditions are applied to each slice of the simulation

➢  &emsp;Particles cannot escape the simulation domain

::::

:::

:::


:::

::: {.column width="50%"}

::: {.fragment fragment-index=1}

![Particles](assets/Fields/CIC/CIC_Init.svg){.nostretch fig-align="center" width="20%"}

:::

::: {.fragment fragment-index=2}

![CIC Kernel](assets/Fields/CIC/CIC_Kernel.svg){.nostretch fig-align="center" width="20%"}


![Mesh](assets/Fields/CIC/CIC_Grid.svg){.nostretch fig-align="center" width="20%"}

:::

::: {.fragment fragment-index=3}

::: {layout="[2, 1 , 2] " layout-valign="center" layout-align="center"}


![Gradients](assets/Fields/CIC/CIC_Gradients.svg){.nostretch fig-align="center" width="60%"}

![](assets/Symboles/Right-Arrow.svg){.nostretch fig-align="center" width="100%"}

![Displacement](assets/Fields/CIC/CIC_Disp.svg){.nostretch fig-align="center" width="60%"}


:::

:::

:::

::::

::: {.notes}

Pour les simulations de particules-mesh, on utilise une fonction d'interpolation pour peindre les particules sur la grille.

La methode s'appelle Cloud-in-Cell (CIC), qui est une méthode simple et rapide pour interpoler les particules sur une grille.

On part d'une liste de particules représentées par leurs coordonnées et leur masse.

**NEXT**

Ensuite on 'paint' chaque particules sur les cellules adjacentes.

Chaque particule contribue à la densité de masse de la cellule en fonction de sa distance à la cellule.

Par exemple dans cette exemple, la plus part des particules sont proche de la cellule (1 , 0) donc elle aura le plus de densité.

**NEXT**

On calcule les forces en utilisant la densité de masse interpolée sur la grille.
On obtient donc des gradients.

**NEXT**

Ensuite on interpole les forces sur les particules pour les mettre à jour.

Chaque gradient contribue au deplacement d'une particule en fonction de sa distance à la cellule.

**NEXT**

Cependant, cette méthode n'est pas compatible avec les simulations distribuées, car elle nécessite de lire les valeurs des cellules voisines, ce qui n'est pas possible si les cellules sont sur un autre GPU.
Ce qui se passe sur un seul GPU c'est l'application de la condition periodique. Ce qui veut dire que les particules qui sort d'un sous domain reviennent de l'autre coté du meme domaine.

Cela le reflète pas la réalité physique, et peut introduire des erreurs dans les simulations.

**NEXT**

Je montre dans le slide suivant quel impact cela peut avoir sur les simulations.

:::


## Halo exchange in distributed simulations

::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/HPC/depict_gathered.png){.absolute top=30 right=20 width="20%"}

::: 

::: {.fragment fragment-index=2 }

![](assets/HPC/depict_split.png){.absolute top=30 right=20 width="20%"}

::: 

:::{.r-stack}

:::{.fragment fragment-index=1 .fade-in-then-out}

![Initial Field](assets/Fields/initial_conditions_1024.png){.nostretch  width="400px"}

:::

:::{.fragment fragment-index=2 .fade-in-then-out}

:::{layout="[[1] , [1] , [1] , [1]]" layout-valign="center" layout-align="center"}

![First slice](assets/Fields/initial_conditions_0_no_halo.png){.nostretch fig-align="center" width="400px"}

![Second slice](assets/Fields/initial_conditions_1_no_halo.png){.nostretch fig-align="center" width="400px"}

![Third slice](assets/Fields/initial_conditions_2_no_halo.png){.nostretch fig-align="center" width="400px"}

![Fourth slice](assets/Fields/initial_conditions_3_no_halo.png){.nostretch fig-align="center" width="400px"}

:::

:::

:::{.fragment fragment-index=3 .fade-in-then-out}

:::{layout="[[1] , [1] , [1] , [1]]" layout-valign="center" layout-align="center"}

![First slice](assets/Fields/LPT_density_field_z0_0_no_halo.png){.nostretch fig-align="center" width="400px"}

![Second slice](assets/Fields/LPT_density_field_z0_1_no_halo.png){.nostretch fig-align="center" width="400px"}

![Third slice](assets/Fields/LPT_density_field_z0_2_no_halo.png){.nostretch fig-align="center" width="400px"}

![Fourth slice](assets/Fields/LPT_density_field_z0_3_no_halo.png){.nostretch fig-align="center" width="400px"}

:::

:::

:::{.fragment fragment-index=4 .fade-in-then-out}


![LPT Field](assets/Fields/LPT_density_field_z0_1024_no_halo.png){.nostretch fig-align="center" width="400px"}

:::


:::{.fragment fragment-index=5 .fade-in-then-out}

![LPT Field](assets/Fields/LPT_density_field_z0_1024.png){.nostretch fig-align="center" width="400px"}

:::

:::{.fragment fragment-index=6 .fade-in-then-out}

![CIC Kernel](assets/Fields/CIC/CIC_Padding.svg){.nostretch fig-align="center" width="600%"}


:::

:::{.fragment fragment-index=7}

![Halo Exchangel](assets/Fields/CIC/Halo_Exchange.svg){.nostretch fig-align="center" width="50%"}

:::


:::

:::{.fragment fragment-index=5}

<div style="position: absolute; top: 75px; left: 0px; width: 500px; font-size: 100%">

```python
from jaxdecomp import halo_exchange

halo_size = 128
field = halo_exchange(field, halo_extent=halo_size)

```

</div>

:::

::: {.notes}

On commence par voir un champs de densité gaussien de , qui est distribué sur 4 GPU.

Chaque GPU a une partie du champ de densité, et peut evoluer de manière indépendante.

Après avoir calculer les forces avec la FFT distribuée, on obtient un champ de densité qui est distribué sur les 4 GPU.

On fait evoluer les particules indépendamment, mais on voit que les particules qui sont proche des bords du domaine ne sont pas correctement traitées.

**NEXT**

On a egalement une solution pour ce problème, qui est l'échange de halo.
Ou un utilisateur peut definir une taille de halo, qui est la taille des cellules qui sont échangées entre les GPU.

**NEXT**

Concretement, pendant la phase de l'interpolaction CIC, on permet les particules d'être peintes sur des cellule plus grande que les cellules locales.

Puis on echange la partie des cellules qui sont dans le halo avec les autres GPU.

L'échange applique les conditions periodiques sur les bords du domaine, et permet de traiter les particules qui sont proche des bords du domaine.
Cela nous donne un resultat très similaire à une simulation sur un seul GPU.

:::

## Conclusion


:::{.solutionbox}

::::{.solutionbox-header}

Work already done

::::

::::{.solutionbox-body}

➢  &emsp;[jaxDecomp](https://github.com/DifferentiableUniverseInitiative/jaxDecomp) : Distributed FFT operations and halo exchange  **(Released)**\
➢  &emsp;[jaxPM](https://github.com/DifferentiableUniverseInitiative/jaxPM) : Distributed Particle-Mesh simulations **(Code)** \
➢  &emsp;[Furax](https://gitlab.in2p3.fr/scipol/furax) : Minimization of the spectral likelihood for CMB component separation **(Code)** \
➢  &emsp;[S2FFT](https://astro-informatics.github.io/s2fft/) : Accelerated spherical harmonics transforms **(Pull request)**

::::

:::

<br />

:::{.solutionbox}

::::{.solutionbox-header}

Work in progress

::::

::::{.solutionbox-body}

➢ &emsp;[Furax](https://gitlab.in2p3.fr/scipol/furax) (Continued) : Creation of a special optimiser that can handle multi-resolution and multi-patch data\
➢ &emsp;[jaxPM](https://github.com/DifferentiableUniverseInitiative/jaxPM) : Benchmarking and testing the scaling of the simulations

::::

:::

<br />

:::{.solutionbox}

::::{.solutionbox-header}

Future work

::::

::::{.solutionbox-body}

➢ &emsp; Optimized and distributed spherical harmonics transforms for CMB lensing\
➢ &emsp; Distributed probabilistic programming for hierarchical Bayesian Modeling

::::

:::

## Attended Conferences 


:::{.solutionbox}

::::{.solutionbox-header}

French Conferences


::::

::::{.solutionbox-body}

- **IAP** 2023 Machine Learning-
- **LSST France 2023** Lyon, France
- **LSST France 2024** Marseille, France **(Talk)**
- ***[Upcomming Conference]*** **IAP GDR CoPhy Tools 2024** Paris, France **(Talk)**

::::

:::

:::{.solutionbox}

::::{.solutionbox-header}

International Conferences

::::

::::{.solutionbox-body}

- **Moriond Cosmology 2023** La Thuile, Italy **(Poster)**
- **LSST DESC 2023** Zurich, Switzerland


::::

:::


## Papers

<br />
<br />

:::{.solutionbox}

::::{.solutionbox-header}

Current/Submitted papers

::::

::::{.solutionbox-body}

- Infrared Radiometric Image Classification and Segmentation of Cloud Structure Using ML <span style="color:darkblue;">**(Sommer et al. 2024, Published)**</span>
- **JaxDecomp**: A Distributed Fast Fourier Transform Library <span style="color:darkblue;">**(Kabalan et al. to be submitted soon)**</span>
- **Furax**: Optimization of the Large Scale Multiresolution Parametric Component Separation <span style="color:darkblue;">**(Kabalan et al., for end of 2024)**</span>
- **JaxPM**: A Differentiable Particle-Mesh Simulation <span style="color:darkblue;">**(Kabalan et al. in prep.)**</span>

::::

:::

<br />

:::{.solutionbox}

::::{.solutionbox-header}

Future papers

::::

::::{.solutionbox-body}

- **Spherical Harmonics for CMB component separation** <span style="color:darkblue;">(Lead author)</span>
- Distributed Probabilistic Programming for Hierarchical Bayesian Modeling

::::



:::

## Formations


<div style="font-size: 285%">

:::{.solutionbox}

::::{.solutionbox-header}

divers

::::


::::{.solutionbox-body}

- Euclid summer school (2023)
- AISSAI AstroInfo Hackathon 2023, Frejus, France
- **Physics informed neural networks** with the IDRIS team (Jean-zay super computer) **(2024)**

::::

:::

:::{.solutionbox}

::::{.solutionbox-header}

Cours Ecole Doctorale

::::

::::{.solutionbox-body}

- **QCD** with Matteo Cacciari **(2023)**

::::

:::

</div>
