---
title: "Massively Parallel Computing in Cosmology with JAX"

author: "Wassim Kabalan"

format:
  revealjs:
    theme: [default, css/custom.scss]
    incremental: false   
    transition: slide
    background-transition: slide
    presentation-size: max-scale
    slide-number: true
    template-partials:
      - css/title-slide.html
output: revealjs

code-block-border-left: "#31BAE9"
title-slide-attributes:
  data-background-image: "assets/JZ-JAX.png"
  data-background-size: fill
  data-background-opacity: "0.2"


logo1: '<div style="display: flex; justify-content: center; align-items: center; height: 100%; gap: 200px;">
  ![](assets/Logos/scipol.jpeg){width=18%}
  ![](assets/Logos/APC.png){width=15%}  
  ![](assets/Logos/AstroDeep-2.png){width=15%}
</div>'

---


## Goals for This Presentation {style="font-size: 28px;"}

:::{.solutionbox}

::: {.solutionbox-header style="font-size: 26px;"}

Key Goals

:::

::::{.solutionbox-body style="font-size: 24px;"}

- **Motivation**: Understand the value of full-field inference in cosmology.
<br />
<br />
- **Background on Parallelism**: Learn how GPUs work and key scaling concepts.
<br />
<br />
- **When to Use (and Avoid) Parallelism**: Discover the benefits and limitations.
<br />
<br />
- **Scaling with JAX**: Explore techniques for scaling computations in JAX.
<br />
<br />
- **Distributed Tools for Cosmology**: Get an overview of multi-node packages like `jaxDecomp` and `jaxPM`.
<br />
<br />
- **Hands-On Practice**: Apply these concepts in interactive notebooks.

::::

:::

---

## <span style="color: white ;">Motivation: Cosmology in the exascale era </span> {background-image="assets/rubin.jpg"} 

<br />

:::{.columns}

::: {.column width="60%"}

:::{.solutionbox}

::: {.solutionbox-header style="font-size: 22px;"}

Upcoming Surveys and Massive Data in Cosmology

:::

::::{.solutionbox-body style="font-size: 21px;"}

- **Massive Data Volume**: LSST will generate **20 TB of raw data per night** over **10 years**, totaling **60 PB**.
- **Catalog Size**: The processed LSST catalog database will reach **15 PB**.

::::


:::


:::{.solutionbox}

::: {.solutionbox-header style="font-size: 22px;"}

Cosmological Models and Pipelines
:::

::::{.solutionbox-body style="font-size: 21px;"}
 - Cosmological simulations and forward modeling can easily reach multiple terabytes in size.
 - We need to scale up cosmological pipelines to handle these data volumes effectively.


::::

:::

:::

:::

---


## Forward Modeling in Cosmology {style="font-size: 20px;"}

:::{.columns}

::: {.column width="50%"}

#### Weak Lensing Model

- **Prediction**:
    - A simulator generates observations from initial conditions and cosmological parameters.

- **Inference**:
    - The simulated results are compared with actual observations.
    - Optimal initial conditions and parameters are inferred to closely match the observed data.


:::{.solutionbox}

::: {.solutionbox-header style="font-size: 20px;"}

Scaling Challenges

:::

::::{.solutionbox-body style="font-size: 19px;"}

- **Resolution Today**: Simulations currently use around **250,000 to 130 million particles**.
- **Ideal Resolution**: Billion-particle simulations are necessary for high accuracy.
- **Software**: Tools like **JaxPM** or **PMWD** support up to ~130 million particles on a single GPU.

::::

:::

:::

::: {.column width="50%"}

:::{.r-stack}

::: {.fragment fragment-index=1 .fade-out}

![](assets/FFI/Forward-Model.svg){fig-align="center" width="75%"}

:::

::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/FFI/Forward-Model-FinalField.svg){fig-align="center" width="75%"}

:::

::: {.fragment fragment-index=2 .fade-in-then-out}

![](assets/FFI/Forward-Model-Constraints.svg){fig-align="center" width="75%"}

:::

:::

:::

:::


:::{.notes}

- **Simulations in Cosmology**: These simulations model the universe's evolution to reproduce observed structures, helping infer parameters like dark matter density, dark energy, and other cosmological constants.
- **Resolution Requirement**: Simulations with more particles provide finer details, making convergence maps closer to observed data. Current particle counts (130 million) are still limited compared to the **billion-particle simulations** required for accurate cosmological inference.

:::


# Background on Parallel Computing

---

## Background: How GPUs Work

:::{.columns}

::: {.column width="50%"}

<br />

#### **Massive Thread Count**
- GPUs are designed with thousands of threads.
- Each core can handle many data elements simultaneously.
- The main bottleneck is memory throughput.


::: {.fragment fragment-index=1}

<br />


#### **Optimizing Throughput with Multiple GPUs**:
- Computation is often only a fraction of total processing time.

:::

::: {.fragment fragment-index=3}

- Using multiple GPUs increases overall data throughput, enhancing performance and reducing idle time.

:::

:::

::: {.column width="50%"}

![GPU threads](assets/HPC/GPUS_CPU.svg){fig-align="center" width="80%"}

:::{.r-stack}

::: {.fragment fragment-index=1 .fade-in-then-out}

![Single GPU throughput](assets/HPC/batching_single_gpu.svg){fig-align="center" width="55%"}

:::

::: {.fragment fragment-index=2 .fade-in-then-out}

![Saturated GPU](assets/HPC/saturated_gpu.svg){fig-align="center" width="60%"}

:::

::: {.fragment fragment-index=3 .fade-in-then-out}

![Multiple GPUs throughput](assets/HPC/multi_gpu_saturate.svg){fig-align="center" width="70%"}

:::

:::

:::

:::

::: {.notes}


- **Massive Thread Count**: GPUs have thousands of threads, allowing them to process large datasets in parallel. Each core can handle multiple data elements, making them ideal for parallel computing tasks.
  
- **Memory Throughput Bottleneck**: The true bottleneck in GPU performance is often memory throughput. Even with many threads, GPU efficiency can drop if data isn't supplied quickly enough.

- **Optimizing Throughput with Multiple GPUs**: In many cases, computation is only a small portion of the total processing time. By adding more GPUs, we increase possible data throughput, reduce idle time, and ultimately improve performance for large workloads.

:::

---

## Background: Types of Data parallelism  {style="font-size: 22px;"}

:::{.columns}

::: {.column width="40%"}


#### **Data Parallelism**
- **Simple Parallelism**: Each device processes a different subset of data independently.

::: {.fragment fragment-index=1}

- **Data Parallelism with Collective Communication**:
    - Devices process data in parallel but periodically share results (e.g., for gradient averaging in training).

:::

::: {.fragment fragment-index=2}

#### **Task Parallelism**
- Each device handles a different part of the computation.
- The computation itself is divided between devices.
- Is generally more complex than data parallelism.

:::

:::


::: {.column width="60%"}

:::{.r-stack}

::: {.fragment fragment-index=1 .fade-out}

![Simple Data Parallelism](assets/HPC/simple_data_parallel.svg){fig-align="center" width="120%"}

:::

::: {.fragment fragment-index=1 .fade-in}

![Data Parallelism with Communication](assets/HPC/data_parallel_collective_comm.svg){fig-align="center" width="120%"}

:::

:::

::: {.fragment fragment-index=2 .fade-in}

![Task Parallelism](assets/HPC/task_parallel.svg){fig-align="center" width="120%"}

:::

:::

:::

::: {.notes}

- **Data Parallelism (Simple Parallelism)**: 
    - In simple data parallelism, each device processes a distinct subset of the data independently, without needing to interact with other devices during computation. This approach works best when:
      - The dataset has to be evenly split across devices.
      - Each subset of data can be processed in isolation, which reduces the complexity and overhead of communication.
      - Example in Cosmology: Simple data parallelism might be used in parameter estimation tasks where each GPU computes likelihoods for different portions of the dataset independently.

- **Data Parallelism with Collective Communication**:
    - In many applications, devices working in parallel need to share intermediate results to ensure consistency and accuracy. For example, in distributed neural network training, devices need to periodically share gradient information to keep model weights in sync.
      - This form of parallelism is more complex than simple data parallelism, as it involves synchronizing data across devices at intervals, typically through collective communication operations.
      - **Challenges**:
         - Collective operations like gradient averaging can become bottlenecks as they require frequent communication between devices, which may impact scaling efficiency.
         - In cosmology, gradient sharing can be particularly useful in distributed optimization routines where multiple GPUs are used to train machine learning models on large cosmological datasets.

- **Task Parallelism**:
    - In task parallelism, each device handles a unique segment of the overall computation pipeline. This approach is ideal when the computation itself is modular and can be split into discrete tasks that contribute independently to the final result.
      - Unlike data parallelism, task parallelism is often sequential in nature, where each device performs a specific part of a larger computational workflow.
      - Example in Cosmology: Task parallelism can be applied in cosmological simulations where certain calculations are interdependent. For example:
        - One device might handle the gravitational calculations, while another processes hydrodynamic interactions, with each task feeding results to the next stage.
      - **Complexity**:
         - Task parallelism generally requires more significant changes to the code structure and may involve custom communication protocols between tasks.
         - This approach is commonly used in model pipelines that require heavy sequential processing, such as large language model (LLM) training (e.g., Gemini or ChatGPT) or in cosmology, where different physical components need distinct treatments in simulations.

- **Summary**: 
    - Data parallelism is typically easier to implement but may be limited by the need for collective communication.
    - Task parallelism is more flexible in terms of workload distribution but often requires substantial restructuring of code and specific coordination between tasks, especially in long, sequential workflows.

:::

## Why Should You Use Parallelism?

<br />

#### Simple cases

- **Data Parallelism (Simple)** ✅
    - If your pipeline resembles simple data parallelism, then parallelism is a good idea.

- **Data Parallelism with Simple Collectives** ✅
    - Simple collectives (e.g., gradient averaging) can be easily expressed in JAX, allowing devices to share intermediate results.

::: {.fragment fragment-index=1}

#### Complex cases


- **Non-splittable Input (e.g., N-body Simulation Fields)** ⚠️
    - When the input is not easily batchable, like a field in an N-body simulation.
:::

::: {.fragment fragment-index=2}

- **Task Parallelism** ⚠️

    - Useful for **long sequential cosmological pipelines** where each device handles a unique task in the sequence.
    - More common in training complex models (e.g., LLMs like Gemini or ChatGPT).

:::


:::{.notes}

- **Data Parallelism**: Ideal if your pipeline can be split into independent data chunks. JAX makes it easy to implement with `pmap` for distributing computations.
  
- **Collectives in Data Parallelism**: Some tasks, like gradient averaging, require simple collective communication between devices. JAX provides tools to express these collectives efficiently.

- **Non-Batchable Input**: In cases where data cannot be split into batches (e.g., field-based data in simulations), you may need to implement more complex collectives to coordinate data across devices.

- **Task Parallelism**: Used in sequential pipelines where each device performs a unique task. This approach is more complex and requires substantial restructuring, making it common in large language model training.

:::

---

## When Not to Use Parallelism 

:::{.columns}

::: {.column width="60%"}

#### To Keep in Mind

- **Data Fits on a Single GPU**  

::: {.fragment fragment-index=1}

- **Need for Complex Collectives**  
    - Additional GPUs can add complexity and may not yield enough performance improvement.

- **Task Parallel Model**  
    - Changing the pipeline or adapting to new devices often requires significant rewrites.

:::

:::

::: {.column width="40%"}

![Not fully used GPU](assets/HPC/parallelism_limitations.svg){fig-align="center" width="50%"}

:::

:::

::: {.fragment fragment-index=2}

:::{.solutionbox}

::: {.solutionbox-header}

**Consider Scaling to multiple GPUs if:**

:::

::::{.solutionbox-body style="font-size: 21px;"}

- You have a single-GPU prototype that’s working but needs significant runtime reduction.
- Has a significant impact on your results.
  - Using multiple GPUs can significantly decrease execution time.
  - OR You have non-splittable input (e.g., fields in a cosmological simulation) that is crucial for your results.

::::

:::

:::

:::{.notes}

- **Single-GPU Sufficiency**: If all data fits on one GPU, adding more GPUs may introduce unnecessary overhead, reducing overall performance.
  
- **Complex Collectives**: Complex collective communication (like frequent data sharing) can slow down computations. In cases where collectives are challenging, stick to a single GPU if possible.

- **Task Parallelism Complexity**: Task parallel models are generally harder to adjust. Each device performs a unique task, making it more challenging to scale or modify the pipeline.

:::


---


## How to Measure Scaling for Parallel Codes {style="font-size: 22px;"}

:::{.columns}

::: {.column width="50%"}

<br />

#### **Strong Scaling**
- Increasing the number of GPUs to reduce runtime for a fixed data size.

<br />

::: {.fragment fragment-index=1}

#### **Weak Scaling**
- Increasing data size with a fixed number of GPUs.

:::

:::{.fragment fragment-index=2}

:::{.solutionbox}

:::{.solutionbox-header style="font-size: 20px;"}

Evaluating Scaling Performance

:::

::::{.solutionbox-body style="font-size: 19px;"}

- **Strong Scaling**: Assesses performance as more GPUs are added to a fixed dataset. *Danger Zone*: Indicates the distributed code is not scaling efficiently.

- **Weak Scaling**: Tests how the code handles increasing data sizes with a fixed number of GPUs. *Danger Zone*: Suggests underlying scaling issues with the code itself.

::::

:::

:::

:::

::: {.column width="50%"}

![Strong Scaling](assets/HPC/STRONG_SCALING.png){fig-align="center" width="70%"}

:::{.fragment fragment-index=1}

![Weak Scaling](assets/HPC/WEAK_SCALING.png){fig-align="center" width="70%"}

:::

:::

:::

:::{.notes}

- **Strong Scaling**: Assesses performance as more GPUs are added to a fixed dataset. *Danger Zone*: Indicates the distributed code is not scaling efficiently.

- **Weak Scaling**: Tests how the code handles increasing data sizes with a fixed number of GPUs. *Danger Zone*: Suggests underlying scaling issues with the code itself.

:::

---

## Environmental Impact of High-Performance Computing {style="font-size: 21px;"}

:::{.r-stack}

:::{.fragment fragment-index=1 .semi-fade-out}

:::{.r-stack-item}

:::{.columns}

::: {.column width="50%"}

#### **Perlmutter Supercomputer (NERSC)**
- **Location**: NERSC, Berkeley Lab, California, USA
- **Compute Power**: ~119 PFlops
- **GPUs**: 6,144 NVIDIA A100 GPUs (Phase 1)
- **Total Nodes**: 1,536 CPU nodes + 6,159 GPU nodes
- **Power Draw**: ~3.2 MW/hr

<br />

#### **Jean Zay Supercomputer (IDRIS)**
- **Location**: IDRIS, France
- **Compute Power**: ~126 PFlops (FP64), 2.88 EFlops (BF/FP16)
- **GPUs**: 3,704 GPUs, including V100, A100, and H100
- **Power Draw**: ~1.4 MW/hr on average (as of September, without full H100 usage), leveraging France’s renewable energy grid.

:::

::: {.column width="50%"}

![Perlmutter Supercomputer](assets/HPC/perlmutter_supercomputer.png){fig-align="center" width="80%"}

![Jean Zay Supercomputer](assets/HPC/jean_zay_photo.png){fig-align="center" width="80%"}

:::

:::

:::

:::

:::{.fragment fragment-index=1}

:::{.r-stack-item}

:::{.solutionbox}

::: {.solutionbox-header style="font-size: 26px;"}

**Environmental Benefits of Efficient Parallel Computing**

:::

::::{.solutionbox-body style="font-size: 24px;"}

<br />

- Higher throughput moves computations closer to peak FLOPS.
<br />
- Operating near peak FLOPS ensures more effective use of computational resources.
<br />
- More computations are achieved per unit of energy, improving energy efficiency.

<br />

::::

:::

:::

:::

:::

::: aside
Credit: Laurent Leger from IDRIS
:::

:::{.notes}

- **Perlmutter**: A hybrid CPU-GPU setup with over 6,000 A100 GPUs, designed for both machine learning and scientific tasks. It operates with a high power draw of ~3.2 MW/hr, but the energy-per-FLOP efficiency is optimized through extensive GPU utilization.
  
- **Jean Zay’s Environmental Efficiency**: With its renewable energy source and GPU partitions, Jean Zay achieves significant computational power (~126 PFlops) while keeping average power consumption at ~1.4 MW/hr. The partitioned GPU setup (V100, A100, and soon H100) allows Jean Zay to scale efficiently for a variety of workloads without excessive energy use.

:::

# How to Scale in JAX {style="font-size: 40px; align=center;"}

---

## Why JAX for Distributed Computing?



- **Distributed Computing Isn’t New**:
   - Tools like **MPI** and **OpenMP** are used extensively.
   - ML frameworks like **TensorFlow** and **PyTorch** offer distributed training.
   - **DiffEqFlux.jl** **Horovod** and **Ray** 

::: {.fragment fragment-index=1}

- **Familiar and Accessible API**:
   - JAX offers a **NumPy-like API** that is both accessible and intuitive.
   - Python users can leverage parallelism without needing in-depth knowledge of low-level parallel frameworks like MPI.

:::

::: {.fragment fragment-index=2}

:::{.solutionbox}

::: {.solutionbox-header}

**Key Points**

:::

::::{.solutionbox-body}

- **Pythonic Scalability**: JAX allows you to write scalable, **pythonic code** that is compiled by XLA for performance.
- **Automatic Differentiation**: JAX offers a trivial way to write diffrentiable distributed code.
- Same code runs on anything from a laptop to multi node supercomputer.

::::

:::

::: 

:::{.notes}

- **Traditional Distributed Computing**: MPI and OpenMP have been essential tools for achieving high-performance distributed computing in fields like cosmology. These frameworks provide fine control but often require specific parallel programming expertise.
  
- **Accessibility and Familiarity**: JAX's familiar, high-level syntax lowers the barrier to entry, bringing distributed computing within reach of Python users without the need to manage intricate MPI or OpenMP settings.

- **JAX’s Unique Advantages**: Through XLA compilation, JAX not only scales code efficiently but also integrates differentiability, crucial for machine learning and simulations that require backpropagation. This blend of performance and flexibility sets JAX apart for scientific and AI applications.

### Talking Points on Alternative Framework Limitations

- **Open MPI**: Primarily optimized for CPU-based parallelism, making it less effective on GPUs where scientific workloads in JAX often run. This can limit its efficiency for cosmology applications that leverage GPU acceleration.

- **ML Frameworks (PyTorch, TensorFlow)**: While powerful for machine learning, these frameworks are ML-centric and don’t natively support the arbitrary scientific functions often needed in cosmology. Customizing these frameworks for scientific use cases requires significant additional effort.

- **DiffEqFlux.jl (Julia)**: While Julia’s ecosystem is growing, it’s still limited compared to Python, particularly for scientific computing and distributed applications. This smaller ecosystem can make it harder to find compatible tools and libraries for complex cosmological simulations.

- **Horovod and Ray**: Neither is natively differentiable, which means they rely on external frameworks (e.g., PyTorch or TensorFlow) for differentiation. This lack of built-in differentiability adds overhead and complexity for workflows that require gradient-based optimization, a key feature that JAX integrates seamlessly.


For questions

### PyTorch Distributed
- **Complex Setup**: Requires more effort to configure distributed training, especially outside deep learning.
- **Performance Overhead**: Lacks JAX’s XLA compilation, which can lead to inefficiencies in scientific applications.
- **Limited Scientific Libraries**: PyTorch’s ecosystem is growing, but it still lacks the depth of JAX for scientific and physics-based computing.

### TensorFlow Distributed
- **Complex and Verbose**: Distributed setup with `tf.distribute.Strategy` is often more cumbersome and requires multiple API layers.
- **Less Flexibility with Gradients**: Limited flexibility in complex gradient computations compared to JAX’s functional approach.
- **Under-Optimized for Scientific Workflows**: XLA support is not as performant in scientific HPC compared to JAX.

### Ray with Auto-Differentiation
- **Not Natively Differentiable**: Ray relies on external libraries (like PyTorch and TensorFlow) for differentiation, adding communication and synchronization overhead.
- **Focus on General Purpose Computing**: Lacks specific optimizations for HPC environments and scientific computing.
- **Limited Low-Level Hardware Control**: Ray abstracts device management, reducing optimization potential in specialized HPC setups.

### DiffEqFlux.jl (Julia)
- **Limited Ecosystem**: Julia’s ecosystem is smaller and less mature, especially for scientific computing.
- **Developing Distributed Support**: Distributed computing in Julia is still evolving and less robust than in Python.
- **Learning Curve**: Julia has a steeper learning curve for Python-based teams, and integration with Python infrastructure can be difficult.

### Mesh TensorFlow
- **Specialized for Transformers**: Primarily designed for partitioning large transformer models, limiting flexibility in other scientific applications.
- **Complex Configuration**: Mesh configuration is often challenging and may be a barrier for scientific users.
- **Tied to TensorFlow**: Mesh TensorFlow’s dependency on TensorFlow makes it less intuitive for scientific computing compared to JAX’s NumPy-like API.

### Horovod (Multi-Framework)
- **Optimized for Data Parallelism in ML**: Primarily suited for data parallelism in ML, not as adaptable for complex scientific workflows.
- **External Library Dependence**: Requires frameworks like PyTorch or TensorFlow for auto-differentiation, adding performance overhead.
- **Limited Scientific Integration**: Does not integrate well with libraries focused on physical simulations, whereas JAX has a growing ecosystem for such applications.

### Key Advantages of JAX
- **Unified, Pythonic API**: JAX’s intuitive API combines ease of use with the power of distributed computing.
- **XLA Compilation for Efficiency**: Optimized for performance across devices, making it highly suitable for HPC environments.
- **Native Differentiability**: Differentiability is built-in and seamlessly integrated with distributed workflows, providing a smooth experience for scientific applications.

:::

---

## Expressing Parallelism in JAX (Simple parallelism) {auto-animate="true"}

#### Example of computing a gaussian from data Points

```Python
import jax
import jax.numpy as jnp


def gaussian(x, mean, variance):
      """Compute Gaussian distribution for given x, mean, and variance."""
      coefficient = 1.0 / jnp.sqrt(2 * jnp.pi * variance)
      exponent = -((x - mean) ** 2) / (2 * variance)
      return coefficient * jnp.exp(exponent)

mean = 0.0
variance = 1.0
x = jnp.linspace(-5, 5, 128)
result = gaussian(x, mean, variance)

```

## Expressing Parallelism in JAX (Simple parallelism) {auto-animate="true" visibility="uncounted"}

#### Example of computing a gaussian from data Points


```Python
import jax
import jax.numpy as jnp
from jax.debug import visualize_array_sharding

def gaussian(x, mean, variance):
      """Compute Gaussian distribution for given x, mean, and variance."""
      coefficient = 1.0 / jnp.sqrt(2 * jnp.pi * variance)
      exponent = -((x - mean) ** 2) / (2 * variance)
      return coefficient * jnp.exp(exponent)

mean = 0.0
variance = 1.0
x = jnp.linspace(-5, 5, 128)
result = gaussian(x, mean, variance)
visualize_array_sharding(x)
visualize_array_sharding(result)

```

<br />

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 0  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>

<br />

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 0  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>


## Expressing Parallelism in JAX (Simple parallelism) {auto-animate="true" visibility="uncounted"}

#### Example of computing a gaussian from data Points


```Python
import jax
import jax.numpy as jnp
from jax.sharding import PartitionSpec as P, NamedSharding
from jax.debug import visualize_array_sharding

assert jax.device_count() == 8

mesh = jax.make_mesh((8,), ('x'))
sharding = NamedSharding(mesh , P('x'))

def gaussian(x, mean, variance):
      """Compute Gaussian distribution for given x, mean, and variance."""
      coefficient = 1.0 / jnp.sqrt(2 * jnp.pi * variance)
      exponent = -((x - mean) ** 2) / (2 * variance)
      return coefficient * jnp.exp(exponent)

mean = 0.0
variance = 1.0
x = jnp.linspace(-5, 5, 128)
x = jax.device_put(x, sharding)
result = gaussian(x, mean, variance)
visualize_array_sharding(x)
visualize_array_sharding(result)

```

<br />

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 0  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  GPU 1  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  GPU 2  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  GPU 3  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7cb94">  GPU 4  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  GPU 5  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  GPU 6  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  GPU 7  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">         </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7cb94">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">         </span>
</pre>

<br />

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 0  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  GPU 1  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  GPU 2  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  GPU 3  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7cb94">  GPU 4  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  GPU 5  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  GPU 6  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  GPU 7  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">         </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7cb94">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">         </span>
</pre>

---

## Expressing Parallelism in JAX (Simple parallelism) {auto-animate="true" visibility="uncounted"}

#### Example of computing a gaussian from data Points


```{.python code-line-numbers="3,8,9,20"}
import jax
import jax.numpy as jnp
from jax.sharding import PartitionSpec as P, NamedSharding
from jax.debug import visualize_array_sharding

assert jax.device_count() == 8

mesh = jax.make_mesh((8,), ('x'))
sharding = NamedSharding(mesh , P('x'))

def gaussian(x, mean, variance):
  """Compute Gaussian distribution for given x, mean, and variance."""
  coefficient = 1.0 / jnp.sqrt(2 * jnp.pi * variance)
  exponent = -((x - mean) ** 2) / (2 * variance)
  return coefficient * jnp.exp(exponent)

mean = 0.0
variance = 1.0
x = jnp.linspace(-5, 5, 128)
x = jax.device_put(x, sharding)
result = gaussian(x, mean, variance)
visualize_array_sharding(x)
visualize_array_sharding(result)

```

<br />

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 0  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  GPU 1  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  GPU 2  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  GPU 3  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7cb94">  GPU 4  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  GPU 5  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  GPU 6  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  GPU 7  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">         </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7cb94">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">         </span>
</pre>

<br />

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 0  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  GPU 1  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  GPU 2  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  GPU 3  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7cb94">  GPU 4  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  GPU 5  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  GPU 6  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  GPU 7  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">         </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7cb94">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">         </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">         </span>
</pre>

## Expressing Parallelism in JAX (Using collectives) {auto-animate="true"}

<br />

#### Example of SGD with Gradient averaging

Example from Jean-Eric's tutorial

```Python
import jax
import jax.numpy as jnp

@jax.jit  
def gradient_descent_step(p, xi, yi, lr=0.1):
  gradients = jax.grad(loss_fun)(p, xi, yi)
  return p - lr * gradients

def minimzer(loss_fun, x_data, y_data, par_init, method, verbose=True):
  ...
# Example usage
par_mini_GD = minimzer(
  loss_fun, 
  x_data=xin, 
  y_data=yin, 
  par_init=jnp.array([0., 0.5]), 
  method=partial(gradient_descent_step, lr=0.5), 
  verbose=True
)

```

---

## Expressing Parallelism in JAX (Using collectives) {auto-animate="true" visibility="uncounted"}

<br />

#### Example of SGD with Gradient averaging

Example from Jean-Eric's tutorial

```{.python code-line-numbers="3,4,8,9,12-16,21,22|8,9|21,22|12-16"}
import jax
import jax.numpy as jnp
from jax.experimental.shard_map import shard_map
from jax.sharding import PartitionSpec as P, NamedSharding

assert jax.device_count() == 8

mesh = jax.make_mesh((8,), ('x'))
sharding = NamedSharding(mesh , P('x'))

@jax.jit 
@partial(shard_map, mesh = mesh , in_specs=P('x'), out_spec=P('x'))
def gradient_descent_step(p, xi, yi, lr=0.1):
      per_device_gradients = jax.grad(loss_fun)(p, xi, yi)
      avg_gradients = jax.lax.pmean(per_device_gradients, axis_name='x')
      return p - lr * avg_gradients

def minimzer(loss_fun, x_data, y_data, par_init, method, verbose=True):
     ...
  # Example usage
xin = jax.device_put(xin, sharding)
yin = jax.device_put(yin, sharding)
par_mini_GD = minimzer(
        loss_fun, 
        x_data=xin, 
        y_data=yin, 
        par_init=jnp.array([0., 0.5]), 
        method=partial(gradient_descent_step, lr=0.5), 
        verbose=True
    )
```

---


## JAX Collective Operations for Parallel Computing {style="font-size: 21px;"}

### Overview of JAX Collectives in `jax.lax.p*` Functions

- **`lax.pmean`**  
  - Computes the mean of arrays across devices. Useful for averaging gradients in distributed training.
  
- **`lax.ppermute`**  
  - Permutes data across devices in a specified order. Very usefull in cosmological simulations.

- **`lax.all_to_all`**  
  - Exchanges data between devices in a controlled manner. Useful for custom data exchange patterns in distributed computing.

- **`lax.pmax` / `lax.pmin`**  
  - Computes the element-wise maximum/minimum across devices. Often used in situations where you want to find the maximum or minimum of a distributed dataset.

- **`lax.psum`**  
  - Sums arrays across devices. Commonly used for aggregating gradients or other values in distributed settings.

- **`lax.pall`**  
  - Checks if all values across devices are `True`. Often used for collective boolean checks across distributed data.

:::{.notes}

- **Gradient Aggregation**: In distributed training, `pmean` is commonly used to average gradients from multiple devices, ensuring each device updates with the same gradient.
- **Logical Collectives**: Operators like `pand`, `por`, and `pall` allow distributed boolean logic operations, which can help in synchronization or conditional checks in parallel code.
- **Flexible Data Distribution**: `ppermute` allows data rearrangement across devices, making it useful in more complex parallelism setups or for rearranging distributed data for specific computations.

:::

# Towards Infinite Scalability with JAX 

---

## A Node vs a Supercomputer {style="font-size: 21px;"}

:::{.columns}

::: {.column width="60%"}

### Differences in Scale 

- **Single GPU**:
    - Maximum memory: **80 GB**

- **Single Node (Octocore)**:
    - Maximum memory: **640 GB**
    - Contains multiple GPUs (e.g., 8 A100 GPUs) connected via high-speed interconnects.

- **Multi-Node Cluster**:
    - **Infinite Memory** 🎉
    - Connects multiple nodes, allowing scaling across potentially thousands of GPUs.


:::{.solutionbox} 

::: {.solutionbox-header style="font-size: 20px;"}

Multi-Node scalability with Jean Zay

:::

::::{.solutionbox-body style="font-size: 20px;"}

- Up to 30TB of memory using all 48 nodes of Jean Zay
- Is enough to run a 15 billion particle simulation.

::::

:::

:::

::: {.column width="40%"}

::: {layout-nrows=3}

![](assets/HPC/single_A100.png){fig-align="center" width="40%"}

![@credit: NVIDIA](assets/HPC/node_A100.png){fig-align="center" width="40%"}


![@credit: servethehome.com](assets/HPC/cluster.jpg){fig-align="center" width="40%"}

:::

:::

:::

:::{.notes}

- **Single GPU**: GPUs have powerful cores but are limited by memory. With a max memory of 80 GB, they are ideal for tasks that fit within this memory constraint, often used for model training or inference.
  
- **Single Node (Octocore)**: An octocore node can host multiple GPUs (e.g., 8 GPUs) and has larger memory (up to 640 GB), enabling it to handle larger datasets. This setup is common in high-performance servers.

- **Multi-Node Cluster**: By connecting nodes in a distributed cluster, we achieve "infinite" scalability in terms of memory and compute. JAX can take advantage of this via distributed parallelism, making it ideal for cosmological simulations and other large-scale scientific computations.

:::

---

## Scaling JAX on a Single GPU vs. Multi-Host Setup

:::{.columns}

::: {.column width="50%"}

#### Single GPU Code

```python
x = jnp.linspace(-5, 5, 128)
mean = 0.0
variance = 1.0
result = gaussian(x, mean, variance)
```

<br />

#### Multi-GPU Code

```python
mesh = jax.make_mesh((8,), ('x'))
sharding = NamedSharding(mesh , P('x'))
x = jnp.linspace(-5, 5, 128)
x = jax.device_put(x, sharding)
mean = 0.0
variance = 1.0
result = gaussian(x, mean, variance)
```

<br />

#### Multi-Host Code

![](assets/Symboles/question-mark-512.png){fig-align="center" width="20%"}


:::

::: {.column width="50%"}

:::{layout-nrows=3 layout-align="center"}

![](assets/HPC/jax-1gpu.svg){fig-align="center" width="15%"}

![](assets/HPC/jax-1node.svg){fig-align="center" width="25%"}


![](assets/HPC/jax-multi-node.svg){fig-align="center" width="70%"}

:::

:::

:::


:::{.notes}

- **Single GPU Code**: This example demonstrates using `jax.pmap` for parallel computation on a single GPU.
- **Multi-Host Setup**: The code placeholder highlights the need for additional configuration to distribute JAX computations across multiple hosts, allowing for large-scale distributed workloads.
- **Single GPU vs Multi-Host**: JAX makes it easy to parallelize across local devices (GPUs on the same host), but multi-host configurations require further setup and are used for even larger-scale tasks.

:::

---

## Scaling JAX on a Single GPU vs. Multi-Host Setup {style="font-size: 22px;" auto-animate="true" visibility="uncounted"}

#### A JAX process per GPU
<br />

:::{.columns}

::: {.column width="50%"}

#### Requesting a slurm job

```bash
$ salloc --gres=gpu:8 --ntasks-per-node=1 --nodes=1
```

<br />

```{.python filename="multi-host-jax.py"}
import jax

mesh = jax.make_mesh((4,), ('x'))
sharding = NamedSharding(mesh , P('x'))

def gaussian(x, mean, variance):
    ...
mean = 0.0
variance = 1.0
x = jnp.linspace(-5, 5, 128)
x = jax.device_put(x, sharding)
result = gaussian(x, mean, variance)
visualize_array_sharding(x)
visualize_array_sharding(result)

```

<br />

#### Running with srun

```bash
$ srun python multi-host-jax.py
```

:::

::: {.column width="50%"}

:::{.r-stack}

![](assets/HPC/jax-1node.svg){fig-align="center" width="100%"}

:::

:::

:::

---

## Scaling JAX on a Single GPU vs. Multi-Host Setup {style="font-size: 22px;" auto-animate="true" visibility="uncounted"}

#### A JAX process per GPU
<br />

:::{.columns}

::: {.column width="50%"}

#### Requesting a slurm job

```bash
$ salloc --gres=gpu:8 --ntasks-per-node=8 --nodes=2
```
<br />

```{.python filename="multi-host-jax.py" code-line-numbers="|2-3|10-11"}
import jax
jax.distributed.initialize()
mesh = jax.make_mesh((16,), ('x'))
sharding = NamedSharding(mesh , P('x'))

def gaussian(x, mean, variance):
    ...
mean = 0.0
variance = 1.0
x = jnp.linspace(-5, 5, 128)
x = jax.device_put(x, sharding) ❌ # DOES NOT WORK
result = gaussian(x, mean, variance)
visualize_array_sharding(x)
visualize_array_sharding(result)

```
<br />


#### Running with srun

```bash
$ srun -n 8 python multi-host-jax.py
```

:::

::: {.column width="50%"}

:::{.r-stack}

![](assets/HPC/jax-multi-node.svg){fig-align="center" width="100%"}

:::

:::

:::

---

## Scaling JAX on a Single GPU vs. Multi-Host Setup {style="font-size: 22px;" auto-animate="true" visibility="uncounted"}

#### A JAX process per GPU
<br />

:::{.columns}

::: {.column width="50%"}

#### Requesting a slurm job

```bash
$ salloc --gres=gpu:8 --ntasks-per-node=8 --nodes=2
```
<br />

```{.python filename="multi-host-jax.py" code-line-numbers="10-11"}
import jax
jax.distributed.initialize()
mesh = jax.make_mesh((16,), ('x'))
sharding = NamedSharding(mesh , P('x'))

def gaussian(x, mean, variance):
    ...
mean = 0
variance = 1.0
x = jnp.linspace(-5, 5, 128)
x = jax.device_put(x, sharding) ❌ # DOES NOT WORK
result = gaussian(x, mean, variance)
visualize_array_sharding(x)
visualize_array_sharding(result)

```
<br />


#### Running with srun

```bash
$ srun -n 8 python multi-host-jax.py
```

:::

::: {.column width="50%"}

:::{.r-stack}

![](assets/HPC/jax-multi-node.svg){fig-align="center" width="100%"}

:::


:::{.solutionbox}

:::{.solutionbox-header style="font-size: 18px;"}
CAUTION ⚠️
:::

:::{.solutionbox-body style="font-size: 18px;"}

- `jax.device_put` does not work with multi-host setups.
- Allocating a jax numpy array does not have the same behavior as single node setups.

:::

:::

:::

:::

---

## Loading Data in JAX in a Multi-Host Setup {style="font-size: 22px;" auto-animate="true"}

#### A JAX process per GPU
<br />

```python
import jax
jax.distributed.initialize()

assert jax.device_count() == 16

x = jnp.linspace(-5, 5, 128)
visualize_array_sharding(x)
```

<br />

:::{.columns}

::: {.column width="50%"}

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 0  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 2  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 1  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 3  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 14  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 8  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 7  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>

:::

::: {.column width="50%"}

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 5  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 6  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 4  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 15  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 12  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 13  </span>
<span style="color: #ffffff; text-decoration-c²olor: #ffffff; background-color: #393b79">         </span>
</pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  GPU 11  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">         </span>
</pre>

:::

:::

---

## Loading Data in JAX in a Multi-Host Setup {style="font-size: 22px;" auto-animate="true" visibility="uncounted"}

#### A JAX process per GPU
<br />

```{.python filename="multi-host-jax.py" code-line-numbers="|7-10|13-14|"}
import jax
jax.distributed.initialize()

mesh = jax.make_mesh((16,) , ('x',))
sharding = NamedSharding(mesh , P('x'))

def distributed_linspace(start, stop, num):
    def local_linspace(indx):
        return np.linspace(start, stop, num)[indx]
    return jax.make_array_from_callback(shape=(num,), sharding=sharding,data_callback=local_linspace)

x = distributed_linspace(-5, 5, 128)
if jax.process_index() == 0:
  visualize_array_sharding(x)
```
<br />

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  …  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173"> G…  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39"> G…  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  0  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">  1  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  2  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  3  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  4  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  5  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  6  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  7  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">  8  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">  9  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a"> 10  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939"> 11  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173"> 12  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b"> 13  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd"> 14  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39"> 15  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">     </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173">     </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39">     </span>
</pre>

---

## Loading Data in JAX in a Multi-Host Setup {style="font-size: 22px;" auto-animate="true" visibility="uncounted"}

#### A JAX process per GPU
<br />

```{.python filename="multi-host-jax.py" code-line-numbers="|4,5|15-17"}
import jax
jax.distributed.initialize()

mesh = jax.make_mesh((16,) , ('x',))
sharding = NamedSharding(mesh , P('x'))

def distributed_linspace(start, stop, num):
    def local_linspace(indx):
        return np.linspace(start, stop, num)[indx]
    return jax.make_array_from_callback(shape=(num,), sharding=sharding,data_callback=local_linspace)

x = distributed_linspace(-5, 5, 128)
if jax.process_index() == 0:
  visualize_array_sharding(x)
mean = 0.0
variance = 1.0
result = gaussian(x, mean, variance)
if jax.process_index() == 0:
  visualize_array_sharding(result)
```
<br />

<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  …  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173"> G…  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39"> G…  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  0  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">  1  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  2  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  3  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  4  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  5  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  6  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  7  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">  8  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">  9  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a"> 10  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939"> 11  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173"> 12  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b"> 13  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd"> 14  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39"> 15  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">     </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173">     </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39">     </span>
</pre>

<br />


<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  …  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">  …  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173"> G…  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd"> G…  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39"> G…  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">  0  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">  1  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">  2  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">  3  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">  4  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">  5  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">  6  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">  7  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">  8  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">  9  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a"> 10  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939"> 11  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173"> 12  </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b"> 13  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd"> 14  </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39"> 15  </span>
<span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #843c39">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #6b6ecf">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #d6616b">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8ca252">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #a55194">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #8c6d31">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #de9ed6">     </span><span style="color: #000000; text-decoration-color: #000000; background-color: #e7ba52">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #5254a3">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ad494a">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #637939">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #7b4173">     </span><span style="color: #000000; text-decoration-color: #000000; background-color: #b5cf6b">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #ce6dbd">     </span><span style="color: #ffffff; text-decoration-color: #ffffff; background-color: #bd9e39">     </span>
</pre>

# Multi-node packages in JAX for Cosmology

---

## jaxDecomp : Components for Distributed Particle Mesh Simulations  {style="font-size: 19px;"}
[![](https://img.shields.io/badge/GitHub-jaxdecomp-blue?logo=github)](https://github.com/DifferentiableUniverseInitiative/jaxDecomp)

:::{.columns}

::: {.column width="50%"}

#### Distributed 3D FFT for Force Computation

- Essential for force calculations in large-scale simulations.

:::

::: {.column width="50%"}

#### Halo Exchange for Boundary Conditions 

- Manages boundary conditions or particles leaving the simulation domain.

:::

:::

::: {layout="[[1 , 1]]"}

![](assets/jaxDecomp/fft.svg){fig-align="center" width="90%"}

![](assets/jaxDecomp/halo-exchange.svg){fig-align="center" width="70%"}
:::

:::{.solutionbox}

:::{.solutionbox-header style="font-size: 19px;"}

Key Features of `jaxDecomp`

:::

:::{.solutionbox-body style="font-size: 17px;"}

- **Differentiable**: Fully compatible with JAX’s automatic differentiation.
- **Multi-Node Support**: Works seamlessly across multiple nodes.
- Open-source and available on PyPI.

:::

:::

---

## Performance benchmarks of `PFFT3D` {style="font-size: 20px;"}

<br />
<br />

:::{.columns}

::: {.column width="50%"}

#### Strong Scaling

![](assets/jaxDecomp/strong_scaling.png){fig-align="center" width="85%"}

:::

::: {.column width="50%"}

#### Weak scaling


![](assets/jaxDecomp/weak_scaling.png){fig-align="center" width="85%"}

:::

:::

:::{.solutionbox}

:::{.solutionbox-header style="font-size: 19px;"}

Key takeaway

:::

:::{.solutionbox-body style="font-size: 17px;"}

In strong scaling, performance decreases when moving from single-node to multi-node setups due to slower inter-node communication. This overhead is balanced by the added compute power of more nodes.

:::

:::

---

## Halo exchange in distributed simulations

::: {.fragment fragment-index=1 .fade-in-then-out}

![](assets/HPC/depict_gathered.png){.absolute top=30 right=20 width="20%"}

::: 

::: {.fragment fragment-index=2 }

![](assets/HPC/depict_split.png){.absolute top=30 right=20 width="20%"}

::: 

:::{.r-stack}

:::{.fragment fragment-index=1 .fade-in-then-out}

![Initial Field](assets/Fields/initial_conditions_1024.png){.nostretch  width="50%"}

:::

:::{.fragment fragment-index=2 .fade-in-then-out}

:::{layout="[[1] , [1] , [1] , [1]]" layout-valign="center" layout-align="center"}

![First slice](assets/Fields/initial_conditions_0_no_halo.png){.nostretch fig-align="center" width="30%"}

![Second slice](assets/Fields/initial_conditions_1_no_halo.png){.nostretch fig-align="center" width="30%"}

![Third slice](assets/Fields/initial_conditions_2_no_halo.png){.nostretch fig-align="center" width="30%"}

![Fourth slice](assets/Fields/initial_conditions_3_no_halo.png){.nostretch fig-align="center" width="30%"}

:::

:::

:::{.fragment fragment-index=3 .fade-in-then-out}

:::{layout="[[1] , [1] , [1] , [1]]" layout-valign="center" layout-align="center"}

![First slice](assets/Fields/LPT_density_field_z0_0_no_halo.png){.nostretch fig-align="center" width="30%"}

![Second slice](assets/Fields/LPT_density_field_z0_1_no_halo.png){.nostretch fig-align="center" width="30%"}

![Third slice](assets/Fields/LPT_density_field_z0_2_no_halo.png){.nostretch fig-align="center" width="30%"}

![Fourth slice](assets/Fields/LPT_density_field_z0_3_no_halo.png){.nostretch fig-align="center" width="30%"}

:::

:::

:::{.fragment fragment-index=4 .fade-in-then-out}

![LPT Field](assets/Fields/LPT_density_field_z0_1024_no_halo.png){.nostretch fig-align="center" width="50%"}

:::


:::{.fragment fragment-index=5}

![LPT Field](assets/Fields/LPT_density_field_z0_1024.png){.nostretch fig-align="center" width="50%"}

:::

:::

## <span style="color: white ;">JaxPM 2.0 : Distributed Particle Mesh Simulation</span> {background-image="assets/Fields/PM_1024.gif"}
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />


:::{.solutionbox}

:::{.solutionbox-header style="font-size: 20px;"}

Key Features of `JaxPM`

:::

:::{.solutionbox-body style="font-size: 19px;"}

- **Multi-Node Performance**: Optimized for efficient scaling across nodes.
- **High Resolution**: Capable of handling billions of particles for accurate simulations.
- **Differentiable**: Compatible with JAX’s automatic differentiation (HMC, NUTS compatible).
- **Open Source**: [![GitHub Badge](https://img.shields.io/badge/GitHub-JaxPM-blue?logo=github)](https://github.com/DifferentiableUniverseInitiative/JaxPM)

:::

:::

# Conclusion

---

## Conclusion: Enabling Scalable Cosmology with Distributed JAX {style="font-size: 22px;"}

:::{.solutionbox}

:::{.solutionbox-header style="font-size: 20px;"}

**Distributed JAX: A Game-Changer for Cosmology**  

:::

:::{.solutionbox-body style="font-size: 19px;"}

- The future is bright for JAX in cosmology!!
- JAX has transformed the landscape for scientific computing, **enabling large-scale, distributed workflows** in a Pythonic environment.
- Recent advancements (JAX 0.4.3x+) make it straightforward to scale computations across multiple GPUs and nodes.

- **Key Advantages**  
  - **Simplicity**: JAX makes it easier than ever to write high-performance code, allowing researchers to focus on science rather than infrastructure.
  - **Differentiability**: JAX allows seamless differentiation of code running across hundreds of GPUs, enabling advanced inference techniques.
  
- **The Future Ahead**  
  - **Scaling Inference Models with Distributed `jaxPM`**: By integrating the new distributed `jaxPM` into existing cosmological inference models, we can achieve unprecedented levels of detail and complexity.
  - Paving the way to fully leverage large-scale survey data for deeper insights into the universe.

:::

:::
