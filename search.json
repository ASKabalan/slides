[
  {
    "objectID": "csi2024/index.html#accelerating-bayesian-inference-in-cosmology",
    "href": "csi2024/index.html#accelerating-bayesian-inference-in-cosmology",
    "title": "CSI Presentation 2024",
    "section": "Accelerating Bayesian Inference in Cosmology",
    "text": "Accelerating Bayesian Inference in Cosmology\n \nWorking Framework\n➢   Using advanced software and tools to constrain cosmological parameters through Bayesian inference\n➢   Leveraging the Cosmic Microwave Background (CMB) as a tracer to constrain the tensor-to-scalar ratio, \\(r\\)\n➢   Utilizing weak lensing to constrain key cosmological parameters like \\(\\Omega_m\\) and \\(\\sigma_8\\)\n \n\n\nEmploying Cutting-Edge Tools\n➢   Transitioning from CPU-based NumPy to GPU-accelerated JAX for faster computation\n➢   Writing optimized CUDA code for cosmology-specific tools\n➢   Scaling cosmological simulations to run on multiple GPUs and HPC nodes for enhanced performance\n\n\nLe titre de ma thèse est « Accélérer les pipelines bayésiens pour la cosmologie ». Ce travail vise à accélérer les processus de calcul pour extraire les paramètres cosmologiques à l’aide de l’inférence bayésienne.\nL’inférence bayésienne, en utilisant le Fond diffus cosmologique (CMB) comme principal traceur, nous aide à contraindre des paramètres importants comme le rapport tenseur-surface \\(r\\), fournissant des informations sur l’inflation et les conditions de l’univers primitif.\nÀ mesure que l’univers évolue et que des structures à grande échelle se forment, nous utilisons le lentillage gravitationnel faible pour étudier ces structures et contraindre des paramètres comme \\(\\Omega_m\\) et \\(\\sigma_8\\).\nNEXT\nSur le plan technique, nous passons des calculs basés sur le CPU à des workflows accélérés par GPU en utilisant JAX, ce qui améliore considérablement la vitesse de nos simulations.\nNous optimisons également les outils cosmologiques avec CUDA et mettons à l’échelle nos simulations pour les exécuter sur plusieurs GPU et nœuds de clusters de calcul haute performance (HPC) afin de traiter efficacement de grandes quantités de données."
  },
  {
    "objectID": "csi2024/index.html#summary-of-projects",
    "href": "csi2024/index.html#summary-of-projects",
    "title": "CSI Presentation 2024",
    "section": "Summary of projects",
    "text": "Summary of projects\n\n\n\n\n\nProjects\n\n\n\n\nMon sujet de recherche est à cheval entre deux domaines : la séparation des composants du CMB dans le cadre du projet SciPol, et le lentillage gravitationnel faible avec AstroDeep. Je vais vous présenter ces deux projets plus en détail dans les prochaines diapositives."
  },
  {
    "objectID": "csi2024/index.html#cosmic-microwave-background---scipol",
    "href": "csi2024/index.html#cosmic-microwave-background---scipol",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background - Scipol",
    "text": "Cosmic Microwave Background - Scipol\n\n\n\n\n\nProjects\n\n\n\n\nOn commence par la separation de composants pour le fond diffus cosmologique"
  },
  {
    "objectID": "csi2024/index.html#cosmic-microwave-background",
    "href": "csi2024/index.html#cosmic-microwave-background",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background",
    "text": "Cosmic Microwave Background\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cosmic Microwave Background (CMB) is the afterglow of the Big Bang, providing a snapshot of the early universe.\n\n\n\nThe CMB is polarized, consisting of E and B modes.\nE modes are curl-free, generated by density fluctuations.\nB modes could be evidence of primordial gravitational waves, indicating cosmic inflation.\nThe tensor-to-scalar ratio \\(r\\), which is the ratio of the tensor power spectrum to the scalar power spectrum\n\n\n\n\n\nLe fond diffus cosmologique, ou CMB, est le rayonnement fossile du Big Bang, qui nous offre une image de l’univers primitif.\nLe CMB est polarisé et se compose de deux types de modes : les E modes et les B modes.\nLes E modes ont un champ rotationnel nul et sont générés par les fluctuations de densité dans l’univers. En revanche, les B modes, qui possèdent un champ rotationnel, pourraient indiquer la présence d’ondes gravitationnelles primordiales.\nUn paramètre clé lié à ces B modes est le rapport tensor-surface \\(r\\), qui mesure la force relative des ondes gravitationnelles par rapport aux perturbations de densité, ce qui nous aide à mieux comprendre l’inflation cosmique."
  },
  {
    "objectID": "csi2024/index.html#cosmic-microwave-background-1",
    "href": "csi2024/index.html#cosmic-microwave-background-1",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background",
    "text": "Cosmic Microwave Background\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cosmic Microwave Background (CMB) signal is obscured by various foregrounds, making it challenging to detect the true cosmological information.\n\n\n\nDust: Emission from galactic dust adds significant noise to the CMB, particularly affecting polarization measurements.\n\n\n\n\nSynchrotron Radiation: Electrons spiraling in the galaxy’s magnetic fields produce synchrotron radiation, another major contaminant.\n\n\n \n\n\nComponent seperation methods\n\nBlind Methods: Like SMICA (Spectral Matching Independent Component Analysis)\nParametric Methods: Like FGbuster (Foreground Buster)\n\n\n \n\n\n\nLe signal du fond diffus cosmologique, ou CMB, est en réalité obscurci par plusieurs avant-plans, ce qui rend difficile l’extraction des informations cosmologiques réelles.\nL’un des contaminants principaux est la poussière galactique. Cette poussière émet du rayonnement qui ajoute un bruit significatif au CMB, affectant particulièrement les mesures de polarisation.\nNEXT\nUn autre contaminant majeur est la radiation synchrotron. Elle est produite par des électrons en spirale dans les champs magnétiques de notre galaxie, ce qui vient encore plus brouiller le signal cosmologique que l’on souhaite observer.\nAFTER\nPour pouvoir extraire une valeur fiable du rapport \\(r\\), il est crucial de séparer ou de “démixer” ces composants. Le signal du CMB est mêlé à diverses émissions parasites.\nIl existe différentes méthodes pour cela, principalement des méthodes aveugles comme SMICA, qui fonctionnent sans connaissances préalables des avant-plans, et des méthodes paramétriques comme FGbuster, qui reposent sur la modélisation explicite des avant-plans.\nDans cette présentation, nous allons nous concentrer sur les méthodes paramétriques. Celles-ci nous permettent d’utiliser des modèles pour les avant-plans et d’améliorer la précision du processus de séparation."
  },
  {
    "objectID": "csi2024/index.html#cmb-component-separation",
    "href": "csi2024/index.html#cmb-component-separation",
    "title": "CSI Presentation 2024",
    "section": "CMB Component Separation",
    "text": "CMB Component Separation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModified Blackbody SED of Dust:\n\\[\n\\boxed{s_{\\mathrm{d}}(\\nu) = A_{\\mathrm{d}} \\cdot \\frac{\\nu}{\\exp\\left(\\frac{h\\nu}{k \\color{red}{T_{\\mathrm{d}}}}\\right) - 1} \\cdot \\frac{\\exp\\left(\\frac{h\\nu_{0}}{k \\color{red}{T_{\\mathrm{d}}}}\\right) - 1}{\\nu_{0}} \\cdot \\left(\\frac{\\nu}{\\nu_{0}}\\right)^{\\color{blue}{\\beta}}}\n\\]\n\n\nPower Law of Synchrotron Emission:\n\\[\n\\boxed{s_{\\text{synch}}(\\nu) = \\left(\\frac{\\nu}{\\nu_0}\\right)^{\\color{green}{\\beta_{\\text{pl}}}}}\n\\]\n\n\n\n\n\nSignal Representation\n\n\n\\[\n\\boxed{\\mathbf{d} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}}\n\\]\n\n\\[\n\\boxed{\\mathbf{d} = \\color{green}{A_{\\text{synch}}} \\cdot s_{\\text{synch}} + \\color{blue}{A_{\\mathrm{d}}} \\cdot s_{\\mathrm{d}} + A_{\\text{cmb}} \\cdot s_{\\text{cmb}} + \\mathbf{n}}\n\\]\n\n\n\n\n\nLikelihood Function\n\\[\n\\boxed{-2 \\ln \\mathcal{L}_{\\text{data}}(\\mathbf{s}, \\boldsymbol{\\beta}) = \\text{const} + \\sum_{p} \\left( \\mathbf{d}_p - \\mathbf{A}_p \\mathbf{s}_p \\right)^T \\mathbf{N}_p^{-1} \\left( \\mathbf{d}_p - \\mathbf{A}_p \\mathbf{s}_p \\right)}\n\\]\n\n\n\n\nMinimization for Component Separation\n\\[\n\\boxed{\\mathbf{s} = \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d} \\right)^T \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d}}\n\\]\n\n\n\nMinimization for Component Separation\n\\[\n\\boxed{\\mathcal{L}(\\color{blue}{\\beta_d}, \\color{red}{T_d}, \\color{green}{\\beta_{\\text{pl}}}) = \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d} \\right)^T  \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d}}\n\\]\n\n\n\nDans cette diapositive, je vais expliquer comment on modélise la séparation des composantes du CMB à partir des données observées.\nNous avons d’abord deux composantes principales qui contaminent le signal CMB pur : - La poussière galactique, modélisée par une loi de corps noir modifiée. Elle est une des sources d’émission la plus importante à haute fréquence. - Le rayonnement synchrotron, produit par les électrons en spirale dans les champs magnétiques de la galaxie, qui est une source dominante à basse fréquence.\nOn représente ensuite le signal d par une combinaison linéaire des contributions de chaque composant multipliées par leur matrice de mélange respective, plus un bruit n.\nL’objectif de la séparation des composantes est de maximiser la vraisemblance de nos données modélisées par rapport aux données observées. Cela se fait par une minimisation, représentée par l’équation en bas de la diapositive.\nLa méthode que nous utilisons ici est paramétrique, où chaque composante a un modèle physique avec des paramètres spécifiques comme \\(\\beta\\), \\(T_d\\) pour la poussière et \\(\\beta_{pl}\\) pour le synchrotron.\nÀ la fin, nous obtenons la partie CMB de la matrice de mélange, à partir de laquelle nous allons pouvoir estimer le ratio tenseur-spectral \\(r\\), un paramètre clé pour contraindre les modèles d’inflation cosmique."
  },
  {
    "objectID": "csi2024/index.html#minimization-process-in-cmb-component-separation-so-litebird",
    "href": "csi2024/index.html#minimization-process-in-cmb-component-separation-so-litebird",
    "title": "CSI Presentation 2024",
    "section": "Minimization Process in CMB Component Separation  ",
    "text": "Minimization Process in CMB Component Separation  \nUsing Scipols’s Furax Library (Chanial et al. in prep.)\n \nblocks = jnp.arange(24).reshape(3, 2, 4)\np = DenseBlockDiagonalOperator(blocks, jax.ShapeDtypeStruct((3, 4), jnp.int32), 'imn,in-&gt;im')\nop.as_matrix()\nArray([[ 0,  1,  2,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 4,  5,  6,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0,  0,  8,  9, 10, 11,  0,  0,  0,  0],\n       [ 0,  0,  0,  0, 12, 13, 14, 15,  0,  0,  0,  0],\n       [ 0,  0,  0,  0,  0,  0,  0,  0, 16, 17, 18, 19],\n       [ 0,  0,  0,  0,  0,  0,  0,  0, 20, 21, 22, 23]], dtype=int32)\n \n\n\n\nMy contributions (in the context of SO and LiteBIRD)\n\n\n\nUse JAX tools to evaluate the spectral likelihood.\nApply gradient descent methods to minimize the likelihood function.\nNext steps\n\nAdapt code to handle multi-resolution data.\nImplement support for multi-patch analysis.\nWrite a paper about GPU-accelerated component seperation\n\n\n\n\n\n\nCette matrice bloc-diagonale que nous utilisons pour la séparation des composantes du CMB a une taille de l’ordre de (fréquence, stokes, composante, npix), et peut rapidement atteindre plusieurs gigaoctets en mémoire. Avec des résolutions élevées et de multiples fréquences, la gestion efficace de cette matrice devient critique. C’est pourquoi l’utilisation d’outils comme JAX est indispensable pour optimiser les calculs, en exploitant l’accélération par GPU tout en minimisant la consommation de mémoire et les temps de calcul.\nMon travail consiste à utiliser les outils de JAX pour évaluer la fonction de vraisemblance spectrale, puis appliquer des méthodes de descente de gradient pour minimiser cette fonction et optimiser la séparation des composantes du CMB.\nÀ l’avenir, je prévois d’adapter ce code pour gérer des données multi-résolution et permettre une analyse multi-patch, afin de traiter plus efficacement des régions distinctes du ciel avec des résolutions variées."
  },
  {
    "objectID": "csi2024/index.html#summary-of-projects-1",
    "href": "csi2024/index.html#summary-of-projects-1",
    "title": "CSI Presentation 2024",
    "section": "Summary of projects",
    "text": "Summary of projects\n\n\n\n\nProjects"
  },
  {
    "objectID": "csi2024/index.html#large-scale-structure---astrodeep",
    "href": "csi2024/index.html#large-scale-structure---astrodeep",
    "title": "CSI Presentation 2024",
    "section": "Large Scale Structure - AstroDeep",
    "text": "Large Scale Structure - AstroDeep\n\n\n\n\nProjects"
  },
  {
    "objectID": "csi2024/index.html#large-scale-structure---statistical-tools",
    "href": "csi2024/index.html#large-scale-structure---statistical-tools",
    "title": "CSI Presentation 2024",
    "section": "Large Scale Structure - Statistical Tools",
    "text": "Large Scale Structure - Statistical Tools\n\n\n\n\n\n\n\nHubble eXtreme Deep Field\n\n\n\n\n\n\n\nHubble eXtreme Deep Field\n\n\n\n\n\\[\n-2 \\underbrace{\\log P(g, \\boldsymbol{\\beta} \\mid \\mathbf{d})}_{\\text{Posterior}} =\n\\sum_{\\vec{k}} \\left[\\underbrace{\\frac{\\left|\\mathbf{d} - f(g \\mid \\boldsymbol{\\beta}, z)\\right|^2}{N}}_{\\text{Likelihood}} + \\underbrace{\\frac{|g|^2}{\\mathcal{P}(\\boldsymbol{\\beta})}}_{\\text{Prior}}\\right]_{\\vec{k}}\n\\]\n\n\n\n\n\n\nPrediction\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian field\n\n\n\n\n \n\n\n\n\n\nLPT Field\n\n\n\n\n \n\n\n\n\n\nGalaxies\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDans le cadre de mon travail avec AstroDeep, je me concentre sur le lentillage gravitationnel faible.\nLe lentillage gravitationnel faible est un tracer important pour mettre une contrante sur les parameters cosmologique liée à la densité de matiere dans l’univers et la formation des sctructure à grande echelle.\ntraditionellement, on utilise une fonction de correlation à deux points pour résumer les données des champs de convergence. typiquiment on utlisé le spectre de puissance .. très concreement il s’agit de encoder les distancess entre chaque pair de galaxy (ce qui assume des données gaussienne)\nCependant, cette approche est limitée car le champs observées à basse red shift est tout sauf gaussian.\nNEXT\nUne autre famille de methodes existent, notammement l’inference basé sur les simulation ou SBI. Cette approche consiste à utiliser des simulations pour générer des données et des champs de convergence, et ça remplace l’utilisation d’une vraisemblance analytique.\nLa vraisemblance devient l’ecart entre la sortie de la simulation et les données observées, On ajoute aussi un apriori gaussian sur les parameters.\nNEXT\nUn exemple de ce genre de processus est illustré ici. On commence par générer un champ gaussien, puis on le fait évoluer en utilisant une Simulation cosmologique. On utilise d’autre methodes specifique pour introduire les galaxies dans les amas de matière noire puis on compare avec les données\nNEXT\nEn bref, on appelle le processus de générer les données à partir de simulations Forward Modeling ou la prédiction, et le processus de comparer les données observées avec les simulations Inference.\nmon travail consist à faire un ce forward model"
  },
  {
    "objectID": "csi2024/index.html#hearchical-bayesian-modeling",
    "href": "csi2024/index.html#hearchical-bayesian-modeling",
    "title": "CSI Presentation 2024",
    "section": "Hearchical Bayesian Modeling",
    "text": "Hearchical Bayesian Modeling\n\n\n\n\n\nProbabilistic Graphical Model\n\n\n\n➕  No longer need to compute the likelihood analytically  \n\n➖  We need to infer the joint posterior \\(p(\\beta, g | z)\\) before marginalization to get \\(p(\\beta | g) = \\int p(\\beta, z | g) \\, dz\\)\n\n\n\nPossible solutions\n\n\n\nHamiltonian Monte Carlo (NUTS)\nVariational Inference\nDimensionality reduction using Fisher Information Matrix\n\n\n\n\n\n\n\n\nDifferentiable forward model for differentiable sampling\n\n\n\nFull NBody simulation are too slow to be used in an iterative sampler\nDynamic grid-based simulations can be fast but hard to differentiate\nFast Particle-Mesh simulations are fast and differentiable\n\n\n\n\n\n\nDans le cadre de l’inférence bayésienne hiérarchique, on utilise un modèle graphique probabiliste pour représenter les relations entre les variables aléatoires.\ntypiquiment ici j’ai un exemple d’un model graphique probabiliste ou les cercle blanches sont les parameters echantillonable et les grise represent les parametes latents.\nL’avatage c’est que on a plus besoin de calculer la vraisemblance analytique,\nNext\nCe genre de simulateur dit explicit, ou les parameters intermediaire ou latents sont pas directement observables mais utilise, contrainerement à un simulateur implicite ou aveugle ou les parameters ne sont plus interpretable, implique une augmentation majeur de temps passé à marginaliser .\nNext\nPour résoudre ce problème, on peut utiliser des méthodes d’échantillonnage comme le NUTS ou le HMC, ce genre de methodes peuvent être significativement accélérées en utilisant un modèle forward différentiable. En ayant accès aux gradient, l’echantillonnage necessite moins de pas pour converger.\nNext\nPour ce faire, des simulation N Corps classique sont trop lentes pour être utilisées dans un échantillonneur itératif. Les simulations basées sur des grilles dynamiques peuvent être rapides mais difficiles à différencier. Les simulations de particules-réseau sont rapides et différentiables."
  },
  {
    "objectID": "csi2024/index.html#fast-particle-mesh-simulations",
    "href": "csi2024/index.html#fast-particle-mesh-simulations",
    "title": "CSI Presentation 2024",
    "section": "Fast Particle-mesh simulations",
    "text": "Fast Particle-mesh simulations\n \n\n\nNumerical scheme\n\n➢  Interpolate particles on a grid to estimate mass density\n\n\n➢  Estimate gravitational force on grid points by FFT\n\n\n➢  Interpolate forces back on particles\n\n\n➢  Update particle velocity and positions, and iterate\n\n\n\n\n\n\n\n\n\\(\\begin{array}{c}{{\\nabla^{2}\\phi=-4\\pi G\\rho}}\\\\\\\\ {{f(\\vec{k})=i\\vec{k}k^{-2}\\rho(\\vec{k})}}\\end{array}\\)\n\n\n\n\n\n\n\n\n     \n\n\n\nFast and simple, at the cost of approximating short range interactions.\nIt is essentially a series of FFTs and interpolations\nIt is differentiable and can run on GPUs\n\n\n\n\n\nLes simulations de particules-mesh sont une méthode rapide et simple pour simuler l’évolution des structures cosmologiques.\nLe schéma numérique est assez simple :\non commence par interpoler les particules sur une grille pour estimer la densité de masse,\npuis on estime la force gravitationnelle sur les points de la grille en utilisant une transformée de Fourier rapide (FFT).\nOn interpole ensuite les forces sur les particules, on met à jour les vitesses et les positions des particules, et on répète le processus.\nOn peut demarrer d’un red shift 10 par exemple et faire evoluer le systeme jusqu’à un red shift 0.\nNEXT\nCette méthode est rapide et simple, mais elle approxime les interactions à courte portée. C’est essentiellement une série de FFT et d’interpolations. Elle est différentiable et peut être exécutée sur des GPU."
  },
  {
    "objectID": "csi2024/index.html#fast-particle-mesh-scaling-lsst-desc",
    "href": "csi2024/index.html#fast-particle-mesh-scaling-lsst-desc",
    "title": "CSI Presentation 2024",
    "section": "Fast Particle-mesh scaling ",
    "text": "Fast Particle-mesh scaling \n➢  (Poqueres et al. 2021) : \\(64^3\\) mesh size, on a 1000 Mpc/h box\n➢  (Li et al. 2022) : \\(512^3\\) mesh size, using pmwd\n➢  (Lanusse et al.) : JaxPM similaire à pmwd.\n➢  FastPM : distributed but CPU-based\n\n\n\n\n\n\n\n\n\n\nInitial Conditions with a 1024 mesh\n\n\n\n\n\n\n\nInitial Conditions with a 64 mesh\n\n\n\n\n\n\n\n\nPower spectrum comparison\n\n\n\n\n\n\n\n\nMuti Node ( \\(\\infty\\) )\n\n\n\n\n\n\n\n\n\n\nFinal field with a 1024 mesh\n\n\n\n\n\n\n\nFinal field with a 64 mesh\n\n\n\n\n\n\nMy contributions (in the context of ISSC and LSST DESC)\n\n\nWe need a fast, differentiable and Scalable Particle-Mesh simulation that can run on multiple GPUs.\nMulti-GPU Particle mesh requires :\n\nDistributed FFT operations\n\nInterpolation scheme to handle boundary conditions in a distributed manner\n\n\n\n\n\n\n\n\nIl existe déja quelque implementation de simulation de particules-mesh qui sont capable de simuler des boites de 1000 Mpc/h avec une résolution de 64^3 ou 512^3.\nLes deux exemples faites dans un papier de poqueres et l’autre par le package pmwd basé sur JAX.\nLe papier de poqueres à utilisé une résolution de 64^3 pour simuler une boite de 1000 Mpc/h, et le package pmwd peut aller jusq’à une résolution de 512^3.\nJaxPM est un package similaire à pmwd, qui est capable de simuler des résolutions similaires à celles de pmwd et toujours sur un seul GPU.\nNEXT\nUn exemple de deux simulation faites sur une boite de 1 Gpc/h avec une résolution de 64^3 et 1024^3. On peut voir que la résolution plus élevée permet de capturer plus de détails dans le champ de densité.\nNEXT\nSi on visualise le spectre de puissance de ces deux champs, on peut voir que la résolution plus basse sous-estime la densité de matière et les interactions à petite échelle. En fonction du type du but cosmologique , ce qui peut dire que ce type d’inference ne sera pas capable de mettre une contrainte mielleure voir meme pire par rapport à une méthode basé sur des fonction de correlation à deux points ou spectre de puissance.\nLa taille de mémoire etant un facteur limitant, il est important de pouvoir mettre à l’échelle ces simulations sur plusieurs GPU et nœuds de calcul haute performance.\nLe passage à une simulation multi-GPU nécessite des opérations de FFT distribuées, des interpolations distribuées et des conditions aux limites.\nNEXT\nNotre but serait de pouvoir faires simulation qui peuvent être distribuées sur plusieur GPU et nœuds de calcul haute performance. Tout en restant diffirentiable et rapide (quelque secondes pour chaque simulation).\npas été fait"
  },
  {
    "objectID": "csi2024/index.html#distributed-fast-fourier-transform",
    "href": "csi2024/index.html#distributed-fast-fourier-transform",
    "title": "CSI Presentation 2024",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n➢  only operation that requires communication is the FFT\n\nJaxdecomp\n\n\nimport jax\nimport jax.numpy as jnp\n\nfield = jax.random.normal(jax.random.PRNGKey(0), (1024, 1024, 1024))\nk_field = jnp.fft.fftn(field)\n\n\n\n\n\nL’opération qui nécessite le plus de communication dans une simulation distribuée est la transformée de Fourier rapide (FFT).\nL’utilisation sur un seul GPU est triviale"
  },
  {
    "objectID": "csi2024/index.html#distributed-fast-fourier-transform-1",
    "href": "csi2024/index.html#distributed-fast-fourier-transform-1",
    "title": "CSI Presentation 2024",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n➢  only operation that requires communication is the FFT\n\nJaxdecomp\n\n DifferentiableUniverseInitiative/jaxDecomp\n\n\n\n\nimport jax\nimport jax.numpy as jnp\nimport jaxdecomp\n\ndevices = mesh_utils.create_device_mesh((2, 2))\nmesh = jax.sharding.Mesh(devices, axis_names=('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n\n# Create gaussian field distributed across the mesh\nfield = jax.make_array_from_single_device_arrays(\n        shape=mesh_shape,\n        sharding=sharding,\n        arrays=[jax.random.normal(jax.random.PRNGKey(rank), (512, 512, 1024))])\n\nk_field = jaxdecomp.fft.pfft3d(field)\n\n\n\n\n\n\n\n\n\n\nJaxDecomp features\n\n\n➢  jaxDecomp supports 2D and 1D decompositions\n➢  Works for multi-node FFTs\n➢  is differentiable\n➢  The package is also provided as a standalone library\n\n\n\n\n\n\n\n\n\n\n\n\n\n\navec JaxDecomp c’est aussi trivial faire des FFT distribuées sur plusieurs GPU et nœuds de calcul.\nL’utilisation est aussi trivial que faire une FFT sur un seul GPU, il suffit de décrire la grille de distribution et la bibliothèque s’occupe du reste.\nNEXT\nça supporte les décompositions 2D et 1D, fonctionne pour les FFT multi-nœuds, tout en restant différentiable.\nNEXT\nCôté implémentation, j’effectue à une série de FFT locales sur chaque GPU, en travaillant à chaque fois sur un axe non distribué. Ensuite, j’effectue une transposition de la grille de distribution pour redistribuer les données et traiter l’axe suivant. Cela permet de répartir la charge de calcul efficacement entre plusieurs GPU tout en s’assurant que chaque transformation de Fourier est correctement alignée avec l’axe de calcul."
  },
  {
    "objectID": "csi2024/index.html#scaling-of-distributed-fft-operations",
    "href": "csi2024/index.html#scaling-of-distributed-fft-operations",
    "title": "CSI Presentation 2024",
    "section": "Scaling of Distributed FFT operations",
    "text": "Scaling of Distributed FFT operations\n\n\nDans cette figure, on peut voir les performances de la bibliothèque JaxDecomp pour les FFT distribuées sur plusieurs GPU.\nOn est capable des faires des FFTs sur des grille de 4096^3 en quelque centaine de millisecondes (ce qui est très rapide).\nEt on voit deja qu’on peut aller jusqu’à une grille de 4096^3 en utilisant 64 GPU. En comparaison, Les simulation actuelles avec un seul GPU ne dépassent pas 1024^3."
  },
  {
    "objectID": "csi2024/index.html#interpolation-function-for-particle-mesh-simulations-cloud-in-cell",
    "href": "csi2024/index.html#interpolation-function-for-particle-mesh-simulations-cloud-in-cell",
    "title": "CSI Presentation 2024",
    "section": "Interpolation function for Particle-Mesh simulations (Cloud-in-Cell)",
    "text": "Interpolation function for Particle-Mesh simulations (Cloud-in-Cell)\n\n\n\nCIC Paint Function:\n\\[\n\\begin{array}{l c r}\ng({\\bf j})=\\sum_{i=1}^{N}m_{i}\\times\\prod_{d=1}^{D}\\left(1-\\left|p_{i}^{d}-j_{d}\\right|\\right)\n\\end{array}\n\\]\n\n\nForces Functions:\n\\[\n\\nabla^{2}\\phi = -4\\pi G\\kappa\\rho\n\\]\n\\[\nf(\\vec{k}) = \\dot{\\vec{k}}k^{-2}\\rho(\\vec{k})\n\\]\n\n\nCIC Read Function:\n\\[\nv_{i} = \\sum_{{\\bf j}}g({\\bf j})\\times\\prod_{d=1}^{D}\\left(1-|p_{i}^{d}-j_{d}|\\right)\n\\]\n\n\n\n\n➢  Periodic boundary conditions are applied to each slice of the simulation\n➢  Particles cannot escape the simulation domain\n\n\n\n\n\n\n\n\nParticles\n\n\n\n\n\n\n\nCIC Kernel\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\n\n\n\n\n\n\nGradients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplacement\n\n\n\n\n\n\n\n\n\nPour les simulations de particules-mesh, on utilise une fonction d’interpolation pour peindre les particules sur la grille.\nLa methode s’appelle Cloud-in-Cell (CIC), qui est une méthode simple et rapide pour interpoler les particules sur une grille.\nOn part d’une liste de particules représentées par leurs coordonnées et leur masse.\nNEXT\nEnsuite on ‘paint’ chaque particules sur les cellules adjacentes.\nChaque particule contribue à la densité de masse de la cellule en fonction de sa distance à la cellule.\nPar exemple dans cette exemple, la plus part des particules sont proche de la cellule (1 , 0) donc elle aura le plus de densité.\nNEXT\nOn calcule les forces en utilisant la densité de masse interpolée sur la grille. On obtient donc des gradients.\nNEXT\nEnsuite on interpole les forces sur les particules pour les mettre à jour.\nChaque gradient contribue au deplacement d’une particule en fonction de sa distance à la cellule.\nNEXT\nCependant, cette méthode n’est pas compatible avec les simulations distribuées, car elle nécessite de lire les valeurs des cellules voisines, ce qui n’est pas possible si les cellules sont sur un autre GPU. Ce qui se passe sur un seul GPU c’est l’application de la condition periodique. Ce qui veut dire que les particules qui sort d’un sous domain reviennent de l’autre coté du meme domaine.\nCela le reflète pas la réalité physique, et peut introduire des erreurs dans les simulations.\nNEXT\nJe montre dans le slide suivant quel impact cela peut avoir sur les simulations."
  },
  {
    "objectID": "csi2024/index.html#halo-exchange-in-distributed-simulations",
    "href": "csi2024/index.html#halo-exchange-in-distributed-simulations",
    "title": "CSI Presentation 2024",
    "section": "Halo exchange in distributed simulations",
    "text": "Halo exchange in distributed simulations\n\n\n\n\n\n\n\n\n\n\n\nInitial Field\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nCIC Kernel\n\n\n\n\n\n\n\nHalo Exchangel\n\n\n\n\n\n\nfrom jaxdecomp import halo_exchange\n\nhalo_size = 128\nfield = halo_exchange(field, halo_extent=halo_size)\n\n\n\nOn commence par voir un champs de densité gaussien de , qui est distribué sur 4 GPU.\nChaque GPU a une partie du champ de densité, et peut evoluer de manière indépendante.\nAprès avoir calculer les forces avec la FFT distribuée, on obtient un champ de densité qui est distribué sur les 4 GPU.\nOn fait evoluer les particules indépendamment, mais on voit que les particules qui sont proche des bords du domaine ne sont pas correctement traitées.\nNEXT\nOn a egalement une solution pour ce problème, qui est l’échange de halo. Ou un utilisateur peut definir une taille de halo, qui est la taille des cellules qui sont échangées entre les GPU.\nNEXT\nConcretement, pendant la phase de l’interpolaction CIC, on permet les particules d’être peintes sur des cellule plus grande que les cellules locales.\nPuis on echange la partie des cellules qui sont dans le halo avec les autres GPU.\nL’échange applique les conditions periodiques sur les bords du domaine, et permet de traiter les particules qui sont proche des bords du domaine. Cela nous donne un resultat très similaire à une simulation sur un seul GPU."
  },
  {
    "objectID": "csi2024/index.html#conclusion",
    "href": "csi2024/index.html#conclusion",
    "title": "CSI Presentation 2024",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nWork already done\n\n\n➢  jaxDecomp : Distributed FFT operations and halo exchange (Released)\n➢  jaxPM : Distributed Particle-Mesh simulations (Code)\n➢  Furax : Minimization of the spectral likelihood for CMB component separation (Code)\n➢  S2FFT : Accelerated spherical harmonics transforms (Pull request)\n\n\n\n\n\nWork in progress\n\n\n➢  Furax (Continued) : Creation of a special optimiser that can handle multi-resolution and multi-patch data\n➢  jaxPM : Benchmarking and testing the scaling of the simulations\n\n\n\n\n\nFuture work\n\n\n➢   Optimized and distributed spherical harmonics transforms for CMB lensing\n➢   Distributed probabilistic programming for hierarchical Bayesian Modeling"
  },
  {
    "objectID": "csi2024/index.html#attended-conferences",
    "href": "csi2024/index.html#attended-conferences",
    "title": "CSI Presentation 2024",
    "section": "Attended Conferences",
    "text": "Attended Conferences\n\n\nFrench Conferences\n\n\n\nIAP 2023 Machine Learning-\nLSST France 2023 Lyon, France\nLSST France 2024 Marseille, France (Talk)\n[Upcomming Conference] IAP GDR CoPhy Tools 2024 Paris, France (Talk)\n\n\n\n\n\nInternational Conferences\n\n\n\nMoriond Cosmology 2023 La Thuile, Italy (Poster)\nLSST DESC 2023 Zurich, Switzerland"
  },
  {
    "objectID": "csi2024/index.html#papers",
    "href": "csi2024/index.html#papers",
    "title": "CSI Presentation 2024",
    "section": "Papers",
    "text": "Papers\n \n\n\nCurrent/Submitted papers\n\n\n\nInfrared Radiometric Image Classification and Segmentation of Cloud Structure Using ML (Sommer et al. 2024, Published)\nJaxDecomp: A Distributed Fast Fourier Transform Library (Kabalan et al. to be submitted soon)\nFurax: Optimization of the Large Scale Multiresolution Parametric Component Separation (Kabalan et al., for end of 2024)\nJaxPM: A Differentiable Particle-Mesh Simulation (Kabalan et al. in prep.)\n\n\n\n\n\n\nFuture papers\n\n\n\nSpherical Harmonics for CMB component separation (Lead author)\nDistributed Probabilistic Programming for Hierarchical Bayesian Modeling"
  },
  {
    "objectID": "csi2024/index.html#formations",
    "href": "csi2024/index.html#formations",
    "title": "CSI Presentation 2024",
    "section": "Formations",
    "text": "Formations\n\n\n\ndivers\n\n\n\nEuclid summer school (2023)\nAISSAI AstroInfo Hackathon 2023, Frejus, France\nPhysics informed neural networks with the IDRIS team (Jean-zay super computer) (2024)\n\n\n\n\n\nCours Ecole Doctorale\n\n\n\nQCD with Matteo Cacciari (2023)\n\n\n\n\n\n\n\nCSI Presentation 2024"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "",
    "section": "",
    "text": "d = As + n\nL = (A N d)(A N A)^-1\nd[frequence,pixel] N[frequence,pixel] A[frequence,Component,pixel]\nSED_0[0,1] = Beta * d[0,1]\nA[0,0,1]\nSED_1[1,2] = exp(Beta * d[1,2]) A[1,1,2]\nd = StokesPytree(IQU , freq , pixel) s = StokesPytree(IQU , Component , pixel)?\nd = A s A(Nu , Beta)\nnu0 P0(Beta0) P1(Beta) -&gt; Beta SED(nu0 , Beta) nu1 P0 P1 -&gt; Beta SED(nu1 , Beta)\nSED = nu0 * Beta\nBasic A[0,0,P] Multipatch A[0,0,3]\nSED (nu , Beta)\nBasic d[freq,pixel] = A(frequence,Component,pixel) s[Component,pixel] d[freq , pixel] = freq * Beta[Component] * s[Component,pixel] Multipatch d[freq , pixel] = freq * Beta[Component,pixel] * s[Component,pixel]\nd = IQU freq pixel\nsky_composant = StokesIQUPyTree.ones(shape=(2 , 48))\nprint(sky_composant) print(sky_composant.structure)\nblocks = jnp.ones(21048 , jnp.int32).reshape(10 , 2 , 48)\nop = DenseBlockDiagonalOperator(blocks, jax.ShapeDtypeStruct((2, 48), jnp.int32) , subscripts = ‘fc…,c…-&gt;f…’) sky_freq = op(sky_composant) # A @ s sky_composant = op.transpose()(sky_freq) # A.T @ A @ s"
  },
  {
    "objectID": "marseille2024/index.html#traditional-cosmological-inference",
    "href": "marseille2024/index.html#traditional-cosmological-inference",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Traditional cosmological inference",
    "text": "Traditional cosmological inference\n\nBayesian inference in cosmology\n\nWe need to infer the cosmological parameters \\(\\theta\\) that generated an observartion \\(x\\)\n\n\\[p(\\theta | x ) \\propto \\underbrace{p(x | \\theta)}_{\\mathrm{likelihood}} \\ \\underbrace{p(\\theta)}_{\\mathrm{prior}}\\]\n\n\n\n\n\n\n\n\n\n\n ➢  Compute summary statistics based on the 2-point correlation function of the shear field\n\n\n ➢  Run an MCMC chain to recover the posterior distribution of the cosmological parameters, using an analytical likelihood\n\n\n\n\n\n\n\nLimitations\n\n\n\nSimple summary statistics assume Gaussianity\nThe need to compute an analytical likelihood"
  },
  {
    "objectID": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration",
    "href": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport numpy as np\n\n\ndef multiply_and_add(a, b, c):\n    return np.dot(a, b) + c\n\n\na, b, c = np.random.normal(size=(3, 32, 32))\nresult = multiply_and_add(a, b, c)\n\n\nimport jax\nimport jax.numpy as jnp\n\n\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c)"
  },
  {
    "objectID": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration-1",
    "href": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration-1",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport jax\nimport jax.numpy as jnp\n\n\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c)"
  },
  {
    "objectID": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration-2",
    "href": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration-2",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c) \ngradient = jax.grad(multiply_and_add)(a, b, c)\n \n\n\nJAX : Numpy + Autograd + GPU\n\n\n\njax.grad uses automatic differentiation to compute the gradient of the function\njax.jit compiles the function to run on GPUs"
  },
  {
    "objectID": "marseille2024/index.html#distributed-fast-fourier-transform",
    "href": "marseille2024/index.html#distributed-fast-fourier-transform",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n➢  only operation that requires communication is the FFT\n\nJaxdecomp\n\n\nimport jax\nimport jax.numpy as jnp\n\nfield = jax.random.normal(jax.random.PRNGKey(0), (1024, 1024, 1024))\nk_field = jnp.fft.fftn(field)"
  },
  {
    "objectID": "marseille2024/index.html#distributed-fast-fourier-transform-1",
    "href": "marseille2024/index.html#distributed-fast-fourier-transform-1",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n➢  only operation that requires communication is the FFT\n\nJaxdecomp\n\n DifferentiableUniverseInitiative/jaxDecomp\n\n\n\nimport jax\nimport jax.numpy as jnp\nimport jaxdecomp\n\ndevices = mesh_utils.create_device_mesh((2, 2))\nmesh = jax.sharding.Mesh(devices, axis_names=('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n\n# Create gaussian field distributed across the mesh\nfield = jax.make_array_from_single_device_arrays(\n    shape=mesh_shape,\n    sharding=sharding,\n    arrays=[jax.random.normal(jax.random.PRNGKey(rank), (512, 512, 1024))])\n\nk_field = jaxdecomp.fft.pfft3d(field)\n\n\n\n\nJaxDecomp features\n\n\n➢  jaxDecomp supports 2D and 1D decompositions\n➢  Works for multi-node FFTs\n➢  is differentiable\n➢  The package is also provided as a standalone library"
  },
  {
    "objectID": "marseille2024/index.html#halo-exchange-in-distributed-simulations",
    "href": "marseille2024/index.html#halo-exchange-in-distributed-simulations",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Halo exchange in distributed simulations",
    "text": "Halo exchange in distributed simulations\n\n\n\n\n\n\n\n\n\n\n\nInitial Field\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\nfrom jaxdecomp import halo_exchange\n\nhalo_size = 128\nfield = halo_exchange(field, halo_extent=halo_size)"
  },
  {
    "objectID": "marseille2024/index.html#conclusion",
    "href": "marseille2024/index.html#conclusion",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Conclusion ",
    "text": "Conclusion \n   \n\n\nDistruibuted Particle-Mesh simulations for cosmological inference\n\n\n\nA shift from analytical likelihoods to full field inference\n\nThe need for fast differentiable simulators\nParticle-Mesh as simulators for full field inference\nDistributed fourrier transforms that work on multi-node HPC using jaxDecomp\nHighly scalable LPT simulations using JaxPM\n\nStill subject to some challenges\n\nSome issues with the ODE solving step\nOnly Euler gives decent results.\n\n\n\n\n\n\n\nLSST France, 2024"
  },
  {
    "objectID": "paris2024/index.html#motivation-parallel-computing-in-cosmology",
    "href": "paris2024/index.html#motivation-parallel-computing-in-cosmology",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Motivation: Parallel Computing in Cosmology",
    "text": "Motivation: Parallel Computing in Cosmology\n  \n\n\nUpcoming Surveys and Massive Data in Cosmology\n\n\n\nMassive Data Volume: LSST will generate 20 TB of raw data per night over 10 years, totaling 60 PB.\nCatalog Size: The processed LSST catalog database will reach 15 PB.\n\n\n\n\n\nCosmological Models and Pipelines\n\n\n\nCosmological simulations and forward modeling can easily reach multiple terabytes in size.\nWe need to scale up cosmological pipelines to handle these data volumes effectively."
  },
  {
    "objectID": "paris2024/index.html#background-how-gpus-work",
    "href": "paris2024/index.html#background-how-gpus-work",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Background: How GPUs Work",
    "text": "Background: How GPUs Work\n\n\n\nMassive Thread Count\n\nGPUs are designed with thousands of threads.\nEach core can handle many data elements simultaneously.\nThe main bottleneck is memory throughput.\n\n\n\nOptimizing Throughput with Multiple GPUs:\n\nComputation is often only a fraction of total processing time.\n\n\n\n\nUsing multiple GPUs increases overall data throughput, enhancing performance and reducing idle time.\n\n\n\n\n\n\nGPU threads\n\n\n\n\n\n\n\nSingle GPU throughput\n\n\n\n\n\n\n\nSaturated GPU\n\n\n\n\n\n\n\nMultiple GPUs throughput\n\n\n\n\n\n\n\n\nMassive Thread Count: GPUs have thousands of threads, allowing them to process large datasets in parallel. Each core can handle multiple data elements, making them ideal for parallel computing tasks.\nMemory Throughput Bottleneck: The true bottleneck in GPU performance is often memory throughput. Even with many threads, GPU efficiency can drop if data isn’t supplied quickly enough.\nOptimizing Throughput with Multiple GPUs: In many cases, computation is only a small portion of the total processing time. By adding more GPUs, we increase possible data throughput, reduce idle time, and ultimately improve performance for large workloads."
  },
  {
    "objectID": "paris2024/index.html#background-types-of-data-parallelism",
    "href": "paris2024/index.html#background-types-of-data-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Background: Types of Data parallelism",
    "text": "Background: Types of Data parallelism\n\n\nData Parallelism\n\nSimple Parallelism: Each device processes a different subset of data independently.\n\n\n\nData Parallelism with Collective Communication:\n\nDevices process data in parallel but periodically share results (e.g., for gradient averaging in training).\n\n\n\n\n\nTask Parallelism\n\nEach device handles a different part of the computation.\nThe computation itself is divided between devices.\nIs generally more complex than data parallelism.\n\n\n\n\n\n\n\n\nSimple Data Parallelism\n\n\n\n\n\n\n\nData Parallelism with Communication\n\n\n\n\n\n\n\n\nTask Parallelism\n\n\n\n\n\n\n\nData Parallelism: In simple data parallelism, each device processes its own data slice independently, ideal for tasks without inter-device dependencies.\nData Parallelism with Collective Communication: Some applications require devices to periodically exchange data, such as in gradient averaging for distributed training. This helps maintain model consistency across devices.\nTask Parallelism: Each device handles a unique slice of the overall computation. This method is used in workflows where the computation on each device contributes to different parts of the task (e.g., in pipeline parallelism for model inference)."
  },
  {
    "objectID": "paris2024/index.html#why-should-you-use-parallelism",
    "href": "paris2024/index.html#why-should-you-use-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Why Should You Use Parallelism?",
    "text": "Why Should You Use Parallelism?\n\nSimple cases\n\nData Parallelism (Simple) ✅\n\nIf your pipeline resembles simple data parallelism, then parallelism is a good idea.\n\nData Parallelism with Simple Collectives ✅\n\nSimple collectives (e.g., gradient averaging) can be easily expressed in JAX, allowing devices to share intermediate results.\n\n\n\n\nComplex cases\n\nNon-splittable Input (e.g., N-body Simulation Fields) ⚠️\n\nWhen the input is not easily batchable, like a field in an N-body simulation.\n\n\n\n\n\nTask Parallelism ⚠️\n\nUseful for long sequential cosmological pipelines where each device handles a unique task in the sequence.\nMore common in training complex models (e.g., LLMs like Gemini or ChatGPT).\n\n\n\n\n\nData Parallelism: Ideal if your pipeline can be split into independent data chunks. JAX makes it easy to implement with pmap for distributing computations.\nCollectives in Data Parallelism: Some tasks, like gradient averaging, require simple collective communication between devices. JAX provides tools to express these collectives efficiently.\nNon-Batchable Input: In cases where data cannot be split into batches (e.g., field-based data in simulations), you may need to implement more complex collectives to coordinate data across devices.\nTask Parallelism: Used in sequential pipelines where each device performs a unique task. This approach is more complex and requires substantial restructuring, making it common in large language model training.\n\n\n❌"
  }
]