[
  {
    "objectID": "paris2024/index.html#goals-for-this-presentation",
    "href": "paris2024/index.html#goals-for-this-presentation",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Goals for This Presentation",
    "text": "Goals for This Presentation\n\n\n\nMotivation: Understand the value of full-field inference in cosmology.  \nBackground on Parallelism: Learn how GPUs work and key scaling concepts.  \nWhen to Use (and Avoid) Parallelism: Discover the benefits and limitations.  \nScaling with JAX: Explore techniques for scaling computations in JAX.  \nDistributed Tools for Cosmology: Get an overview of multi-node packages like jaxDecomp and jaxPM.  \nHands-On Practice: Apply these concepts in interactive notebooks."
  },
  {
    "objectID": "paris2024/index.html#motivation-cosmology-in-the-exascale-era",
    "href": "paris2024/index.html#motivation-cosmology-in-the-exascale-era",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Motivation: Cosmology in the Exascale Era ",
    "text": "Motivation: Cosmology in the Exascale Era \n\n\n\n\n\nUpcoming Surveys and Massive Data in Cosmology\n\n\n\nMassive Data Volume: LSST will generate 20 TB of raw data per night over 10 years, totaling 60 PB.\nCatalog Size: The processed LSST catalog database will reach 15 PB.\n\n\n\n\n\nCosmological Models and Pipelines\n\n\n\nCosmological simulations and forward modeling can easily reach multiple terabytes in size.\nWe need to scale up cosmological pipelines to handle these data volumes effectively.\n\n\n\n\n\nwill explain scaling in here"
  },
  {
    "objectID": "paris2024/index.html#forward-modeling-in-cosmology",
    "href": "paris2024/index.html#forward-modeling-in-cosmology",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Forward Modeling in Cosmology",
    "text": "Forward Modeling in Cosmology\n\n\nWeak Lensing Model\n\nPrediction:\n\nA simulator generates observations from initial conditions and cosmological parameters.\n\nInference:\n\nThe simulated results are compared with actual observations.\nOptimal initial conditions and parameters are inferred to closely match the observed data.\n\n\n\n\nScaling Challenges\n\n\n\nResolution Today: Simulations currently use around 250,000 to 130 million particles.\nIdeal Resolution: Billion-particle simulations are necessary for high accuracy.\nSoftware: Tools like JaxPM or PMWD support up to ~130 million particles on a single GPU.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulations in Cosmology: These simulations model the universe‚Äôs evolution to reproduce observed structures, helping infer parameters like dark matter density, dark energy, and other cosmological constants.\nResolution Requirement: Simulations with more particles provide finer details, making convergence maps closer to observed data. Current particle counts (130 million) are still limited compared to the billion-particle simulations required for accurate cosmological inference."
  },
  {
    "objectID": "paris2024/index.html#how-gpus-work",
    "href": "paris2024/index.html#how-gpus-work",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "How GPUs Work",
    "text": "How GPUs Work\n\n\n\nMassive Thread Count\n\nGPUs are designed with thousands of threads.\nEach core can handle many data elements simultaneously.\nThe main bottleneck is memory throughput.\n\n\n\nOptimizing Throughput with Multiple GPUs:\n\nComputation is often only a fraction of total processing time.\n\n\n\n\nUsing multiple GPUs increases overall data throughput, enhancing performance and reducing idle time.\n\n\n\n\n\n\nGPU threads\n\n\n\n\n\n\n\nSingle GPU throughput\n\n\n\n\n\n\n\nSaturated GPU\n\n\n\n\n\n\n\nMultiple GPUs throughput\n\n\n\n\n\n\n\nMassive Thread Count: GPUs have thousands of threads, allowing them to process large datasets in parallel. Each core can handle multiple data elements, making them ideal for parallel computing tasks.\nMemory Throughput Bottleneck: The true bottleneck in GPU performance is often memory throughput. Even with many threads, GPU efficiency can drop if data isn‚Äôt supplied quickly enough.\nOptimizing Throughput with Multiple GPUs: In many cases, computation is only a small portion of the total processing time. By adding more GPUs, we increase possible data throughput, reduce idle time, and ultimately improve performance for large workloads."
  },
  {
    "objectID": "paris2024/index.html#types-of-data-parallelism",
    "href": "paris2024/index.html#types-of-data-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Types of Data parallelism",
    "text": "Types of Data parallelism\n\n\nData Parallelism\n\nSimple Parallelism: Each device processes a different subset of data independently.\n\n\n\nData Parallelism with Collective Communication:\n\nDevices process data in parallel but periodically share results (e.g., for gradient averaging in training).\n\n\n\n\n\nTask Parallelism\n\nEach device handles a different part of the computation.\nThe computation itself is divided between devices.\nIs generally more complex than data parallelism.\n\n\n\n\n\n\n\n\nSimple Data Parallelism\n\n\n\n\n\n\n\nData Parallelism with Communication\n\n\n\n\n\n\n\n\nTask Parallelism\n\n\n\n\n\n\nData Parallelism (Simple Parallelism):\n\nIn simple data parallelism, each device processes a distinct subset of the data independently, without needing to interact with other devices during computation. This approach works best when:\n\nThe dataset has to be evenly split across devices.\nEach subset of data can be processed in isolation, which reduces the complexity and overhead of communication.\nExample in Cosmology: Simple data parallelism might be used in parameter estimation tasks where each GPU computes likelihoods for different portions of the dataset independently.\n\n\nData Parallelism with Collective Communication:\n\nIn many applications, devices working in parallel need to share intermediate results to ensure consistency and accuracy. For example, in distributed neural network training, devices need to periodically share gradient information to keep model weights in sync.\n\nThis form of parallelism is more complex than simple data parallelism, as it involves synchronizing data across devices at intervals, typically through collective communication operations.\nChallenges:\n\nCollective operations like gradient averaging can become bottlenecks as they require frequent communication between devices, which may impact scaling efficiency.\nIn cosmology, gradient sharing can be particularly useful in distributed optimization routines where multiple GPUs are used to train machine learning models on large cosmological datasets.\n\n\n\nTask Parallelism:\n\nIn task parallelism, each device handles a unique segment of the overall computation pipeline. This approach is ideal when the computation itself is modular and can be split into discrete tasks that contribute independently to the final result.\n\nUnlike data parallelism, task parallelism is often sequential in nature, where each device performs a specific part of a larger computational workflow.\nExample in Cosmology: Task parallelism can be applied in cosmological simulations where certain calculations are interdependent. For example:\n\nOne device might handle the gravitational calculations, while another processes hydrodynamic interactions, with each task feeding results to the next stage.\n\nComplexity:\n\nTask parallelism generally requires more significant changes to the code structure and may involve custom communication protocols between tasks.\nThis approach is commonly used in model pipelines that require heavy sequential processing, such as large language model (LLM) training (e.g., Gemini or ChatGPT) or in cosmology, where different physical components need distinct treatments in simulations.\n\n\n\nSummary:\n\nData parallelism is typically easier to implement but may be limited by the need for collective communication.\nTask parallelism is more flexible in terms of workload distribution but often requires substantial restructuring of code and specific coordination between tasks, especially in long, sequential workflows."
  },
  {
    "objectID": "paris2024/index.html#why-should-you-use-parallelism",
    "href": "paris2024/index.html#why-should-you-use-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Why Should You Use Parallelism?",
    "text": "Why Should You Use Parallelism?\n\nSimple cases\n\nData Parallelism (Simple) ‚úÖ\n\nIf your pipeline resembles simple data parallelism, then parallelism is a good idea.\n\nData Parallelism with Simple Collectives ‚úÖ\n\nSimple collectives (e.g., gradient averaging) can be easily expressed in JAX, allowing devices to share intermediate results.\n\n\n\n\nComplex cases\n\nNon-splittable Input (e.g., N-body Simulation Fields) ‚ö†Ô∏è\n\nWhen the input is not easily batchable, like a field in an N-body simulation.\n\n\n\n\n\nTask Parallelism ‚ö†Ô∏è\n\nUseful for long sequential cosmological pipelines where each device handles a unique task in the sequence.\nMore common in training complex models (e.g., LLMs like Gemini or ChatGPT).\n\n\n\n\n\nData Parallelism: Ideal if your pipeline can be split into independent data chunks. JAX makes it easy to implement with pmap for distributing computations.\nCollectives in Data Parallelism: Some tasks, like gradient averaging, require simple collective communication between devices. JAX provides tools to express these collectives efficiently.\nNon-Batchable Input: In cases where data cannot be split into batches (e.g., field-based data in simulations), you may need to implement more complex collectives to coordinate data across devices.\nTask Parallelism: Used in sequential pipelines where each device performs a unique task. This approach is more complex and requires substantial restructuring, making it common in large language model training."
  },
  {
    "objectID": "paris2024/index.html#when-not-to-use-parallelism",
    "href": "paris2024/index.html#when-not-to-use-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "When NOT to Use Parallelism",
    "text": "When NOT to Use Parallelism\n\n\nTo Keep in Mind\n\nData Fits on a Single GPU\n\n\n\nNeed for Complex Collectives\n\nAdditional GPUs can add complexity and may not yield enough performance improvement.\n\nTask Parallel Model\n\nChanging the pipeline or adapting to new devices often requires significant rewrites.\n\n\n\n\n\n\n\nNot fully used GPU\n\n\n\n\n\n\nConsider Scaling to multiple GPUs if:\n\n\n\nYou have a single-GPU prototype that‚Äôs working but needs significant runtime reduction.\nHas a significant impact on your results.\n\nUsing multiple GPUs can significantly decrease execution time.\nOR You have non-splittable input (e.g., fields in a cosmological simulation) that is crucial for your results.\n\n\n\n\n\n\n\nSingle-GPU Sufficiency: If all data fits on one GPU, adding more GPUs may introduce unnecessary overhead, reducing overall performance.\nComplex Collectives: Complex collective communication (like frequent data sharing) can slow down computations. In cases where collectives are challenging, stick to a single GPU if possible.\nTask Parallelism Complexity: Task parallel models are generally harder to adjust. Each device performs a unique task, making it more challenging to scale or modify the pipeline."
  },
  {
    "objectID": "paris2024/index.html#how-to-measure-scaling-for-parallel-codes",
    "href": "paris2024/index.html#how-to-measure-scaling-for-parallel-codes",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "How to Measure Scaling for Parallel Codes",
    "text": "How to Measure Scaling for Parallel Codes\n\n\n\n\n\nStrong Scaling\n\nIncreasing the number of GPUs to reduce runtime for a fixed data size.\n\n\n\nAssesses performance as more GPUs are added to a fixed dataset. Danger Zone‚ö†Ô∏è: Indicates the distributed code is not scaling efficiently.\n\n\n\n\n\nWeak Scaling\n\nIncreasing data size with a fixed number of GPUs.\n\n\n\nTests how the code handles increasing data sizes with a fixed number of GPUs. Danger Zone‚ö†Ô∏è: Suggests underlying scaling issues with the code itself.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrong Scaling: Assesses performance as more GPUs are added to a fixed dataset. Danger Zone: Indicates the distributed code is not scaling efficiently.\nWeak Scaling: Tests how the code handles increasing data sizes with a fixed number of GPUs. Danger Zone: Suggests underlying scaling issues with the code itself."
  },
  {
    "objectID": "paris2024/index.html#environmental-impact-of-high-performance-computing",
    "href": "paris2024/index.html#environmental-impact-of-high-performance-computing",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Environmental Impact of High-Performance Computing",
    "text": "Environmental Impact of High-Performance Computing\n\n\n\n\n\nPerlmutter Supercomputer (NERSC)\n\nLocation: NERSC, Berkeley Lab, California, USA\nCompute Power: ~170 PFlops\nGPUs: 7,208 NVIDIA A100 GPUs\nPower Draw: ~ 3-4 MW\n\n\nJean Zay Supercomputer (IDRIS)\n\nLocation: IDRIS, France\nCompute Power: ~126 PFlops (FP64), 2.88 EFlops (BF/FP16)\nGPUs: 3,704 GPUs, including V100, A100, and H100\nPower Draw: ~1.4 MW on average (as of September, without full H100 usage), leveraging France‚Äôs renewable energy grid.\n\n\n\n\n\nPerlmutter Supercomputer\n\n\n\n\n\nJean Zay Supercomputer\n\n\n\n\n\n\n\n\n\nEnvironmental Benefits of Efficient Parallel Computing\n\n\n\n\nHigher throughput moves computations closer to peak FLOPS. \nOperating near peak FLOPS ensures more effective use of computational resources. \nMore computations are achieved per unit of energy, improving energy efficiency.\n\n\n\n\n\n\n\n\n\n\nPerlmutter: A hybrid CPU-GPU setup with over 6,000 A100 GPUs, designed for both machine learning and scientific tasks. It operates with a high power draw of ~3.2 MW/hr, but the energy-per-FLOP efficiency is optimized through extensive GPU utilization.\nJean Zay‚Äôs Environmental Efficiency: With its renewable energy source and GPU partitions, Jean Zay achieves significant computational power (~126 PFlops) while keeping average power consumption at ~1.4 MW/hr. The partitioned GPU setup (V100, A100, and soon H100) allows Jean Zay to scale efficiently for a variety of workloads without excessive energy use.\n\n\n\nCredit: Laurent Leger from IDRIS"
  },
  {
    "objectID": "paris2024/index.html#why-jax-for-distributed-computing",
    "href": "paris2024/index.html#why-jax-for-distributed-computing",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Why JAX for Distributed Computing?",
    "text": "Why JAX for Distributed Computing?\n\nDistributed Computing Isn‚Äôt New:\n\nTools like MPI and OpenMP are used extensively.\nML frameworks like TensorFlow and PyTorch offer distributed training.\nDiffEqFlux.jl Horovod and Ray\n\n\n\n\nFamiliar and Accessible API:\n\nJAX offers a NumPy-like API that is both accessible and intuitive.\nPython users can leverage parallelism without needing in-depth knowledge of low-level parallel frameworks like MPI.\n\n\n\n\n\n\nKey Points\n\n\n\nPythonic Scalability: JAX allows you to write scalable, pythonic code that is compiled by XLA for performance.\nAutomatic Differentiation: JAX offers a trivial way to write diffrentiable distributed code.\nSame code runs on anything from a laptop to multi node supercomputer.\n\n\n\n\n\n\nTraditional Distributed Computing: MPI and OpenMP have been essential tools for achieving high-performance distributed computing in fields like cosmology. These frameworks provide fine control but often require specific parallel programming expertise.\nAccessibility and Familiarity: JAX‚Äôs familiar, high-level syntax lowers the barrier to entry, bringing distributed computing within reach of Python users without the need to manage intricate MPI or OpenMP settings.\nJAX‚Äôs Unique Advantages: Through XLA compilation, JAX not only scales code efficiently but also integrates differentiability, crucial for machine learning and simulations that require backpropagation. This blend of performance and flexibility sets JAX apart for scientific and AI applications.\n\nTalking Points on Alternative Framework Limitations\n\nOpen MPI: Primarily optimized for CPU-based parallelism, making it less effective on GPUs where scientific workloads in JAX often run. This can limit its efficiency for cosmology applications that leverage GPU acceleration.\nML Frameworks (PyTorch, TensorFlow): While powerful for machine learning, these frameworks are ML-centric and don‚Äôt natively support the arbitrary scientific functions often needed in cosmology. Customizing these frameworks for scientific use cases requires significant additional effort.\nDiffEqFlux.jl (Julia): While Julia‚Äôs ecosystem is growing, it‚Äôs still limited compared to Python, particularly for scientific computing and distributed applications. This smaller ecosystem can make it harder to find compatible tools and libraries for complex cosmological simulations.\nHorovod and Ray: Neither is natively differentiable, which means they rely on external frameworks (e.g., PyTorch or TensorFlow) for differentiation. This lack of built-in differentiability adds overhead and complexity for workflows that require gradient-based optimization, a key feature that JAX integrates seamlessly.\n\nFor questions\nPyTorch Distributed\n\nComplex Setup: Requires more effort to configure distributed training, especially outside deep learning.\nPerformance Overhead: Lacks JAX‚Äôs XLA compilation, which can lead to inefficiencies in scientific applications.\nLimited Scientific Libraries: PyTorch‚Äôs ecosystem is growing, but it still lacks the depth of JAX for scientific and physics-based computing.\n\nTensorFlow Distributed\n\nComplex and Verbose: Distributed setup with tf.distribute.Strategy is often more cumbersome and requires multiple API layers.\nLess Flexibility with Gradients: Limited flexibility in complex gradient computations compared to JAX‚Äôs functional approach.\nUnder-Optimized for Scientific Workflows: XLA support is not as performant in scientific HPC compared to JAX.\n\nRay with Auto-Differentiation\n\nNot Natively Differentiable: Ray relies on external libraries (like PyTorch and TensorFlow) for differentiation, adding communication and synchronization overhead.\nFocus on General Purpose Computing: Lacks specific optimizations for HPC environments and scientific computing.\nLimited Low-Level Hardware Control: Ray abstracts device management, reducing optimization potential in specialized HPC setups.\n\nDiffEqFlux.jl (Julia)\n\nLimited Ecosystem: Julia‚Äôs ecosystem is smaller and less mature, especially for scientific computing.\nDeveloping Distributed Support: Distributed computing in Julia is still evolving and less robust than in Python.\nLearning Curve: Julia has a steeper learning curve for Python-based teams, and integration with Python infrastructure can be difficult.\n\nMesh TensorFlow\n\nSpecialized for Transformers: Primarily designed for partitioning large transformer models, limiting flexibility in other scientific applications.\nComplex Configuration: Mesh configuration is often challenging and may be a barrier for scientific users.\nTied to TensorFlow: Mesh TensorFlow‚Äôs dependency on TensorFlow makes it less intuitive for scientific computing compared to JAX‚Äôs NumPy-like API.\n\nHorovod (Multi-Framework)\n\nOptimized for Data Parallelism in ML: Primarily suited for data parallelism in ML, not as adaptable for complex scientific workflows.\nExternal Library Dependence: Requires frameworks like PyTorch or TensorFlow for auto-differentiation, adding performance overhead.\nLimited Scientific Integration: Does not integrate well with libraries focused on physical simulations, whereas JAX has a growing ecosystem for such applications.\n\nKey Advantages of JAX\n\nUnified, Pythonic API: JAX‚Äôs intuitive API combines ease of use with the power of distributed computing.\nXLA Compilation for Efficiency: Optimized for performance across devices, making it highly suitable for HPC environments.\nNative Differentiability: Differentiability is built-in and seamlessly integrated with distributed workflows, providing a smooth experience for scientific applications."
  },
  {
    "objectID": "paris2024/index.html#expressing-parallelism-in-jax-simple-parallelism",
    "href": "paris2024/index.html#expressing-parallelism-in-jax-simple-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Expressing Parallelism in JAX (Simple parallelism)",
    "text": "Expressing Parallelism in JAX (Simple parallelism)\nExample of computing a gaussian from data Points\nimport jax\nimport jax.numpy as jnp\nfrom jax.debug import visualize_array_sharding\n\ndef gaussian(x, mean, variance):\n  coefficient = 1.0 / jnp.sqrt(2 * jnp.pi * variance)\n  exponent = -((x - mean) ** 2) / (2 * variance)\n  return coefficient * jnp.exp(exponent)\n\nmean = 0.0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n\n  GPU 0  \n         \n\n\n  GPU 0"
  },
  {
    "objectID": "paris2024/index.html#expressing-parallelism-in-jax-simple-parallelism-1",
    "href": "paris2024/index.html#expressing-parallelism-in-jax-simple-parallelism-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Expressing Parallelism in JAX (Simple parallelism)",
    "text": "Expressing Parallelism in JAX (Simple parallelism)\nExample of computing a gaussian from data Points\nassert jax.device_count() == 8\n\nfrom jax.sharding import PartitionSpec as P, NamedSharding\n\ndef gaussian(x, mean, variance):\n  coefficient = 1.0 / jnp.sqrt(2 * jnp.pi * variance)\n  exponent = -((x - mean) ** 2) / (2 * variance)\n  return coefficient * jnp.exp(exponent)\n\nmesh = jax.make_mesh((8,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\n\nmean = 0.0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding)\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n  GPU 0    GPU 1    GPU 2    GPU 3    GPU 4    GPU 5    GPU 6    GPU 7  \n                                                                        \n\n\n  GPU 0    GPU 1    GPU 2    GPU 3    GPU 4    GPU 5    GPU 6    GPU 7"
  },
  {
    "objectID": "paris2024/index.html#expressing-parallelism-in-jax-using-collectives",
    "href": "paris2024/index.html#expressing-parallelism-in-jax-using-collectives",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Expressing Parallelism in JAX (Using collectives)",
    "text": "Expressing Parallelism in JAX (Using collectives)\nExample of SGD with Gradient averaging (from Jean-Eric‚Äôs tutorial)\n\n\n@jax.jit  \ndef gradient_descent_step(p, xi, yi, lr=0.1):\n  gradients = jax.grad(loss_fun)(p, xi, yi)\n  return p - lr * gradients\n\ndef minimzer(loss_fun, x_data, y_data, par_init, method, verbose=True):\n  ...\n# Example usage\npar_mini_GD = minimzer(\n  loss_fun, \n  x_data=xin, \n  y_data=yin, \n  par_init=jnp.array([0., 0.5]), \n  method=partial(gradient_descent_step, lr=0.5), \n  verbose=True\n)"
  },
  {
    "objectID": "paris2024/index.html#expressing-parallelism-in-jax-using-collectives-1",
    "href": "paris2024/index.html#expressing-parallelism-in-jax-using-collectives-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Expressing Parallelism in JAX (Using collectives)",
    "text": "Expressing Parallelism in JAX (Using collectives)\nExample of SGD with Gradient averaging (from Jean-Eric‚Äôs tutorial)\nfrom jax.experimental.shard_map import shard_map\n\n@jax.jit \n@partial(shard_map, mesh=mesh , in_specs=P('x'), out_spec=P('x'))\ndef gradient_descent_step(p, xi, yi, lr=0.1):\n      per_device_gradients = jax.grad(loss_fun)(p, xi, yi)\n      avg_gradients = jax.lax.pmean(per_device_gradients, axis_name='x')\n      return p - lr * avg_gradients\n\ndef minimzer(loss_fun, x_data, y_data, par_init, method, verbose=True):\n     ...\n  # Example usage\nxin = jax.device_put(xin, sharding)\nyin = jax.device_put(yin, sharding)\npar_mini_GD = minimzer(\n        loss_fun, \n        x_data=xin, \n        y_data=yin, \n        par_init=jnp.array([0., 0.5]), \n        method=partial(gradient_descent_step, lr=0.5), \n        verbose=True\n    )"
  },
  {
    "objectID": "paris2024/index.html#jax-collective-operations-for-parallel-computing",
    "href": "paris2024/index.html#jax-collective-operations-for-parallel-computing",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "JAX Collective Operations for Parallel Computing",
    "text": "JAX Collective Operations for Parallel Computing\nOverview of JAX Collectives in jax.lax.p* Functions\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlax.pmean\nComputes the mean of arrays across devices. Useful for averaging gradients in distributed training.\n\n\nlax.ppermute\nPermutes data across devices in a specified order. Very useful in cosmological simulations.\n\n\nlax.all_to_all\nExchanges data between devices in a controlled manner. Useful for custom data exchange patterns in distributed computing.\n\n\nlax.pmax / lax.pmin\nComputes the element-wise maximum/minimum across devices. Often used in situations where you want to find the maximum or minimum of a distributed dataset.\n\n\nlax.psum\nSums arrays across devices. Commonly used for aggregating gradients or other values in distributed settings.\n\n\nlax.pall\nChecks if all values across devices are True. Often used for collective boolean checks across distributed data.\n\n\n\n\n\nGradient Aggregation: In distributed training, pmean is commonly used to average gradients from multiple devices, ensuring each device updates with the same gradient.\nLogical Collectives: Operators like pand, por, and pall allow distributed boolean logic operations, which can help in synchronization or conditional checks in parallel code.\nFlexible Data Distribution: ppermute allows data rearrangement across devices, making it useful in more complex parallelism setups or for rearranging distributed data for specific computations."
  },
  {
    "objectID": "paris2024/index.html#a-node-vs-a-supercomputer",
    "href": "paris2024/index.html#a-node-vs-a-supercomputer",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "A Node vs a Supercomputer",
    "text": "A Node vs a Supercomputer\n\n\nDifferences in Scale\n\n\nSingle GPU:\n\nMaximum memory: 80 GB\n\n\n\n\n\nSingle Node (Octocore):\n\nMaximum memory: 640 GB\nContains multiple GPUs (e.g., 8 A100 GPUs) connected via high-speed interconnects.\n\n\n\n\n\nMulti-Node Cluster:\n\nInfinite Memory üéâ\nConnects multiple nodes, allowing scaling across potentially thousands of GPUs.\n\n\n\n\n\n\nMulti-Node scalability with Jean Zay\n\n\n\nUp to 30TB of memory using all 48 nodes of Jean Zay\nIs enough to run a 15 billion particle simulation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@credit: NVIDIA\n\n\n\n\n\n\n\n@credit: servethehome.com\n\n\n\n\n\n\n\nSingle GPU: GPUs have powerful cores but are limited by memory. With a max memory of 80 GB, they are ideal for tasks that fit within this memory constraint, often used for model training or inference.\nSingle Node (Octocore): An octocore node can host multiple GPUs (e.g., 8 GPUs) and has larger memory (up to 640 GB), enabling it to handle larger datasets. This setup is common in high-performance servers.\nMulti-Node Cluster: By connecting nodes in a distributed cluster, we achieve ‚Äúinfinite‚Äù scalability in terms of memory and compute. JAX can take advantage of this via distributed parallelism, making it ideal for cosmological simulations and other large-scale scientific computations."
  },
  {
    "objectID": "paris2024/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup",
    "href": "paris2024/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Scaling JAX on a Single GPU vs.¬†Multi-Host Setup",
    "text": "Scaling JAX on a Single GPU vs.¬†Multi-Host Setup\n\n\nSingle GPU Code\nx = jnp.linspace(-5, 5, 128)\nmean = 0.0\nvariance = 1.0\nresult = gaussian(x, mean, variance)\n\nMulti-GPU Code\nmesh = jax.make_mesh((8,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding)\nmean = 0.0\nvariance = 1.0\nresult = gaussian(x, mean, variance)\n\nMulti-Host Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingle GPU Code: This example demonstrates using jax.pmap for parallel computation on a single GPU.\nMulti-Host Setup: The code placeholder highlights the need for additional configuration to distribute JAX computations across multiple hosts, allowing for large-scale distributed workloads.\nSingle GPU vs Multi-Host: JAX makes it easy to parallelize across local devices (GPUs on the same host), but multi-host configurations require further setup and are used for even larger-scale tasks."
  },
  {
    "objectID": "paris2024/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-1",
    "href": "paris2024/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Scaling JAX on a Single GPU vs.¬†Multi-Host Setup",
    "text": "Scaling JAX on a Single GPU vs.¬†Multi-Host Setup\nA JAX process per GPU\n\n\n\nRequesting a slurm job\n$ salloc --gres=gpu:8 --ntasks-per-node=1 --nodes=1\n\n\n\nmulti-host-jax.py\n\nimport jax\n\nmesh = jax.make_mesh((4,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\n\ndef gaussian(x, mean, variance):\n    ...\nmean = 0.0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding)\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n\nRunning with srun\n$ srun python multi-host-jax.py"
  },
  {
    "objectID": "paris2024/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-2",
    "href": "paris2024/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-2",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Scaling JAX on a Single GPU vs.¬†Multi-Host Setup",
    "text": "Scaling JAX on a Single GPU vs.¬†Multi-Host Setup\nA JAX process per GPU\n\n\n\nRequesting a slurm job\n$ salloc --gres=gpu:8 --ntasks-per-node=8 --nodes=2\n\n\n\nmulti-host-jax.py\n\nimport jax\njax.distributed.initialize()\nmesh = jax.make_mesh((16,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\n\ndef gaussian(x, mean, variance):\n    ...\nmean = 0.0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding) ‚ùå # DOES NOT WORK\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n\nRunning with srun\n$ srun -n 8 python multi-host-jax.py"
  },
  {
    "objectID": "paris2024/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-3",
    "href": "paris2024/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-3",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Scaling JAX on a Single GPU vs.¬†Multi-Host Setup",
    "text": "Scaling JAX on a Single GPU vs.¬†Multi-Host Setup\nA JAX process per GPU\n\n\n\nRequesting a slurm job\n$ salloc --gres=gpu:8 --ntasks-per-node=8 --nodes=2\n\n\n\nmulti-host-jax.py\n\nimport jax\njax.distributed.initialize()\nmesh = jax.make_mesh((16,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\n\ndef gaussian(x, mean, variance):\n    ...\nmean = 0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding) ‚ùå # DOES NOT WORK\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n\nRunning with srun\n$ srun -n 8 python multi-host-jax.py\n\n\n\n\n\n\n\n\n\n\nCAUTION ‚ö†Ô∏è\n\n\n\njax.device_put does not work with multi-host setups.\nAllocating a jax numpy array does not have the same behavior as single node setups."
  },
  {
    "objectID": "paris2024/index.html#loading-data-in-jax-in-a-multi-host-setup",
    "href": "paris2024/index.html#loading-data-in-jax-in-a-multi-host-setup",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Loading Data in JAX in a Multi-Host Setup",
    "text": "Loading Data in JAX in a Multi-Host Setup\nA JAX process per GPU\n\nimport jax\njax.distributed.initialize()\n\nassert jax.device_count() == 16\n\nx = jnp.linspace(-5, 5, 128)\nvisualize_array_sharding(x)\n\n\n\n  GPU 0  \n         \n\n  GPU 2  \n         \n\n  GPU 1  \n         \n\n  GPU 3  \n         \n\n  GPU 14  \n         \n\n  GPU 8  \n         \n\n  GPU 7  \n         \n\n\n  GPU 5  \n         \n\n  GPU 6  \n         \n\n  GPU 4  \n         \n\n  GPU 15  \n         \n\n  GPU 12  \n         \n\n  GPU 13  \n         \n\n  GPU 11"
  },
  {
    "objectID": "paris2024/index.html#loading-data-in-jax-in-a-multi-host-setup-1",
    "href": "paris2024/index.html#loading-data-in-jax-in-a-multi-host-setup-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Loading Data in JAX in a Multi-Host Setup",
    "text": "Loading Data in JAX in a Multi-Host Setup\nA JAX process per GPU\n\n\n\nmulti-host-jax.py\n\nimport jax\njax.distributed.initialize()\n\nmesh = jax.make_mesh((16,) , ('x',))\nsharding = NamedSharding(mesh , P('x'))\n\ndef distributed_linspace(start, stop, num):\n    def local_linspace(indx):\n        return np.linspace(start, stop, num)[indx]\n    return jax.make_array_from_callback(shape=(num,), sharding=sharding,data_callback=local_linspace)\n\nx = distributed_linspace(-5, 5, 128)\nif jax.process_index() == 0:\n  visualize_array_sharding(x)\n\n\n  ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶  \n  0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15"
  },
  {
    "objectID": "paris2024/index.html#loading-data-in-jax-in-a-multi-host-setup-2",
    "href": "paris2024/index.html#loading-data-in-jax-in-a-multi-host-setup-2",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Loading Data in JAX in a Multi-Host Setup",
    "text": "Loading Data in JAX in a Multi-Host Setup\nA JAX process per GPU\n\n\n\nmulti-host-jax.py\n\nimport jax\njax.distributed.initialize()\n\nmesh = jax.make_mesh((16,) , ('x',))\nsharding = NamedSharding(mesh , P('x'))\n\ndef distributed_linspace(start, stop, num):\n    def local_linspace(indx):\n        return np.linspace(start, stop, num)[indx]\n    return jax.make_array_from_callback(shape=(num,), sharding=sharding,data_callback=local_linspace)\n\nx = distributed_linspace(-5, 5, 128)\nif jax.process_index() == 0:\n  visualize_array_sharding(x)\nmean = 0.0\nvariance = 1.0\nresult = gaussian(x, mean, variance)\nif jax.process_index() == 0:\n  visualize_array_sharding(result)\n\n\n  ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶  \n  0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15  \n                                                                                \n\n\n  ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶    ‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶   G‚Ä¶  \n  0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15"
  },
  {
    "objectID": "paris2024/index.html#jaxdecomp-components-for-distributed-particle-mesh-simulations",
    "href": "paris2024/index.html#jaxdecomp-components-for-distributed-particle-mesh-simulations",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "jaxDecomp : Components for Distributed Particle Mesh Simulations",
    "text": "jaxDecomp : Components for Distributed Particle Mesh Simulations\n\n\n\nKey Features\n\n\nDistributed 3D FFT\n\nEssential for force calculations in large-scale simulations.\n\n\n\n\n\nHalo Exchange for Boundary Conditions\n\nManages boundary conditions or particles leaving the simulation domain.\n\n\n\n\n\nFully Differentiable\n\nCan be used with differentiable simulations.\n\n\n\n\n\nMulti-Node Supports\n\nWorks seamlessly across multiple nodes.\n\n\n\n\n\nSupports Different Sharding strategies\n\n\n\n\nOpen-source and available on PyPI"
  },
  {
    "objectID": "paris2024/index.html#performance-benchmarks-of-pfft3d",
    "href": "paris2024/index.html#performance-benchmarks-of-pfft3d",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Performance benchmarks of PFFT3D",
    "text": "Performance benchmarks of PFFT3D\n \n\n\nStrong Scaling\n\n\n\n\n\n\nWeak scaling"
  },
  {
    "objectID": "paris2024/index.html#halo-exchange-in-distributed-simulations",
    "href": "paris2024/index.html#halo-exchange-in-distributed-simulations",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Halo exchange in distributed simulations",
    "text": "Halo exchange in distributed simulations\n\n\n\n\n\n\n\n\n\n\n\nInitial Field\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nLPT Field"
  },
  {
    "objectID": "paris2024/index.html#jaxpm-2.0-distributed-particle-mesh-simulation",
    "href": "paris2024/index.html#jaxpm-2.0-distributed-particle-mesh-simulation",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "JaxPM 2.0 : Distributed Particle Mesh Simulation",
    "text": "JaxPM 2.0 : Distributed Particle Mesh Simulation\n\n\nBox size: 1G Mpc/h Resolution: \\(1024^3\\) Number of particles: 1 billion Number of snapshots: 10 Halo size: 128 Number of GPU used : 32 time taken : 45s\n\n     \n\n\n\nKey Features of JaxPM\n\n\n\nMulti-Node Performance: Optimized for efficient scaling across nodes.\nHigh Resolution: Capable of handling billions of particles for accurate simulations.\nDifferentiable: Compatible with JAX‚Äôs automatic differentiation (HMC, NUTS compatible).\nOpen Source:"
  },
  {
    "objectID": "paris2024/index.html#conclusion-enabling-scalable-cosmology-with-distributed-jax",
    "href": "paris2024/index.html#conclusion-enabling-scalable-cosmology-with-distributed-jax",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Conclusion: Enabling Scalable Cosmology with Distributed JAX",
    "text": "Conclusion: Enabling Scalable Cosmology with Distributed JAX\n\n\nDistributed JAX: A Game-Changer for Cosmology\n\n\n\nThe future is bright for JAX in cosmology üéâüéâ!!\nJAX has transformed the landscape for scientific computing, enabling large-scale, distributed workflows in a Pythonic environment.\nRecent advancements (JAX 0.4.3x+) make it straightforward to scale computations across multiple GPUs and nodes.\nKey Advantages\n\nSimplicity: JAX makes it easier than ever to write high-performance code, allowing researchers to focus on science rather than infrastructure.\nDifferentiability: JAX allows seamless differentiation of code running across hundreds of GPUs, enabling advanced inference techniques.\n\nThe Future Ahead\n\nScaling Inference Models with Distributed jaxPM: By integrating the new distributed jaxPM into existing cosmological inference models, we can achieve unprecedented levels of detail and complexity.\nPaving the way to fully leverage large-scale survey data for deeper insights into the universe."
  },
  {
    "objectID": "csi2024/index.html#accelerating-bayesian-inference-in-cosmology",
    "href": "csi2024/index.html#accelerating-bayesian-inference-in-cosmology",
    "title": "CSI Presentation 2024",
    "section": "Accelerating Bayesian Inference in Cosmology",
    "text": "Accelerating Bayesian Inference in Cosmology\n \nWorking Framework\n‚û¢ ‚ÄÉ Using advanced software and tools to constrain cosmological parameters through Bayesian inference\n‚û¢ ‚ÄÉ Leveraging the Cosmic Microwave Background (CMB) as a tracer to constrain the tensor-to-scalar ratio, \\(r\\)\n‚û¢ ‚ÄÉ Utilizing weak lensing to constrain key cosmological parameters like \\(\\Omega_m\\) and \\(\\sigma_8\\)\n \n\n\nEmploying Cutting-Edge Tools\n‚û¢ ‚ÄÉ Transitioning from CPU-based NumPy to GPU-accelerated JAX for faster computation\n‚û¢ ‚ÄÉ Writing optimized CUDA code for cosmology-specific tools\n‚û¢ ‚ÄÉ Scaling cosmological simulations to run on multiple GPUs and HPC nodes for enhanced performance\n\n\nLe titre de ma th√®se est ¬´ Acc√©l√©rer les pipelines bay√©siens pour la cosmologie ¬ª. Ce travail vise √† acc√©l√©rer les processus de calcul pour extraire les param√®tres cosmologiques √† l‚Äôaide de l‚Äôinf√©rence bay√©sienne.\nL‚Äôinf√©rence bay√©sienne, en utilisant le Fond diffus cosmologique (CMB) comme principal traceur, nous aide √† contraindre des param√®tres importants comme le rapport tenseur-surface \\(r\\), fournissant des informations sur l‚Äôinflation et les conditions de l‚Äôunivers primitif.\n√Ä mesure que l‚Äôunivers √©volue et que des structures √† grande √©chelle se forment, nous utilisons le lentillage gravitationnel faible pour √©tudier ces structures et contraindre des param√®tres comme \\(\\Omega_m\\) et \\(\\sigma_8\\).\nNEXT\nSur le plan technique, nous passons des calculs bas√©s sur le CPU √† des workflows acc√©l√©r√©s par GPU en utilisant JAX, ce qui am√©liore consid√©rablement la vitesse de nos simulations.\nNous optimisons √©galement les outils cosmologiques avec CUDA et mettons √† l‚Äô√©chelle nos simulations pour les ex√©cuter sur plusieurs GPU et n≈ìuds de clusters de calcul haute performance (HPC) afin de traiter efficacement de grandes quantit√©s de donn√©es."
  },
  {
    "objectID": "csi2024/index.html#summary-of-projects",
    "href": "csi2024/index.html#summary-of-projects",
    "title": "CSI Presentation 2024",
    "section": "Summary of projects",
    "text": "Summary of projects\n\n\n\n\n\nProjects\n\n\n\n\nMon sujet de recherche est √† cheval entre deux domaines : la s√©paration des composants du CMB dans le cadre du projet SciPol, et le lentillage gravitationnel faible avec AstroDeep. Je vais vous pr√©senter ces deux projets plus en d√©tail dans les prochaines diapositives."
  },
  {
    "objectID": "csi2024/index.html#cosmic-microwave-background---scipol",
    "href": "csi2024/index.html#cosmic-microwave-background---scipol",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background - Scipol",
    "text": "Cosmic Microwave Background - Scipol\n\n\n\n\n\nProjects\n\n\n\n\nOn commence par la separation de composants pour le fond diffus cosmologique"
  },
  {
    "objectID": "csi2024/index.html#cosmic-microwave-background",
    "href": "csi2024/index.html#cosmic-microwave-background",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background",
    "text": "Cosmic Microwave Background\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cosmic Microwave Background (CMB) is the afterglow of the Big Bang, providing a snapshot of the early universe.\n\n\n\nThe CMB is polarized, consisting of E and B modes.\nE modes are curl-free, generated by density fluctuations.\nB modes could be evidence of primordial gravitational waves, indicating cosmic inflation.\nThe tensor-to-scalar ratio \\(r\\), which is the ratio of the tensor power spectrum to the scalar power spectrum\n\n\n\n\nLe fond diffus cosmologique, ou CMB, est le rayonnement fossile du Big Bang, qui nous offre une image de l‚Äôunivers primitif.\nLe CMB est polaris√© et se compose de deux types de modes : les E modes et les B modes.\nLes E modes ont un champ rotationnel nul et sont g√©n√©r√©s par les fluctuations de densit√© dans l‚Äôunivers. En revanche, les B modes, qui poss√®dent un champ rotationnel, pourraient indiquer la pr√©sence d‚Äôondes gravitationnelles primordiales.\nUn param√®tre cl√© li√© √† ces B modes est le rapport tensor-surface \\(r\\), qui mesure la force relative des ondes gravitationnelles par rapport aux perturbations de densit√©, ce qui nous aide √† mieux comprendre l‚Äôinflation cosmique."
  },
  {
    "objectID": "csi2024/index.html#cosmic-microwave-background-1",
    "href": "csi2024/index.html#cosmic-microwave-background-1",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background",
    "text": "Cosmic Microwave Background\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cosmic Microwave Background (CMB) signal is obscured by various foregrounds, making it challenging to detect the true cosmological information.\n\n\n\nDust: Emission from galactic dust adds significant noise to the CMB, particularly affecting polarization measurements.\n\n\n\n\nSynchrotron Radiation: Electrons spiraling in the galaxy‚Äôs magnetic fields produce synchrotron radiation, another major contaminant.\n\n\n \n\n\nComponent seperation methods\n\nBlind Methods: Like SMICA (Spectral Matching Independent Component Analysis)\nParametric Methods: Like FGbuster (Foreground Buster)\n\n\n \n\n\nLe signal du fond diffus cosmologique, ou CMB, est en r√©alit√© obscurci par plusieurs avant-plans, ce qui rend difficile l‚Äôextraction des informations cosmologiques r√©elles.\nL‚Äôun des contaminants principaux est la poussi√®re galactique. Cette poussi√®re √©met du rayonnement qui ajoute un bruit significatif au CMB, affectant particuli√®rement les mesures de polarisation.\nNEXT\nUn autre contaminant majeur est la radiation synchrotron. Elle est produite par des √©lectrons en spirale dans les champs magn√©tiques de notre galaxie, ce qui vient encore plus brouiller le signal cosmologique que l‚Äôon souhaite observer.\nAFTER\nPour pouvoir extraire une valeur fiable du rapport \\(r\\), il est crucial de s√©parer ou de ‚Äúd√©mixer‚Äù ces composants. Le signal du CMB est m√™l√© √† diverses √©missions parasites.\nIl existe diff√©rentes m√©thodes pour cela, principalement des m√©thodes aveugles comme SMICA, qui fonctionnent sans connaissances pr√©alables des avant-plans, et des m√©thodes param√©triques comme FGbuster, qui reposent sur la mod√©lisation explicite des avant-plans.\nDans cette pr√©sentation, nous allons nous concentrer sur les m√©thodes param√©triques. Celles-ci nous permettent d‚Äôutiliser des mod√®les pour les avant-plans et d‚Äôam√©liorer la pr√©cision du processus de s√©paration."
  },
  {
    "objectID": "csi2024/index.html#cmb-component-separation",
    "href": "csi2024/index.html#cmb-component-separation",
    "title": "CSI Presentation 2024",
    "section": "CMB Component Separation",
    "text": "CMB Component Separation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModified Blackbody SED of Dust:\n\\[\n\\boxed{s_{\\mathrm{d}}(\\nu) = A_{\\mathrm{d}} \\cdot \\frac{\\nu}{\\exp\\left(\\frac{h\\nu}{k \\color{red}{T_{\\mathrm{d}}}}\\right) - 1} \\cdot \\frac{\\exp\\left(\\frac{h\\nu_{0}}{k \\color{red}{T_{\\mathrm{d}}}}\\right) - 1}{\\nu_{0}} \\cdot \\left(\\frac{\\nu}{\\nu_{0}}\\right)^{\\color{blue}{\\beta}}}\n\\]\n\n\nPower Law of Synchrotron Emission:\n\\[\n\\boxed{s_{\\text{synch}}(\\nu) = \\left(\\frac{\\nu}{\\nu_0}\\right)^{\\color{green}{\\beta_{\\text{pl}}}}}\n\\]\n\n\n\n\nSignal Representation\n\n\n\\[\n\\boxed{\\mathbf{d} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}}\n\\]\n\n\\[\n\\boxed{\\mathbf{d} = \\color{green}{A_{\\text{synch}}} \\cdot s_{\\text{synch}} + \\color{blue}{A_{\\mathrm{d}}} \\cdot s_{\\mathrm{d}} + A_{\\text{cmb}} \\cdot s_{\\text{cmb}} + \\mathbf{n}}\n\\]\n\n\n\n\nLikelihood Function\n\\[\n\\boxed{-2 \\ln \\mathcal{L}_{\\text{data}}(\\mathbf{s}, \\boldsymbol{\\beta}) = \\text{const} + \\sum_{p} \\left( \\mathbf{d}_p - \\mathbf{A}_p \\mathbf{s}_p \\right)^T \\mathbf{N}_p^{-1} \\left( \\mathbf{d}_p - \\mathbf{A}_p \\mathbf{s}_p \\right)}\n\\]\n\n\n\n\nMinimization for Component Separation\n\\[\n\\boxed{\\mathbf{s} = \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d} \\right)^T \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d}}\n\\]\n\n\n\nMinimization for Component Separation\n\\[\n\\boxed{\\mathcal{L}(\\color{blue}{\\beta_d}, \\color{red}{T_d}, \\color{green}{\\beta_{\\text{pl}}}) = \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d} \\right)^T  \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d}}\n\\]\n\n\n\nDans cette diapositive, je vais expliquer comment on mod√©lise la s√©paration des composantes du CMB √† partir des donn√©es observ√©es.\nNous avons d‚Äôabord deux composantes principales qui contaminent le signal CMB pur : - La poussi√®re galactique, mod√©lis√©e par une loi de corps noir modifi√©e. Elle est une des sources d‚Äô√©mission la plus importante √† haute fr√©quence. - Le rayonnement synchrotron, produit par les √©lectrons en spirale dans les champs magn√©tiques de la galaxie, qui est une source dominante √† basse fr√©quence.\nOn repr√©sente ensuite le signal d par une combinaison lin√©aire des contributions de chaque composant multipli√©es par leur matrice de m√©lange respective, plus un bruit n.\nL‚Äôobjectif de la s√©paration des composantes est de maximiser la vraisemblance de nos donn√©es mod√©lis√©es par rapport aux donn√©es observ√©es. Cela se fait par une minimisation, repr√©sent√©e par l‚Äô√©quation en bas de la diapositive.\nLa m√©thode que nous utilisons ici est param√©trique, o√π chaque composante a un mod√®le physique avec des param√®tres sp√©cifiques comme \\(\\beta\\), \\(T_d\\) pour la poussi√®re et \\(\\beta_{pl}\\) pour le synchrotron.\n√Ä la fin, nous obtenons la partie CMB de la matrice de m√©lange, √† partir de laquelle nous allons pouvoir estimer le ratio tenseur-spectral \\(r\\), un param√®tre cl√© pour contraindre les mod√®les d‚Äôinflation cosmique."
  },
  {
    "objectID": "csi2024/index.html#minimization-process-in-cmb-component-separation-so-litebird",
    "href": "csi2024/index.html#minimization-process-in-cmb-component-separation-so-litebird",
    "title": "CSI Presentation 2024",
    "section": "Minimization Process in CMB Component Separation  ",
    "text": "Minimization Process in CMB Component Separation  \nUsing Scipols‚Äôs Furax Library (Chanial et al.¬†in prep.)\n \nblocks = jnp.arange(24).reshape(3, 2, 4)\np = DenseBlockDiagonalOperator(blocks, jax.ShapeDtypeStruct((3, 4), jnp.int32), 'imn,in-&gt;im')\nop.as_matrix()\nArray([[ 0,  1,  2,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 4,  5,  6,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0,  0,  8,  9, 10, 11,  0,  0,  0,  0],\n       [ 0,  0,  0,  0, 12, 13, 14, 15,  0,  0,  0,  0],\n       [ 0,  0,  0,  0,  0,  0,  0,  0, 16, 17, 18, 19],\n       [ 0,  0,  0,  0,  0,  0,  0,  0, 20, 21, 22, 23]], dtype=int32)\n \n\n\n\nMy contributions (in the context of SO and LiteBIRD)\n\n\n\nUse JAX tools to evaluate the spectral likelihood.\nApply gradient descent methods to minimize the likelihood function.\nNext steps\n\nAdapt code to handle multi-resolution data.\nImplement support for multi-patch analysis.\nWrite a paper about GPU-accelerated component seperation\n\n\n\n\n\n\nCette matrice bloc-diagonale que nous utilisons pour la s√©paration des composantes du CMB a une taille de l‚Äôordre de (fr√©quence, stokes, composante, npix), et peut rapidement atteindre plusieurs gigaoctets en m√©moire. Avec des r√©solutions √©lev√©es et de multiples fr√©quences, la gestion efficace de cette matrice devient critique. C‚Äôest pourquoi l‚Äôutilisation d‚Äôoutils comme JAX est indispensable pour optimiser les calculs, en exploitant l‚Äôacc√©l√©ration par GPU tout en minimisant la consommation de m√©moire et les temps de calcul.\nMon travail consiste √† utiliser les outils de JAX pour √©valuer la fonction de vraisemblance spectrale, puis appliquer des m√©thodes de descente de gradient pour minimiser cette fonction et optimiser la s√©paration des composantes du CMB.\n√Ä l‚Äôavenir, je pr√©vois d‚Äôadapter ce code pour g√©rer des donn√©es multi-r√©solution et permettre une analyse multi-patch, afin de traiter plus efficacement des r√©gions distinctes du ciel avec des r√©solutions vari√©es."
  },
  {
    "objectID": "csi2024/index.html#summary-of-projects-1",
    "href": "csi2024/index.html#summary-of-projects-1",
    "title": "CSI Presentation 2024",
    "section": "Summary of projects",
    "text": "Summary of projects\n\n\n\n\nProjects"
  },
  {
    "objectID": "csi2024/index.html#large-scale-structure---astrodeep",
    "href": "csi2024/index.html#large-scale-structure---astrodeep",
    "title": "CSI Presentation 2024",
    "section": "Large Scale Structure - AstroDeep",
    "text": "Large Scale Structure - AstroDeep\n\n\n\n\nProjects"
  },
  {
    "objectID": "csi2024/index.html#large-scale-structure---statistical-tools",
    "href": "csi2024/index.html#large-scale-structure---statistical-tools",
    "title": "CSI Presentation 2024",
    "section": "Large Scale Structure - Statistical Tools",
    "text": "Large Scale Structure - Statistical Tools\n\n\n\n\n\n\n\nHubble eXtreme Deep Field\n\n\n\n\n\n\n\nHubble eXtreme Deep Field\n\n\n\n\n\\[\n-2 \\underbrace{\\log P(g, \\boldsymbol{\\beta} \\mid \\mathbf{d})}_{\\text{Posterior}} =\n\\sum_{\\vec{k}} \\left[\\underbrace{\\frac{\\left|\\mathbf{d} - f(g \\mid \\boldsymbol{\\beta}, z)\\right|^2}{N}}_{\\text{Likelihood}} + \\underbrace{\\frac{|g|^2}{\\mathcal{P}(\\boldsymbol{\\beta})}}_{\\text{Prior}}\\right]_{\\vec{k}}\n\\]\n\n\n\n\n\n\nPrediction\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian field\n\n\n\n\n¬†\n\n\n\n\n\nLPT Field\n\n\n\n\n¬†\n\n\n\n\n\nGalaxies\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDans le cadre de mon travail avec AstroDeep, je me concentre sur le lentillage gravitationnel faible.\nLe lentillage gravitationnel faible est un tracer important pour mettre une contrante sur les parameters cosmologique li√©e √† la densit√© de matiere dans l‚Äôunivers et la formation des sctructure √† grande echelle.\ntraditionellement, on utilise une fonction de correlation √† deux points pour r√©sumer les donn√©es des champs de convergence. typiquiment on utlis√© le spectre de puissance .. tr√®s concreement il s‚Äôagit de encoder les distancess entre chaque pair de galaxy (ce qui assume des donn√©es gaussienne)\nCependant, cette approche est limit√©e car le champs observ√©es √† basse red shift est tout sauf gaussian.\nNEXT\nUne autre famille de methodes existent, notammement l‚Äôinference bas√© sur les simulation ou SBI. Cette approche consiste √† utiliser des simulations pour g√©n√©rer des donn√©es et des champs de convergence, et √ßa remplace l‚Äôutilisation d‚Äôune vraisemblance analytique.\nLa vraisemblance devient l‚Äôecart entre la sortie de la simulation et les donn√©es observ√©es, On ajoute aussi un apriori gaussian sur les parameters.\nNEXT\nUn exemple de ce genre de processus est illustr√© ici. On commence par g√©n√©rer un champ gaussien, puis on le fait √©voluer en utilisant une Simulation cosmologique. On utilise d‚Äôautre methodes specifique pour introduire les galaxies dans les amas de mati√®re noire puis on compare avec les donn√©es\nNEXT\nEn bref, on appelle le processus de g√©n√©rer les donn√©es √† partir de simulations Forward Modeling ou la pr√©diction, et le processus de comparer les donn√©es observ√©es avec les simulations Inference.\nmon travail consist √† faire un ce forward model"
  },
  {
    "objectID": "csi2024/index.html#hearchical-bayesian-modeling",
    "href": "csi2024/index.html#hearchical-bayesian-modeling",
    "title": "CSI Presentation 2024",
    "section": "Hearchical Bayesian Modeling",
    "text": "Hearchical Bayesian Modeling\n\n\n\n\n\nProbabilistic Graphical Model\n\n\n\n‚ûï ‚ÄÉNo longer need to compute the likelihood analytically  \n\n‚ûñ ‚ÄÉWe need to infer the joint posterior \\(p(\\beta, g | z)\\) before marginalization to get \\(p(\\beta | g) = \\int p(\\beta, z | g) \\, dz\\)\n\n\n\nPossible solutions\n\n\n\nHamiltonian Monte Carlo (NUTS)\nVariational Inference\nDimensionality reduction using Fisher Information Matrix\n\n\n\n\n\n\n\n\nDifferentiable forward model for differentiable sampling\n\n\n\nFull NBody simulation are too slow to be used in an iterative sampler\nDynamic grid-based simulations can be fast but hard to differentiate\nFast Particle-Mesh simulations are fast and differentiable\n\n\n\n\n\n\nDans le cadre de l‚Äôinf√©rence bay√©sienne hi√©rarchique, on utilise un mod√®le graphique probabiliste pour repr√©senter les relations entre les variables al√©atoires.\ntypiquiment ici j‚Äôai un exemple d‚Äôun model graphique probabiliste ou les cercle blanches sont les parameters echantillonable et les grise represent les parametes latents.\nL‚Äôavatage c‚Äôest que on a plus besoin de calculer la vraisemblance analytique,\nNext\nCe genre de simulateur dit explicit, ou les parameters intermediaire ou latents sont pas directement observables mais utilise, contrainerement √† un simulateur implicite ou aveugle ou les parameters ne sont plus interpretable, implique une augmentation majeur de temps pass√© √† marginaliser .\nNext\nPour r√©soudre ce probl√®me, on peut utiliser des m√©thodes d‚Äô√©chantillonnage comme le NUTS ou le HMC, ce genre de methodes peuvent √™tre significativement acc√©l√©r√©es en utilisant un mod√®le forward diff√©rentiable. En ayant acc√®s aux gradient, l‚Äôechantillonnage necessite moins de pas pour converger.\nNext\nPour ce faire, des simulation N Corps classique sont trop lentes pour √™tre utilis√©es dans un √©chantillonneur it√©ratif. Les simulations bas√©es sur des grilles dynamiques peuvent √™tre rapides mais difficiles √† diff√©rencier. Les simulations de particules-r√©seau sont rapides et diff√©rentiables."
  },
  {
    "objectID": "csi2024/index.html#fast-particle-mesh-simulations",
    "href": "csi2024/index.html#fast-particle-mesh-simulations",
    "title": "CSI Presentation 2024",
    "section": "Fast Particle-mesh simulations",
    "text": "Fast Particle-mesh simulations\n \n\n\nNumerical scheme\n\n‚û¢ ‚ÄÉInterpolate particles on a grid to estimate mass density\n\n\n‚û¢ ‚ÄÉEstimate gravitational force on grid points by FFT\n\n\n‚û¢ ‚ÄÉInterpolate forces back on particles\n\n\n‚û¢ ‚ÄÉUpdate particle velocity and positions, and iterate\n\n\n\n\n\n\n\n\n\\(\\begin{array}{c}{{\\nabla^{2}\\phi=-4\\pi G\\rho}}\\\\\\\\ {{f(\\vec{k})=i\\vec{k}k^{-2}\\rho(\\vec{k})}}\\end{array}\\)\n\n\n\n\n\n\n\n     \n\n\n\nFast and simple, at the cost of approximating short range interactions.\nIt is essentially a series of FFTs and interpolations\nIt is differentiable and can run on GPUs\n\n\n\n\n\nLes simulations de particules-mesh sont une m√©thode rapide et simple pour simuler l‚Äô√©volution des structures cosmologiques.\nLe sch√©ma num√©rique est assez simple :\non commence par interpoler les particules sur une grille pour estimer la densit√© de masse,\npuis on estime la force gravitationnelle sur les points de la grille en utilisant une transform√©e de Fourier rapide (FFT).\nOn interpole ensuite les forces sur les particules, on met √† jour les vitesses et les positions des particules, et on r√©p√®te le processus.\nOn peut demarrer d‚Äôun red shift 10 par exemple et faire evoluer le systeme jusqu‚Äô√† un red shift 0.\nNEXT\nCette m√©thode est rapide et simple, mais elle approxime les interactions √† courte port√©e. C‚Äôest essentiellement une s√©rie de FFT et d‚Äôinterpolations. Elle est diff√©rentiable et peut √™tre ex√©cut√©e sur des GPU."
  },
  {
    "objectID": "csi2024/index.html#fast-particle-mesh-scaling-lsst-desc",
    "href": "csi2024/index.html#fast-particle-mesh-scaling-lsst-desc",
    "title": "CSI Presentation 2024",
    "section": "Fast Particle-mesh scaling ",
    "text": "Fast Particle-mesh scaling \n‚û¢ ‚ÄÉ(Poqueres et al.¬†2021) : \\(64^3\\) mesh size, on a 1000 Mpc/h box\n‚û¢ ‚ÄÉ(Li et al.¬†2022) : \\(512^3\\) mesh size, using pmwd\n‚û¢ ‚ÄÉ(Lanusse et al.) : JaxPM similaire √† pmwd.\n‚û¢ ‚ÄÉFastPM : distributed but CPU-based\n\n\n\n\n\n\n\n\n\n\nInitial Conditions with a 1024 mesh\n\n\n\n\n\n\n\nInitial Conditions with a 64 mesh\n\n\n\n\n\n\n\n\nPower spectrum comparison\n\n\n\n\n\n\n\n\nMuti Node ( \\(\\infty\\) )\n\n\n\n\n\n\n\n\n\n\nFinal field with a 1024 mesh\n\n\n\n\n\n\n\nFinal field with a 64 mesh\n\n\n\n\n\n\nMy contributions (in the context of ISSC and LSST DESC)\n\n\nWe need a fast, differentiable and Scalable Particle-Mesh simulation that can run on multiple GPUs.\nMulti-GPU Particle mesh requires :\n\nDistributed FFT operations\n\nInterpolation scheme to handle boundary conditions in a distributed manner\n\n\n\n\n\n\n\nIl existe d√©ja quelque implementation de simulation de particules-mesh qui sont capable de simuler des boites de 1000 Mpc/h avec une r√©solution de 64^3 ou 512^3.\nLes deux exemples faites dans un papier de poqueres et l‚Äôautre par le package pmwd bas√© sur JAX.\nLe papier de poqueres √† utilis√© une r√©solution de 64^3 pour simuler une boite de 1000 Mpc/h, et le package pmwd peut aller jusq‚Äô√† une r√©solution de 512^3.\nJaxPM est un package similaire √† pmwd, qui est capable de simuler des r√©solutions similaires √† celles de pmwd et toujours sur un seul GPU.\nNEXT\nUn exemple de deux simulation faites sur une boite de 1 Gpc/h avec une r√©solution de 64^3 et 1024^3. On peut voir que la r√©solution plus √©lev√©e permet de capturer plus de d√©tails dans le champ de densit√©.\nNEXT\nSi on visualise le spectre de puissance de ces deux champs, on peut voir que la r√©solution plus basse sous-estime la densit√© de mati√®re et les interactions √† petite √©chelle. En fonction du type du but cosmologique , ce qui peut dire que ce type d‚Äôinference ne sera pas capable de mettre une contrainte mielleure voir meme pire par rapport √† une m√©thode bas√© sur des fonction de correlation √† deux points ou spectre de puissance.\nLa taille de m√©moire etant un facteur limitant, il est important de pouvoir mettre √† l‚Äô√©chelle ces simulations sur plusieurs GPU et n≈ìuds de calcul haute performance.\nLe passage √† une simulation multi-GPU n√©cessite des op√©rations de FFT distribu√©es, des interpolations distribu√©es et des conditions aux limites.\nNEXT\nNotre but serait de pouvoir faires simulation qui peuvent √™tre distribu√©es sur plusieur GPU et n≈ìuds de calcul haute performance. Tout en restant diffirentiable et rapide (quelque secondes pour chaque simulation).\npas √©t√© fait"
  },
  {
    "objectID": "csi2024/index.html#distributed-fast-fourier-transform",
    "href": "csi2024/index.html#distributed-fast-fourier-transform",
    "title": "CSI Presentation 2024",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n‚û¢ ‚ÄÉonly operation that requires communication is the FFT\n\nJaxdecomp\n\n\nimport jax\nimport jax.numpy as jnp\n\nfield = jax.random.normal(jax.random.PRNGKey(0), (1024, 1024, 1024))\nk_field = jnp.fft.fftn(field)\n\n\n\n\nL‚Äôop√©ration qui n√©cessite le plus de communication dans une simulation distribu√©e est la transform√©e de Fourier rapide (FFT).\nL‚Äôutilisation sur un seul GPU est triviale"
  },
  {
    "objectID": "csi2024/index.html#distributed-fast-fourier-transform-1",
    "href": "csi2024/index.html#distributed-fast-fourier-transform-1",
    "title": "CSI Presentation 2024",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n‚û¢ ‚ÄÉonly operation that requires communication is the FFT\n\nJaxdecomp\n\n DifferentiableUniverseInitiative/jaxDecomp\n\n\n\n\nimport jax\nimport jax.numpy as jnp\nimport jaxdecomp\n\ndevices = mesh_utils.create_device_mesh((2, 2))\nmesh = jax.sharding.Mesh(devices, axis_names=('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n\n# Create gaussian field distributed across the mesh\nfield = jax.make_array_from_single_device_arrays(\n        shape=mesh_shape,\n        sharding=sharding,\n        arrays=[jax.random.normal(jax.random.PRNGKey(rank), (512, 512, 1024))])\n\nk_field = jaxdecomp.fft.pfft3d(field)\n\n\n\n\n\n\n\n\n\n\nJaxDecomp features\n\n\n‚û¢ ‚ÄÉjaxDecomp supports 2D and 1D decompositions\n‚û¢ ‚ÄÉWorks for multi-node FFTs\n‚û¢ ‚ÄÉis differentiable\n‚û¢ ‚ÄÉThe package is also provided as a standalone library\n\n\n\n\n\n\n\n\n\n\n\n\n\navec JaxDecomp c‚Äôest aussi trivial faire des FFT distribu√©es sur plusieurs GPU et n≈ìuds de calcul.\nL‚Äôutilisation est aussi trivial que faire une FFT sur un seul GPU, il suffit de d√©crire la grille de distribution et la biblioth√®que s‚Äôoccupe du reste.\nNEXT\n√ßa supporte les d√©compositions 2D et 1D, fonctionne pour les FFT multi-n≈ìuds, tout en restant diff√©rentiable.\nNEXT\nC√¥t√© impl√©mentation, j‚Äôeffectue √† une s√©rie de FFT locales sur chaque GPU, en travaillant √† chaque fois sur un axe non distribu√©. Ensuite, j‚Äôeffectue une transposition de la grille de distribution pour redistribuer les donn√©es et traiter l‚Äôaxe suivant. Cela permet de r√©partir la charge de calcul efficacement entre plusieurs GPU tout en s‚Äôassurant que chaque transformation de Fourier est correctement align√©e avec l‚Äôaxe de calcul."
  },
  {
    "objectID": "csi2024/index.html#scaling-of-distributed-fft-operations",
    "href": "csi2024/index.html#scaling-of-distributed-fft-operations",
    "title": "CSI Presentation 2024",
    "section": "Scaling of Distributed FFT operations",
    "text": "Scaling of Distributed FFT operations\n\n\nDans cette figure, on peut voir les performances de la biblioth√®que JaxDecomp pour les FFT distribu√©es sur plusieurs GPU.\nOn est capable des faires des FFTs sur des grille de 4096^3 en quelque centaine de millisecondes (ce qui est tr√®s rapide).\nEt on voit deja qu‚Äôon peut aller jusqu‚Äô√† une grille de 4096^3 en utilisant 64 GPU. En comparaison, Les simulation actuelles avec un seul GPU ne d√©passent pas 1024^3."
  },
  {
    "objectID": "csi2024/index.html#interpolation-function-for-particle-mesh-simulations-cloud-in-cell",
    "href": "csi2024/index.html#interpolation-function-for-particle-mesh-simulations-cloud-in-cell",
    "title": "CSI Presentation 2024",
    "section": "Interpolation function for Particle-Mesh simulations (Cloud-in-Cell)",
    "text": "Interpolation function for Particle-Mesh simulations (Cloud-in-Cell)\n\n\n\nCIC Paint Function:\n\\[\n\\begin{array}{l c r}\ng({\\bf j})=\\sum_{i=1}^{N}m_{i}\\times\\prod_{d=1}^{D}\\left(1-\\left|p_{i}^{d}-j_{d}\\right|\\right)\n\\end{array}\n\\]\n\n\nForces Functions:\n\\[\n\\nabla^{2}\\phi = -4\\pi G\\kappa\\rho\n\\]\n\\[\nf(\\vec{k}) = \\dot{\\vec{k}}k^{-2}\\rho(\\vec{k})\n\\]\n\n\nCIC Read Function:\n\\[\nv_{i} = \\sum_{{\\bf j}}g({\\bf j})\\times\\prod_{d=1}^{D}\\left(1-|p_{i}^{d}-j_{d}|\\right)\n\\]\n\n\n\n\n‚û¢ ‚ÄÉPeriodic boundary conditions are applied to each slice of the simulation\n‚û¢ ‚ÄÉParticles cannot escape the simulation domain\n\n\n\n\n\n\n\n\nParticles\n\n\n\n\n\n\n\nCIC Kernel\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\n\n\n\n\n\n\nGradients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplacement\n\n\n\n\n\n\n\n\nPour les simulations de particules-mesh, on utilise une fonction d‚Äôinterpolation pour peindre les particules sur la grille.\nLa methode s‚Äôappelle Cloud-in-Cell (CIC), qui est une m√©thode simple et rapide pour interpoler les particules sur une grille.\nOn part d‚Äôune liste de particules repr√©sent√©es par leurs coordonn√©es et leur masse.\nNEXT\nEnsuite on ‚Äòpaint‚Äô chaque particules sur les cellules adjacentes.\nChaque particule contribue √† la densit√© de masse de la cellule en fonction de sa distance √† la cellule.\nPar exemple dans cette exemple, la plus part des particules sont proche de la cellule (1 , 0) donc elle aura le plus de densit√©.\nNEXT\nOn calcule les forces en utilisant la densit√© de masse interpol√©e sur la grille. On obtient donc des gradients.\nNEXT\nEnsuite on interpole les forces sur les particules pour les mettre √† jour.\nChaque gradient contribue au deplacement d‚Äôune particule en fonction de sa distance √† la cellule.\nNEXT\nCependant, cette m√©thode n‚Äôest pas compatible avec les simulations distribu√©es, car elle n√©cessite de lire les valeurs des cellules voisines, ce qui n‚Äôest pas possible si les cellules sont sur un autre GPU. Ce qui se passe sur un seul GPU c‚Äôest l‚Äôapplication de la condition periodique. Ce qui veut dire que les particules qui sort d‚Äôun sous domain reviennent de l‚Äôautre cot√© du meme domaine.\nCela le refl√®te pas la r√©alit√© physique, et peut introduire des erreurs dans les simulations.\nNEXT\nJe montre dans le slide suivant quel impact cela peut avoir sur les simulations."
  },
  {
    "objectID": "csi2024/index.html#halo-exchange-in-distributed-simulations",
    "href": "csi2024/index.html#halo-exchange-in-distributed-simulations",
    "title": "CSI Presentation 2024",
    "section": "Halo exchange in distributed simulations",
    "text": "Halo exchange in distributed simulations\n\n\n\n\n\n\n\n\n\n\n\nInitial Field\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nCIC Kernel\n\n\n\n\n\n\n\nHalo Exchangel\n\n\n\n\n\n\nfrom jaxdecomp import halo_exchange\n\nhalo_size = 128\nfield = halo_exchange(field, halo_extent=halo_size)\n\n\n\nOn commence par voir un champs de densit√© gaussien de , qui est distribu√© sur 4 GPU.\nChaque GPU a une partie du champ de densit√©, et peut evoluer de mani√®re ind√©pendante.\nApr√®s avoir calculer les forces avec la FFT distribu√©e, on obtient un champ de densit√© qui est distribu√© sur les 4 GPU.\nOn fait evoluer les particules ind√©pendamment, mais on voit que les particules qui sont proche des bords du domaine ne sont pas correctement trait√©es.\nNEXT\nOn a egalement une solution pour ce probl√®me, qui est l‚Äô√©change de halo. Ou un utilisateur peut definir une taille de halo, qui est la taille des cellules qui sont √©chang√©es entre les GPU.\nNEXT\nConcretement, pendant la phase de l‚Äôinterpolaction CIC, on permet les particules d‚Äô√™tre peintes sur des cellule plus grande que les cellules locales.\nPuis on echange la partie des cellules qui sont dans le halo avec les autres GPU.\nL‚Äô√©change applique les conditions periodiques sur les bords du domaine, et permet de traiter les particules qui sont proche des bords du domaine. Cela nous donne un resultat tr√®s similaire √† une simulation sur un seul GPU."
  },
  {
    "objectID": "csi2024/index.html#conclusion",
    "href": "csi2024/index.html#conclusion",
    "title": "CSI Presentation 2024",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nWork already done\n\n\n‚û¢ ‚ÄÉjaxDecomp : Distributed FFT operations and halo exchange (Released)\n‚û¢ ‚ÄÉjaxPM : Distributed Particle-Mesh simulations (Code)\n‚û¢ ‚ÄÉFurax : Minimization of the spectral likelihood for CMB component separation (Code)\n‚û¢ ‚ÄÉS2FFT : Accelerated spherical harmonics transforms (Pull request)\n\n\n\n\n\nWork in progress\n\n\n‚û¢ ‚ÄÉFurax (Continued) : Creation of a special optimiser that can handle multi-resolution and multi-patch data\n‚û¢ ‚ÄÉjaxPM : Benchmarking and testing the scaling of the simulations\n\n\n\n\n\nFuture work\n\n\n‚û¢ ‚ÄÉ Optimized and distributed spherical harmonics transforms for CMB lensing\n‚û¢ ‚ÄÉ Distributed probabilistic programming for hierarchical Bayesian Modeling"
  },
  {
    "objectID": "csi2024/index.html#attended-conferences",
    "href": "csi2024/index.html#attended-conferences",
    "title": "CSI Presentation 2024",
    "section": "Attended Conferences",
    "text": "Attended Conferences\n\n\nFrench Conferences\n\n\n\nIAP 2023 Machine Learning-\nLSST France 2023 Lyon, France\nLSST France 2024 Marseille, France (Talk)\n[Upcomming Conference] IAP GDR CoPhy Tools 2024 Paris, France (Talk)\n\n\n\n\n\nInternational Conferences\n\n\n\nMoriond Cosmology 2023 La Thuile, Italy (Poster)\nLSST DESC 2023 Zurich, Switzerland"
  },
  {
    "objectID": "csi2024/index.html#papers",
    "href": "csi2024/index.html#papers",
    "title": "CSI Presentation 2024",
    "section": "Papers",
    "text": "Papers\n \n\n\nCurrent/Submitted papers\n\n\n\nInfrared Radiometric Image Classification and Segmentation of Cloud Structure Using ML (Sommer et al.¬†2024, Published)\nJaxDecomp: A Distributed Fast Fourier Transform Library (Kabalan et al.¬†to be submitted soon)\nFurax: Optimization of the Large Scale Multiresolution Parametric Component Separation (Kabalan et al., for end of 2024)\nJaxPM: A Differentiable Particle-Mesh Simulation (Kabalan et al.¬†in prep.)\n\n\n\n\n\n\nFuture papers\n\n\n\nSpherical Harmonics for CMB component separation (Lead author)\nDistributed Probabilistic Programming for Hierarchical Bayesian Modeling"
  },
  {
    "objectID": "csi2024/index.html#formations",
    "href": "csi2024/index.html#formations",
    "title": "CSI Presentation 2024",
    "section": "Formations",
    "text": "Formations\n\n\n\ndivers\n\n\n\nEuclid summer school (2023)\nAISSAI AstroInfo Hackathon 2023, Frejus, France\nPhysics informed neural networks with the IDRIS team (Jean-zay super computer) (2024)\n\n\n\n\n\nCours Ecole Doctorale\n\n\n\nQCD with Matteo Cacciari (2023)\n\n\n\n\n\n\n\nCSI Presentation 2024"
  },
  {
    "objectID": "marseille2024/index.html#traditional-cosmological-inference",
    "href": "marseille2024/index.html#traditional-cosmological-inference",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Traditional cosmological inference",
    "text": "Traditional cosmological inference\n\nBayesian inference in cosmology\n\nWe need to infer the cosmological parameters \\(\\theta\\) that generated an observartion \\(x\\)\n\n\\[p(\\theta | x ) \\propto \\underbrace{p(x | \\theta)}_{\\mathrm{likelihood}} \\ \\underbrace{p(\\theta)}_{\\mathrm{prior}}\\]\n\n\n\n\n\n\n\n\n\n\n ‚û¢ ‚ÄÉCompute summary statistics based on the 2-point correlation function of the shear field\n\n\n ‚û¢ ‚ÄÉRun an MCMC chain to recover the posterior distribution of the cosmological parameters, using an analytical likelihood\n\n\n\n\n\n\nLimitations\n\n\n\nSimple summary statistics assume Gaussianity\nThe need to compute an analytical likelihood"
  },
  {
    "objectID": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration",
    "href": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport numpy as np\n\n\ndef multiply_and_add(a, b, c):\n    return np.dot(a, b) + c\n\n\na, b, c = np.random.normal(size=(3, 32, 32))\nresult = multiply_and_add(a, b, c)\n\n\nimport jax\nimport jax.numpy as jnp\n\n\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c)"
  },
  {
    "objectID": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration-1",
    "href": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration-1",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport jax\nimport jax.numpy as jnp\n\n\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c)"
  },
  {
    "objectID": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration-2",
    "href": "marseille2024/index.html#jax-automatic-differentiation-and-hardware-acceleration-2",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c) \ngradient = jax.grad(multiply_and_add)(a, b, c)\n \n\n\nJAX : Numpy + Autograd + GPU\n\n\n\njax.grad uses automatic differentiation to compute the gradient of the function\njax.jit compiles the function to run on GPUs"
  },
  {
    "objectID": "marseille2024/index.html#distributed-fast-fourier-transform",
    "href": "marseille2024/index.html#distributed-fast-fourier-transform",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n‚û¢ ‚ÄÉonly operation that requires communication is the FFT\n\nJaxdecomp\n\n\nimport jax\nimport jax.numpy as jnp\n\nfield = jax.random.normal(jax.random.PRNGKey(0), (1024, 1024, 1024))\nk_field = jnp.fft.fftn(field)"
  },
  {
    "objectID": "marseille2024/index.html#distributed-fast-fourier-transform-1",
    "href": "marseille2024/index.html#distributed-fast-fourier-transform-1",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n‚û¢ ‚ÄÉonly operation that requires communication is the FFT\n\nJaxdecomp\n\n DifferentiableUniverseInitiative/jaxDecomp\n\n\n\nimport jax\nimport jax.numpy as jnp\nimport jaxdecomp\n\ndevices = mesh_utils.create_device_mesh((2, 2))\nmesh = jax.sharding.Mesh(devices, axis_names=('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n\n# Create gaussian field distributed across the mesh\nfield = jax.make_array_from_single_device_arrays(\n    shape=mesh_shape,\n    sharding=sharding,\n    arrays=[jax.random.normal(jax.random.PRNGKey(rank), (512, 512, 1024))])\n\nk_field = jaxdecomp.fft.pfft3d(field)\n\n\n\n\nJaxDecomp features\n\n\n‚û¢ ‚ÄÉjaxDecomp supports 2D and 1D decompositions\n‚û¢ ‚ÄÉWorks for multi-node FFTs\n‚û¢ ‚ÄÉis differentiable\n‚û¢ ‚ÄÉThe package is also provided as a standalone library"
  },
  {
    "objectID": "marseille2024/index.html#halo-exchange-in-distributed-simulations",
    "href": "marseille2024/index.html#halo-exchange-in-distributed-simulations",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Halo exchange in distributed simulations",
    "text": "Halo exchange in distributed simulations\n\n\n\n\n\n\n\n\n\n\n\nInitial Field\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\nfrom jaxdecomp import halo_exchange\n\nhalo_size = 128\nfield = halo_exchange(field, halo_extent=halo_size)"
  },
  {
    "objectID": "marseille2024/index.html#conclusion",
    "href": "marseille2024/index.html#conclusion",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Conclusion ",
    "text": "Conclusion \n   \n\n\nDistruibuted Particle-Mesh simulations for cosmological inference\n\n\n\nA shift from analytical likelihoods to full field inference\n\nThe need for fast differentiable simulators\nParticle-Mesh as simulators for full field inference\nDistributed fourrier transforms that work on multi-node HPC using jaxDecomp\nHighly scalable LPT simulations using JaxPM\n\nStill subject to some challenges\n\nSome issues with the ODE solving step\nOnly Euler gives decent results.\n\n\n\n\n\n\n\nLSST France, 2024"
  }
]