[
  {
    "objectID": "2024_10_CSI/index.html#accelerating-bayesian-inference-in-cosmology",
    "href": "2024_10_CSI/index.html#accelerating-bayesian-inference-in-cosmology",
    "title": "CSI Presentation 2024",
    "section": "Accelerating Bayesian Inference in Cosmology",
    "text": "Accelerating Bayesian Inference in Cosmology\n \nWorking Framework\n➢   Using advanced software and tools to constrain cosmological parameters through Bayesian inference\n➢   Leveraging the Cosmic Microwave Background (CMB) as a tracer to constrain the tensor-to-scalar ratio, \\(r\\)\n➢   Utilizing weak lensing to constrain key cosmological parameters like \\(\\Omega_m\\) and \\(\\sigma_8\\)\n \n\nEmploying Cutting-Edge Tools\n➢   Transitioning from CPU-based NumPy to GPU-accelerated JAX for faster computation\n➢   Writing optimized CUDA code for cosmology-specific tools\n➢   Scaling cosmological simulations to run on multiple GPUs and HPC nodes for enhanced performance\n\n\nLe titre de ma thèse est « Accélérer les pipelines bayésiens pour la cosmologie ». Ce travail vise à accélérer les processus de calcul pour extraire les paramètres cosmologiques à l’aide de l’inférence bayésienne.\nL’inférence bayésienne, en utilisant le Fond diffus cosmologique (CMB) comme principal traceur, nous aide à contraindre des paramètres importants comme le rapport tenseur-surface \\(r\\), fournissant des informations sur l’inflation et les conditions de l’univers primitif.\nÀ mesure que l’univers évolue et que des structures à grande échelle se forment, nous utilisons le lentillage gravitationnel faible pour étudier ces structures et contraindre des paramètres comme \\(\\Omega_m\\) et \\(\\sigma_8\\).\nNEXT\nSur le plan technique, nous passons des calculs basés sur le CPU à des workflows accélérés par GPU en utilisant JAX, ce qui améliore considérablement la vitesse de nos simulations.\nNous optimisons également les outils cosmologiques avec CUDA et mettons à l’échelle nos simulations pour les exécuter sur plusieurs GPU et nœuds de clusters de calcul haute performance (HPC) afin de traiter efficacement de grandes quantités de données."
  },
  {
    "objectID": "2024_10_CSI/index.html#summary-of-projects",
    "href": "2024_10_CSI/index.html#summary-of-projects",
    "title": "CSI Presentation 2024",
    "section": "Summary of projects",
    "text": "Summary of projects\n\n\n\n\n\nProjects\n\n\n\n\nMon sujet de recherche est à cheval entre deux domaines : la séparation des composants du CMB dans le cadre du projet SciPol, et le lentillage gravitationnel faible avec AstroDeep. Je vais vous présenter ces deux projets plus en détail dans les prochaines diapositives."
  },
  {
    "objectID": "2024_10_CSI/index.html#cosmic-microwave-background---scipol",
    "href": "2024_10_CSI/index.html#cosmic-microwave-background---scipol",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background - Scipol",
    "text": "Cosmic Microwave Background - Scipol\n\n\n\n\n\nProjects\n\n\n\n\nOn commence par la separation de composants pour le fond diffus cosmologique"
  },
  {
    "objectID": "2024_10_CSI/index.html#cosmic-microwave-background",
    "href": "2024_10_CSI/index.html#cosmic-microwave-background",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background",
    "text": "Cosmic Microwave Background\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cosmic Microwave Background (CMB) is the afterglow of the Big Bang, providing a snapshot of the early universe.\n\n\n\nThe CMB is polarized, consisting of E and B modes.\nE modes are curl-free, generated by density fluctuations.\nB modes could be evidence of primordial gravitational waves, indicating cosmic inflation.\nThe tensor-to-scalar ratio \\(r\\), which is the ratio of the tensor power spectrum to the scalar power spectrum\n\n\n\n\nLe fond diffus cosmologique, ou CMB, est le rayonnement fossile du Big Bang, qui nous offre une image de l’univers primitif.\nLe CMB est polarisé et se compose de deux types de modes : les E modes et les B modes.\nLes E modes ont un champ rotationnel nul et sont générés par les fluctuations de densité dans l’univers. En revanche, les B modes, qui possèdent un champ rotationnel, pourraient indiquer la présence d’ondes gravitationnelles primordiales.\nUn paramètre clé lié à ces B modes est le rapport tensor-surface \\(r\\), qui mesure la force relative des ondes gravitationnelles par rapport aux perturbations de densité, ce qui nous aide à mieux comprendre l’inflation cosmique."
  },
  {
    "objectID": "2024_10_CSI/index.html#cosmic-microwave-background-1",
    "href": "2024_10_CSI/index.html#cosmic-microwave-background-1",
    "title": "CSI Presentation 2024",
    "section": "Cosmic Microwave Background",
    "text": "Cosmic Microwave Background\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Cosmic Microwave Background (CMB) signal is obscured by various foregrounds, making it challenging to detect the true cosmological information.\n\n\n\nDust: Emission from galactic dust adds significant noise to the CMB, particularly affecting polarization measurements.\n\n\n\n\nSynchrotron Radiation: Electrons spiraling in the galaxy’s magnetic fields produce synchrotron radiation, another major contaminant.\n\n\n \n\nComponent seperation methods\n\nBlind Methods: Like SMICA (Spectral Matching Independent Component Analysis)\nParametric Methods: Like FGbuster (Foreground Buster)\n\n\n \n\n\nLe signal du fond diffus cosmologique, ou CMB, est en réalité obscurci par plusieurs avant-plans, ce qui rend difficile l’extraction des informations cosmologiques réelles.\nL’un des contaminants principaux est la poussière galactique. Cette poussière émet du rayonnement qui ajoute un bruit significatif au CMB, affectant particulièrement les mesures de polarisation.\nNEXT\nUn autre contaminant majeur est la radiation synchrotron. Elle est produite par des électrons en spirale dans les champs magnétiques de notre galaxie, ce qui vient encore plus brouiller le signal cosmologique que l’on souhaite observer.\nAFTER\nPour pouvoir extraire une valeur fiable du rapport \\(r\\), il est crucial de séparer ou de “démixer” ces composants. Le signal du CMB est mêlé à diverses émissions parasites.\nIl existe différentes méthodes pour cela, principalement des méthodes aveugles comme SMICA, qui fonctionnent sans connaissances préalables des avant-plans, et des méthodes paramétriques comme FGbuster, qui reposent sur la modélisation explicite des avant-plans.\nDans cette présentation, nous allons nous concentrer sur les méthodes paramétriques. Celles-ci nous permettent d’utiliser des modèles pour les avant-plans et d’améliorer la précision du processus de séparation."
  },
  {
    "objectID": "2024_10_CSI/index.html#cmb-component-separation",
    "href": "2024_10_CSI/index.html#cmb-component-separation",
    "title": "CSI Presentation 2024",
    "section": "CMB Component Separation",
    "text": "CMB Component Separation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModified Blackbody SED of Dust:\n\\[\n\\boxed{s_{\\mathrm{d}}(\\nu) = A_{\\mathrm{d}} \\cdot \\frac{\\nu}{\\exp\\left(\\frac{h\\nu}{k \\color{red}{T_{\\mathrm{d}}}}\\right) - 1} \\cdot \\frac{\\exp\\left(\\frac{h\\nu_{0}}{k \\color{red}{T_{\\mathrm{d}}}}\\right) - 1}{\\nu_{0}} \\cdot \\left(\\frac{\\nu}{\\nu_{0}}\\right)^{\\color{blue}{\\beta}}}\n\\]\n\n\nPower Law of Synchrotron Emission:\n\\[\n\\boxed{s_{\\text{synch}}(\\nu) = \\left(\\frac{\\nu}{\\nu_0}\\right)^{\\color{green}{\\beta_{\\text{pl}}}}}\n\\]\n\n\n\nSignal Representation\n\n\n\\[\n\\boxed{\\mathbf{d} = \\mathbf{A} \\mathbf{s} + \\mathbf{n}}\n\\]\n\n\\[\n\\boxed{\\mathbf{d} = \\color{green}{A_{\\text{synch}}} \\cdot s_{\\text{synch}} + \\color{blue}{A_{\\mathrm{d}}} \\cdot s_{\\mathrm{d}} + A_{\\text{cmb}} \\cdot s_{\\text{cmb}} + \\mathbf{n}}\n\\]\n\n\n\nLikelihood Function\n\\[\n\\boxed{-2 \\ln \\mathcal{L}_{\\text{data}}(\\mathbf{s}, \\boldsymbol{\\beta}) = \\text{const} + \\sum_{p} \\left( \\mathbf{d}_p - \\mathbf{A}_p \\mathbf{s}_p \\right)^T \\mathbf{N}_p^{-1} \\left( \\mathbf{d}_p - \\mathbf{A}_p \\mathbf{s}_p \\right)}\n\\]\n\n\n\nMinimization for Component Separation\n\\[\n\\boxed{\\mathbf{s} = \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d} \\right)^T \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d}}\n\\]\n\n\nMinimization for Component Separation\n\\[\n\\boxed{\\mathcal{L}(\\color{blue}{\\beta_d}, \\color{red}{T_d}, \\color{green}{\\beta_{\\text{pl}}}) = \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d} \\right)^T  \\left( \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{A} \\right)^{-1} \\mathbf{A}^T \\mathbf{N}^{-1} \\mathbf{d}}\n\\]\n\n\n\nDans cette diapositive, je vais expliquer comment on modélise la séparation des composantes du CMB à partir des données observées.\nNous avons d’abord deux composantes principales qui contaminent le signal CMB pur : - La poussière galactique, modélisée par une loi de corps noir modifiée. Elle est une des sources d’émission la plus importante à haute fréquence. - Le rayonnement synchrotron, produit par les électrons en spirale dans les champs magnétiques de la galaxie, qui est une source dominante à basse fréquence.\nOn représente ensuite le signal d par une combinaison linéaire des contributions de chaque composant multipliées par leur matrice de mélange respective, plus un bruit n.\nL’objectif de la séparation des composantes est de maximiser la vraisemblance de nos données modélisées par rapport aux données observées. Cela se fait par une minimisation, représentée par l’équation en bas de la diapositive.\nLa méthode que nous utilisons ici est paramétrique, où chaque composante a un modèle physique avec des paramètres spécifiques comme \\(\\beta\\), \\(T_d\\) pour la poussière et \\(\\beta_{pl}\\) pour le synchrotron.\nÀ la fin, nous obtenons la partie CMB de la matrice de mélange, à partir de laquelle nous allons pouvoir estimer le ratio tenseur-spectral \\(r\\), un paramètre clé pour contraindre les modèles d’inflation cosmique."
  },
  {
    "objectID": "2024_10_CSI/index.html#minimization-process-in-cmb-component-separation-so-litebird",
    "href": "2024_10_CSI/index.html#minimization-process-in-cmb-component-separation-so-litebird",
    "title": "CSI Presentation 2024",
    "section": "Minimization Process in CMB Component Separation  ",
    "text": "Minimization Process in CMB Component Separation  \nUsing Scipols’s Furax Library (Chanial et al. in prep.)\n \nblocks = jnp.arange(24).reshape(3, 2, 4)\np = DenseBlockDiagonalOperator(blocks, jax.ShapeDtypeStruct((3, 4), jnp.int32), 'imn,in-&gt;im')\nop.as_matrix()\nArray([[ 0,  1,  2,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 4,  5,  6,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0,  0,  8,  9, 10, 11,  0,  0,  0,  0],\n       [ 0,  0,  0,  0, 12, 13, 14, 15,  0,  0,  0,  0],\n       [ 0,  0,  0,  0,  0,  0,  0,  0, 16, 17, 18, 19],\n       [ 0,  0,  0,  0,  0,  0,  0,  0, 20, 21, 22, 23]], dtype=int32)\n \n\n\n\nMy contributions (in the context of SO and LiteBIRD)\n\n\n\nUse JAX tools to evaluate the spectral likelihood.\nApply gradient descent methods to minimize the likelihood function.\nNext steps\n\nAdapt code to handle multi-resolution data.\nImplement support for multi-patch analysis.\nWrite a paper about GPU-accelerated component seperation\n\n\n\n\n\n\nCette matrice bloc-diagonale que nous utilisons pour la séparation des composantes du CMB a une taille de l’ordre de (fréquence, stokes, composante, npix), et peut rapidement atteindre plusieurs gigaoctets en mémoire. Avec des résolutions élevées et de multiples fréquences, la gestion efficace de cette matrice devient critique. C’est pourquoi l’utilisation d’outils comme JAX est indispensable pour optimiser les calculs, en exploitant l’accélération par GPU tout en minimisant la consommation de mémoire et les temps de calcul.\nMon travail consiste à utiliser les outils de JAX pour évaluer la fonction de vraisemblance spectrale, puis appliquer des méthodes de descente de gradient pour minimiser cette fonction et optimiser la séparation des composantes du CMB.\nÀ l’avenir, je prévois d’adapter ce code pour gérer des données multi-résolution et permettre une analyse multi-patch, afin de traiter plus efficacement des régions distinctes du ciel avec des résolutions variées."
  },
  {
    "objectID": "2024_10_CSI/index.html#summary-of-projects-1",
    "href": "2024_10_CSI/index.html#summary-of-projects-1",
    "title": "CSI Presentation 2024",
    "section": "Summary of projects",
    "text": "Summary of projects\n\n\n\n\nProjects"
  },
  {
    "objectID": "2024_10_CSI/index.html#large-scale-structure---astrodeep",
    "href": "2024_10_CSI/index.html#large-scale-structure---astrodeep",
    "title": "CSI Presentation 2024",
    "section": "Large Scale Structure - AstroDeep",
    "text": "Large Scale Structure - AstroDeep\n\n\n\n\nProjects"
  },
  {
    "objectID": "2024_10_CSI/index.html#large-scale-structure---statistical-tools",
    "href": "2024_10_CSI/index.html#large-scale-structure---statistical-tools",
    "title": "CSI Presentation 2024",
    "section": "Large Scale Structure - Statistical Tools",
    "text": "Large Scale Structure - Statistical Tools\n\n\n\n\n\n\n\nHubble eXtreme Deep Field\n\n\n\n\n\n\n\nHubble eXtreme Deep Field\n\n\n\n\n\\[\n-2 \\underbrace{\\log P(g, \\boldsymbol{\\beta} \\mid \\mathbf{d})}_{\\text{Posterior}} =\n\\sum_{\\vec{k}} \\left[\\underbrace{\\frac{\\left|\\mathbf{d} - f(g \\mid \\boldsymbol{\\beta}, z)\\right|^2}{N}}_{\\text{Likelihood}} + \\underbrace{\\frac{|g|^2}{\\mathcal{P}(\\boldsymbol{\\beta})}}_{\\text{Prior}}\\right]_{\\vec{k}}\n\\]\n\n\n\n\n\n\nPrediction\n\n\n\n\n\n\n\n\n\n\nGaussian field\n\n\n\n\n \n\n\n\n\n\nLPT Field\n\n\n\n\n \n\n\n\n\n\nGalaxies\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDans le cadre de mon travail avec AstroDeep, je me concentre sur le lentillage gravitationnel faible.\nLe lentillage gravitationnel faible est un tracer important pour mettre une contrante sur les parameters cosmologique liée à la densité de matiere dans l’univers et la formation des sctructure à grande echelle.\ntraditionellement, on utilise une fonction de correlation à deux points pour résumer les données des champs de convergence. typiquiment on utlisé le spectre de puissance .. très concreement il s’agit de encoder les distancess entre chaque pair de galaxy (ce qui assume des données gaussienne)\nCependant, cette approche est limitée car le champs observées à basse red shift est tout sauf gaussian.\nNEXT\nUne autre famille de methodes existent, notammement l’inference basé sur les simulation ou SBI. Cette approche consiste à utiliser des simulations pour générer des données et des champs de convergence, et ça remplace l’utilisation d’une vraisemblance analytique.\nLa vraisemblance devient l’ecart entre la sortie de la simulation et les données observées, On ajoute aussi un apriori gaussian sur les parameters.\nNEXT\nUn exemple de ce genre de processus est illustré ici. On commence par générer un champ gaussien, puis on le fait évoluer en utilisant une Simulation cosmologique. On utilise d’autre methodes specifique pour introduire les galaxies dans les amas de matière noire puis on compare avec les données\nNEXT\nEn bref, on appelle le processus de générer les données à partir de simulations Forward Modeling ou la prédiction, et le processus de comparer les données observées avec les simulations Inference.\nmon travail consist à faire un ce forward model"
  },
  {
    "objectID": "2024_10_CSI/index.html#hearchical-bayesian-modeling",
    "href": "2024_10_CSI/index.html#hearchical-bayesian-modeling",
    "title": "CSI Presentation 2024",
    "section": "Hearchical Bayesian Modeling",
    "text": "Hearchical Bayesian Modeling\n\n\n\n\n\nProbabilistic Graphical Model\n\n\n\n➕  No longer need to compute the likelihood analytically  \n\n➖  We need to infer the joint posterior \\(p(\\beta, g | z)\\) before marginalization to get \\(p(\\beta | g) = \\int p(\\beta, z | g) \\, dz\\)\n\n\n\nPossible solutions\n\n\n\nHamiltonian Monte Carlo (NUTS)\nVariational Inference\nDimensionality reduction using Fisher Information Matrix\n\n\n\n\n\n\n\n\nDifferentiable forward model for differentiable sampling\n\n\n\nFull NBody simulation are too slow to be used in an iterative sampler\nDynamic grid-based simulations can be fast but hard to differentiate\nFast Particle-Mesh simulations are fast and differentiable\n\n\n\n\n\n\nDans le cadre de l’inférence bayésienne hiérarchique, on utilise un modèle graphique probabiliste pour représenter les relations entre les variables aléatoires.\ntypiquiment ici j’ai un exemple d’un model graphique probabiliste ou les cercle blanches sont les parameters echantillonable et les grise represent les parametes latents.\nL’avatage c’est que on a plus besoin de calculer la vraisemblance analytique,\nNext\nCe genre de simulateur dit explicit, ou les parameters intermediaire ou latents sont pas directement observables mais utilise, contrainerement à un simulateur implicite ou aveugle ou les parameters ne sont plus interpretable, implique une augmentation majeur de temps passé à marginaliser .\nNext\nPour résoudre ce problème, on peut utiliser des méthodes d’échantillonnage comme le NUTS ou le HMC, ce genre de methodes peuvent être significativement accélérées en utilisant un modèle forward différentiable. En ayant accès aux gradient, l’echantillonnage necessite moins de pas pour converger.\nNext\nPour ce faire, des simulation N Corps classique sont trop lentes pour être utilisées dans un échantillonneur itératif. Les simulations basées sur des grilles dynamiques peuvent être rapides mais difficiles à différencier. Les simulations de particules-réseau sont rapides et différentiables."
  },
  {
    "objectID": "2024_10_CSI/index.html#fast-particle-mesh-simulations",
    "href": "2024_10_CSI/index.html#fast-particle-mesh-simulations",
    "title": "CSI Presentation 2024",
    "section": "Fast Particle-mesh simulations",
    "text": "Fast Particle-mesh simulations\n \n\n\nNumerical scheme\n\n➢  Interpolate particles on a grid to estimate mass density\n\n\n➢  Estimate gravitational force on grid points by FFT\n\n\n➢  Interpolate forces back on particles\n\n\n➢  Update particle velocity and positions, and iterate\n\n\n\n\n\n\n\n\n\\(\\begin{array}{c}{{\\nabla^{2}\\phi=-4\\pi G\\rho}}\\\\\\\\ {{f(\\vec{k})=i\\vec{k}k^{-2}\\rho(\\vec{k})}}\\end{array}\\)\n\n\n\n\n\n\n\n     \n\n\n\nFast and simple, at the cost of approximating short range interactions.\nIt is essentially a series of FFTs and interpolations\nIt is differentiable and can run on GPUs\n\n\n\n\n\nLes simulations de particules-mesh sont une méthode rapide et simple pour simuler l’évolution des structures cosmologiques.\nLe schéma numérique est assez simple :\non commence par interpoler les particules sur une grille pour estimer la densité de masse,\npuis on estime la force gravitationnelle sur les points de la grille en utilisant une transformée de Fourier rapide (FFT).\nOn interpole ensuite les forces sur les particules, on met à jour les vitesses et les positions des particules, et on répète le processus.\nOn peut demarrer d’un red shift 10 par exemple et faire evoluer le systeme jusqu’à un red shift 0.\nNEXT\nCette méthode est rapide et simple, mais elle approxime les interactions à courte portée. C’est essentiellement une série de FFT et d’interpolations. Elle est différentiable et peut être exécutée sur des GPU."
  },
  {
    "objectID": "2024_10_CSI/index.html#fast-particle-mesh-scaling-lsst-desc",
    "href": "2024_10_CSI/index.html#fast-particle-mesh-scaling-lsst-desc",
    "title": "CSI Presentation 2024",
    "section": "Fast Particle-mesh scaling ",
    "text": "Fast Particle-mesh scaling \n➢  (Poqueres et al. 2021) : \\(64^3\\) mesh size, on a 1000 Mpc/h box\n➢  (Li et al. 2022) : \\(512^3\\) mesh size, using pmwd\n➢  (Lanusse et al.) : JaxPM similaire à pmwd.\n➢  FastPM : distributed but CPU-based\n\n\n\n\n\n\n\n\n\n\nInitial Conditions with a 1024 mesh\n\n\n\n\n\n\n\nInitial Conditions with a 64 mesh\n\n\n\n\n\n\n\n\nPower spectrum comparison\n\n\n\n\n\n\n\n\nMuti Node ( \\(\\infty\\) )\n\n\n\n\n\n\n\n\n\n\nFinal field with a 1024 mesh\n\n\n\n\n\n\n\nFinal field with a 64 mesh\n\n\n\n\n\n\nMy contributions (in the context of ISSC and LSST DESC)\n\n\nWe need a fast, differentiable and Scalable Particle-Mesh simulation that can run on multiple GPUs.\nMulti-GPU Particle mesh requires :\n\nDistributed FFT operations\n\nInterpolation scheme to handle boundary conditions in a distributed manner\n\n\n\n\n\n\n\nIl existe déja quelque implementation de simulation de particules-mesh qui sont capable de simuler des boites de 1000 Mpc/h avec une résolution de 64^3 ou 512^3.\nLes deux exemples faites dans un papier de poqueres et l’autre par le package pmwd basé sur JAX.\nLe papier de poqueres à utilisé une résolution de 64^3 pour simuler une boite de 1000 Mpc/h, et le package pmwd peut aller jusq’à une résolution de 512^3.\nJaxPM est un package similaire à pmwd, qui est capable de simuler des résolutions similaires à celles de pmwd et toujours sur un seul GPU.\nNEXT\nUn exemple de deux simulation faites sur une boite de 1 Gpc/h avec une résolution de 64^3 et 1024^3. On peut voir que la résolution plus élevée permet de capturer plus de détails dans le champ de densité.\nNEXT\nSi on visualise le spectre de puissance de ces deux champs, on peut voir que la résolution plus basse sous-estime la densité de matière et les interactions à petite échelle. En fonction du type du but cosmologique , ce qui peut dire que ce type d’inference ne sera pas capable de mettre une contrainte mielleure voir meme pire par rapport à une méthode basé sur des fonction de correlation à deux points ou spectre de puissance.\nLa taille de mémoire etant un facteur limitant, il est important de pouvoir mettre à l’échelle ces simulations sur plusieurs GPU et nœuds de calcul haute performance.\nLe passage à une simulation multi-GPU nécessite des opérations de FFT distribuées, des interpolations distribuées et des conditions aux limites.\nNEXT\nNotre but serait de pouvoir faires simulation qui peuvent être distribuées sur plusieur GPU et nœuds de calcul haute performance. Tout en restant diffirentiable et rapide (quelque secondes pour chaque simulation).\npas été fait"
  },
  {
    "objectID": "2024_10_CSI/index.html#distributed-fast-fourier-transform",
    "href": "2024_10_CSI/index.html#distributed-fast-fourier-transform",
    "title": "CSI Presentation 2024",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n➢  only operation that requires communication is the FFT\n\nJaxdecomp\n\n\nimport jax\nimport jax.numpy as jnp\n\nfield = jax.random.normal(jax.random.PRNGKey(0), (1024, 1024, 1024))\nk_field = jnp.fft.fftn(field)\n\n\n\n\nL’opération qui nécessite le plus de communication dans une simulation distribuée est la transformée de Fourier rapide (FFT).\nL’utilisation sur un seul GPU est triviale"
  },
  {
    "objectID": "2024_10_CSI/index.html#distributed-fast-fourier-transform-1",
    "href": "2024_10_CSI/index.html#distributed-fast-fourier-transform-1",
    "title": "CSI Presentation 2024",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n➢  only operation that requires communication is the FFT\n\nJaxdecomp\n\n DifferentiableUniverseInitiative/jaxDecomp\n\n\n\n\nimport jax\nimport jax.numpy as jnp\nimport jaxdecomp\n\ndevices = mesh_utils.create_device_mesh((2, 2))\nmesh = jax.sharding.Mesh(devices, axis_names=('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n\n# Create gaussian field distributed across the mesh\nfield = jax.make_array_from_single_device_arrays(\n        shape=mesh_shape,\n        sharding=sharding,\n        arrays=[jax.random.normal(jax.random.PRNGKey(rank), (512, 512, 1024))])\n\nk_field = jaxdecomp.fft.pfft3d(field)\n\n\n\n\n\n\n\n\n\n\nJaxDecomp features\n\n\n➢  jaxDecomp supports 2D and 1D decompositions\n➢  Works for multi-node FFTs\n➢  is differentiable\n➢  The package is also provided as a standalone library\n\n\n\n\n\n\n\n\n\n\n\n\n\navec JaxDecomp c’est aussi trivial faire des FFT distribuées sur plusieurs GPU et nœuds de calcul.\nL’utilisation est aussi trivial que faire une FFT sur un seul GPU, il suffit de décrire la grille de distribution et la bibliothèque s’occupe du reste.\nNEXT\nça supporte les décompositions 2D et 1D, fonctionne pour les FFT multi-nœuds, tout en restant différentiable.\nNEXT\nCôté implémentation, j’effectue à une série de FFT locales sur chaque GPU, en travaillant à chaque fois sur un axe non distribué. Ensuite, j’effectue une transposition de la grille de distribution pour redistribuer les données et traiter l’axe suivant. Cela permet de répartir la charge de calcul efficacement entre plusieurs GPU tout en s’assurant que chaque transformation de Fourier est correctement alignée avec l’axe de calcul."
  },
  {
    "objectID": "2024_10_CSI/index.html#scaling-of-distributed-fft-operations",
    "href": "2024_10_CSI/index.html#scaling-of-distributed-fft-operations",
    "title": "CSI Presentation 2024",
    "section": "Scaling of Distributed FFT operations",
    "text": "Scaling of Distributed FFT operations\n\n\nDans cette figure, on peut voir les performances de la bibliothèque JaxDecomp pour les FFT distribuées sur plusieurs GPU.\nOn est capable des faires des FFTs sur des grille de 4096^3 en quelque centaine de millisecondes (ce qui est très rapide).\nEt on voit deja qu’on peut aller jusqu’à une grille de 4096^3 en utilisant 64 GPU. En comparaison, Les simulation actuelles avec un seul GPU ne dépassent pas 1024^3."
  },
  {
    "objectID": "2024_10_CSI/index.html#interpolation-function-for-particle-mesh-simulations-cloud-in-cell",
    "href": "2024_10_CSI/index.html#interpolation-function-for-particle-mesh-simulations-cloud-in-cell",
    "title": "CSI Presentation 2024",
    "section": "Interpolation function for Particle-Mesh simulations (Cloud-in-Cell)",
    "text": "Interpolation function for Particle-Mesh simulations (Cloud-in-Cell)\n\n\n\nCIC Paint Function:\n\\[\n\\begin{array}{l c r}\ng({\\bf j})=\\sum_{i=1}^{N}m_{i}\\times\\prod_{d=1}^{D}\\left(1-\\left|p_{i}^{d}-j_{d}\\right|\\right)\n\\end{array}\n\\]\n\n\nForces Functions:\n\\[\n\\nabla^{2}\\phi = -4\\pi G\\kappa\\rho\n\\]\n\\[\nf(\\vec{k}) = \\dot{\\vec{k}}k^{-2}\\rho(\\vec{k})\n\\]\n\n\nCIC Read Function:\n\\[\nv_{i} = \\sum_{{\\bf j}}g({\\bf j})\\times\\prod_{d=1}^{D}\\left(1-|p_{i}^{d}-j_{d}|\\right)\n\\]\n\n\n\n\n➢  Periodic boundary conditions are applied to each slice of the simulation\n➢  Particles cannot escape the simulation domain\n\n\n\n\n\n\n\n\nParticles\n\n\n\n\n\n\n\nCIC Kernel\n\n\n\n\n\nMesh\n\n\n\n\n\n\n\n\n\n\nGradients\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplacement\n\n\n\n\n\n\n\n\nPour les simulations de particules-mesh, on utilise une fonction d’interpolation pour peindre les particules sur la grille.\nLa methode s’appelle Cloud-in-Cell (CIC), qui est une méthode simple et rapide pour interpoler les particules sur une grille.\nOn part d’une liste de particules représentées par leurs coordonnées et leur masse.\nNEXT\nEnsuite on ‘paint’ chaque particules sur les cellules adjacentes.\nChaque particule contribue à la densité de masse de la cellule en fonction de sa distance à la cellule.\nPar exemple dans cette exemple, la plus part des particules sont proche de la cellule (1 , 0) donc elle aura le plus de densité.\nNEXT\nOn calcule les forces en utilisant la densité de masse interpolée sur la grille. On obtient donc des gradients.\nNEXT\nEnsuite on interpole les forces sur les particules pour les mettre à jour.\nChaque gradient contribue au deplacement d’une particule en fonction de sa distance à la cellule.\nNEXT\nCependant, cette méthode n’est pas compatible avec les simulations distribuées, car elle nécessite de lire les valeurs des cellules voisines, ce qui n’est pas possible si les cellules sont sur un autre GPU. Ce qui se passe sur un seul GPU c’est l’application de la condition periodique. Ce qui veut dire que les particules qui sort d’un sous domain reviennent de l’autre coté du meme domaine.\nCela le reflète pas la réalité physique, et peut introduire des erreurs dans les simulations.\nNEXT\nJe montre dans le slide suivant quel impact cela peut avoir sur les simulations."
  },
  {
    "objectID": "2024_10_CSI/index.html#halo-exchange-in-distributed-simulations",
    "href": "2024_10_CSI/index.html#halo-exchange-in-distributed-simulations",
    "title": "CSI Presentation 2024",
    "section": "Halo exchange in distributed simulations",
    "text": "Halo exchange in distributed simulations\n\n\n\n\n\n\n\n\n\n\n\nInitial Field\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nCIC Kernel\n\n\n\n\n\n\n\nHalo Exchangel\n\n\n\n\n\n\nfrom jaxdecomp import halo_exchange\n\nhalo_size = 128\nfield = halo_exchange(field, halo_extent=halo_size)\n\n\n\nOn commence par voir un champs de densité gaussien de , qui est distribué sur 4 GPU.\nChaque GPU a une partie du champ de densité, et peut evoluer de manière indépendante.\nAprès avoir calculer les forces avec la FFT distribuée, on obtient un champ de densité qui est distribué sur les 4 GPU.\nOn fait evoluer les particules indépendamment, mais on voit que les particules qui sont proche des bords du domaine ne sont pas correctement traitées.\nNEXT\nOn a egalement une solution pour ce problème, qui est l’échange de halo. Ou un utilisateur peut definir une taille de halo, qui est la taille des cellules qui sont échangées entre les GPU.\nNEXT\nConcretement, pendant la phase de l’interpolaction CIC, on permet les particules d’être peintes sur des cellule plus grande que les cellules locales.\nPuis on echange la partie des cellules qui sont dans le halo avec les autres GPU.\nL’échange applique les conditions periodiques sur les bords du domaine, et permet de traiter les particules qui sont proche des bords du domaine. Cela nous donne un resultat très similaire à une simulation sur un seul GPU."
  },
  {
    "objectID": "2024_10_CSI/index.html#conclusion",
    "href": "2024_10_CSI/index.html#conclusion",
    "title": "CSI Presentation 2024",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nWork already done\n\n\n➢  jaxDecomp : Distributed FFT operations and halo exchange (Released)\n➢  jaxPM : Distributed Particle-Mesh simulations (Code)\n➢  Furax : Minimization of the spectral likelihood for CMB component separation (Code)\n➢  S2FFT : Accelerated spherical harmonics transforms (Pull request)\n\n\n\n\n\nWork in progress\n\n\n➢  Furax (Continued) : Creation of a special optimiser that can handle multi-resolution and multi-patch data\n➢  jaxPM : Benchmarking and testing the scaling of the simulations\n\n\n\n\n\nFuture work\n\n\n➢   Optimized and distributed spherical harmonics transforms for CMB lensing\n➢   Distributed probabilistic programming for hierarchical Bayesian Modeling"
  },
  {
    "objectID": "2024_10_CSI/index.html#attended-conferences",
    "href": "2024_10_CSI/index.html#attended-conferences",
    "title": "CSI Presentation 2024",
    "section": "Attended Conferences",
    "text": "Attended Conferences\n\n\nFrench Conferences\n\n\n\nIAP 2023 Machine Learning-\nLSST France 2023 Lyon, France\nLSST France 2024 Marseille, France (Talk)\n[Upcomming Conference] IAP GDR CoPhy Tools 2024 Paris, France (Talk)\n\n\n\n\n\nInternational Conferences\n\n\n\nMoriond Cosmology 2023 La Thuile, Italy (Poster)\nLSST DESC 2023 Zurich, Switzerland"
  },
  {
    "objectID": "2024_10_CSI/index.html#papers",
    "href": "2024_10_CSI/index.html#papers",
    "title": "CSI Presentation 2024",
    "section": "Papers",
    "text": "Papers\n \n\n\nCurrent/Submitted papers\n\n\n\nInfrared Radiometric Image Classification and Segmentation of Cloud Structure Using ML (Sommer et al. 2024, Published)\nJaxDecomp: A Distributed Fast Fourier Transform Library (Kabalan et al. to be submitted soon)\nFurax: Optimization of the Large Scale Multiresolution Parametric Component Separation (Kabalan et al., for end of 2024)\nJaxPM: A Differentiable Particle-Mesh Simulation (Kabalan et al. in prep.)\n\n\n\n\n\n\nFuture papers\n\n\n\nSpherical Harmonics for CMB component separation (Lead author)\nDistributed Probabilistic Programming for Hierarchical Bayesian Modeling"
  },
  {
    "objectID": "2024_10_CSI/index.html#formations",
    "href": "2024_10_CSI/index.html#formations",
    "title": "CSI Presentation 2024",
    "section": "Formations",
    "text": "Formations\n\n\n\ndivers\n\n\n\nEuclid summer school (2023)\nAISSAI AstroInfo Hackathon 2023, Frejus, France\nPhysics informed neural networks with the IDRIS team (Jean-zay super computer) (2024)\n\n\n\n\n\nCours Ecole Doctorale\n\n\n\nQCD with Matteo Cacciari (2023)"
  },
  {
    "objectID": "2024_06_LSST_FR/index.html#traditional-cosmological-inference",
    "href": "2024_06_LSST_FR/index.html#traditional-cosmological-inference",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Traditional cosmological inference",
    "text": "Traditional cosmological inference\n\nBayesian inference in cosmology\n\nWe need to infer the cosmological parameters \\(\\theta\\) that generated an observartion \\(x\\)\n\n\\[p(\\theta | x ) \\propto \\underbrace{p(x | \\theta)}_{\\mathrm{likelihood}} \\ \\underbrace{p(\\theta)}_{\\mathrm{prior}}\\]\n\n\n\n\n\n\n\n\n\n\n ➢  Compute summary statistics based on the 2-point correlation function of the shear field\n\n\n ➢  Run an MCMC chain to recover the posterior distribution of the cosmological parameters, using an analytical likelihood\n\n\n\n\n\n\nLimitations\n\n\n\nSimple summary statistics assume Gaussianity\nThe need to compute an analytical likelihood"
  },
  {
    "objectID": "2024_06_LSST_FR/index.html#jax-automatic-differentiation-and-hardware-acceleration",
    "href": "2024_06_LSST_FR/index.html#jax-automatic-differentiation-and-hardware-acceleration",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport numpy as np\n\n\ndef multiply_and_add(a, b, c):\n    return np.dot(a, b) + c\n\n\na, b, c = np.random.normal(size=(3, 32, 32))\nresult = multiply_and_add(a, b, c)\n\n\nimport jax\nimport jax.numpy as jnp\n\n\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c)"
  },
  {
    "objectID": "2024_06_LSST_FR/index.html#jax-automatic-differentiation-and-hardware-acceleration-1",
    "href": "2024_06_LSST_FR/index.html#jax-automatic-differentiation-and-hardware-acceleration-1",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport jax\nimport jax.numpy as jnp\n\n\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c)"
  },
  {
    "objectID": "2024_06_LSST_FR/index.html#jax-automatic-differentiation-and-hardware-acceleration-2",
    "href": "2024_06_LSST_FR/index.html#jax-automatic-differentiation-and-hardware-acceleration-2",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "JAX : Automatic differentiation and Hardware acceleration",
    "text": "JAX : Automatic differentiation and Hardware acceleration\n\n     \nimport jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multiply_and_add(a, b, c):\n    return jnp.dot(a, b) + c\n\n\nkey = jax.random.PRNGKey(0)\na, b, c = jax.random.normal(key, (3, 32, 32))\n\nresult = multiply_and_add(a, b, c) \ngradient = jax.grad(multiply_and_add)(a, b, c)\n \n\n\nJAX : Numpy + Autograd + GPU\n\n\n\njax.grad uses automatic differentiation to compute the gradient of the function\njax.jit compiles the function to run on GPUs"
  },
  {
    "objectID": "2024_06_LSST_FR/index.html#distributed-fast-fourier-transform",
    "href": "2024_06_LSST_FR/index.html#distributed-fast-fourier-transform",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n➢  only operation that requires communication is the FFT\n\nJaxdecomp\n\n\nimport jax\nimport jax.numpy as jnp\n\nfield = jax.random.normal(jax.random.PRNGKey(0), (1024, 1024, 1024))\nk_field = jnp.fft.fftn(field)"
  },
  {
    "objectID": "2024_06_LSST_FR/index.html#distributed-fast-fourier-transform-1",
    "href": "2024_06_LSST_FR/index.html#distributed-fast-fourier-transform-1",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Distributed Fast Fourier Transform",
    "text": "Distributed Fast Fourier Transform\n➢  only operation that requires communication is the FFT\n\nJaxdecomp\n\n DifferentiableUniverseInitiative/jaxDecomp\n\n\n\nimport jax\nimport jax.numpy as jnp\nimport jaxdecomp\n\ndevices = mesh_utils.create_device_mesh((2, 2))\nmesh = jax.sharding.Mesh(devices, axis_names=('x', 'y'))\nsharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n\n# Create gaussian field distributed across the mesh\nfield = jax.make_array_from_single_device_arrays(\n    shape=mesh_shape,\n    sharding=sharding,\n    arrays=[jax.random.normal(jax.random.PRNGKey(rank), (512, 512, 1024))])\n\nk_field = jaxdecomp.fft.pfft3d(field)\n\n\n\n\nJaxDecomp features\n\n\n➢  jaxDecomp supports 2D and 1D decompositions\n➢  Works for multi-node FFTs\n➢  is differentiable\n➢  The package is also provided as a standalone library"
  },
  {
    "objectID": "2024_06_LSST_FR/index.html#halo-exchange-in-distributed-simulations",
    "href": "2024_06_LSST_FR/index.html#halo-exchange-in-distributed-simulations",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Halo exchange in distributed simulations",
    "text": "Halo exchange in distributed simulations\n\n\n\n\n\n\n\n\n\n\n\nInitial Field\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\nfrom jaxdecomp import halo_exchange\n\nhalo_size = 128\nfield = halo_exchange(field, halo_extent=halo_size)"
  },
  {
    "objectID": "2024_06_LSST_FR/index.html#conclusion",
    "href": "2024_06_LSST_FR/index.html#conclusion",
    "title": "Differentiable and distributed Particle-Mesh n-body simulations",
    "section": "Conclusion ",
    "text": "Conclusion \n   \n\n\nDistruibuted Particle-Mesh simulations for cosmological inference\n\n\n\nA shift from analytical likelihoods to full field inference\n\nThe need for fast differentiable simulators\nParticle-Mesh as simulators for full field inference\nDistributed fourrier transforms that work on multi-node HPC using jaxDecomp\nHighly scalable LPT simulations using JaxPM\n\nStill subject to some challenges\n\nSome issues with the ODE solving step\nOnly Euler gives decent results."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#goals-for-this-presentation",
    "href": "2024_11_CoPhy/index.html#goals-for-this-presentation",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Goals for This Presentation",
    "text": "Goals for This Presentation\n\n\n\nUnderstand the Basics of Parallelism: Learn how parallelism works and its importance for high-performance computing.  \nKnow When (and When Not) to Parallelize: Discover when it is beneficial to parallelize your code and when it’s better to avoid it.  \nWhen to Use (and Avoid) Parallelism: Discover the benefits and limitations.  \nScale Code Using JAX: Explore techniques to scale your computations using JAX for large-scale tasks.  \nHands-On Tutorials: Apply the concepts discussed with interactive code examples and tutorials.\n\n\n\n\n\nUnderstand the Basics of Parallelism:\n\n“First, we’ll start with the fundamentals of parallelism — understanding how parallel computing works.”\n“We’ll look at different types of parallelism, such as task parallelism and data parallelism, and see how they are applied in computational problems.”\n“It’s important to get a solid understanding of these basic concepts before we dive into how to scale them effectively in cosmology.”\n\nKnow When (and When Not) to Parallelize:\n\n“Next, we’ll cover a critical aspect: knowing when to use parallelism and, just as importantly, when not to use it.”\n“While parallelism can offer huge speedups, not all problems are suitable for parallelization. In fact, sometimes parallelism can make things slower due to overhead. I’ll show you how to identify the right cases for parallelism and how to avoid it when it’s not the best approach.”\n\nScale Code Using JAX:\n\n“Then, we’ll explore how to scale your code using JAX.”\n“JAX makes it easy to scale computations by automatically taking advantage of GPUs. We’ll also look at how your code might change depending on the parallelism strategy you choose. JAX allows for flexible parallelization strategies, so you can tailor it to your specific needs.”\n\nHands-On Tutorials:\n\n“Finally, we’ll wrap up with hands-on tutorials. We’ll work through interactive code examples so you can see firsthand how these concepts are implemented in practice.”\n“These examples will give you the opportunity to apply what we’ve discussed and see the power of parallel computing in action.”\n\nTransition:\n\n“Now that we know what we aim to cover, let’s dive into the basics of parallelism and lay the groundwork for the rest of the talk.”"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#how-gpus-work",
    "href": "2024_11_CoPhy/index.html#how-gpus-work",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "How GPUs Work",
    "text": "How GPUs Work\n\n\n\nMassive Thread Count\n\nGPUs are designed with thousands of threads.\nEach core can handle many data elements simultaneously.\n\n\n\nThe main bottleneck is memory throughput\n\nComputation is often only a fraction of total processing time.\n\n\n\nOptimizing Throughput with Multiple GPUs:\n\nUsing multiple GPUs increases overall data throughput, enhancing performance and reducing idle time.\n\n\n\n\n\n\nGPU threads\n\n\n\n\n\n\n\nSingle GPU throughput\n\n\n\n\n\n\n\nSaturated GPU\n\n\n\n\n\n\n\nMultiple GPUs throughput\n\n\n\n\n\n\n\nIntroduction to GPUs:\n\n“GPUs are designed for high-throughput, parallel computation. Unlike CPUs, which are optimized for single-threaded tasks, GPUs have thousands of threads, allowing them to process large datasets in parallel. This makes them ideal for tasks like simulations and large-scale data processing.”\n\nMassive Thread Count:\n\n“Each core in a GPU can do certain steps of the compuation.”\n\n\nNEXT\n\nMemory Throughput Bottleneck:\n\n“However, even with so many threads, the true bottleneck in GPU performance is often memory throughput. If the GPU doesn’t receive data quickly enough, even the large number of cores can’t operate at full capacity. In other words, GPUs can process a lot of data in parallel, but only if they’re fed enough data to keep all those cores busy.”\n“As you can see in the illustration, when the GPU becomes saturated, it can no longer process new data until the current batch is fully processed. This results in idle time, where the GPU isn’t doing any useful computation, even though it’s fully capable of processing more data.”\n\n\nNEXT\n\nOptimizing Throughput with Multiple GPUs:\n\n“This leads us to the concept of optimizing throughput with multiple GPUs.”\n“In many workloads, especially in cosmology and other large-scale computations, the computation is only a small portion of the total processing time. A significant amount of time is spent waiting for data, transferring it, or processing smaller chunks of data that can’t be fully parallelized.”\n“By adding more GPUs, we can increase the total data throughput, reduce idle time, and ultimately improve overall performance. With multiple GPUs working in parallel, the data is distributed more effectively, reducing bottlenecks and allowing us to handle larger and more complex datasets.”"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#types-of-data-parallelism",
    "href": "2024_11_CoPhy/index.html#types-of-data-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Types of Data parallelism",
    "text": "Types of Data parallelism\n\n\nData Parallelism\n\nSimple Parallelism: Each device processes a different subset of data independently.\n\n\n\nData Parallelism with Collective Communication:\n\nDevices process data in parallel but periodically share results (e.g., for gradient averaging in training).\n\n\n\n\nTask Parallelism\n\nEach device handles a different part of the computation.\nThe computation itself is divided between devices.\nIs generally more complex than data parallelism.\n\n\n\n\n\n\n\n\nSimple Data Parallelism\n\n\n\n\n\n\n\nData Parallelism with Communication\n\n\n\n\n\n\n\n\nTask Parallelism\n\n\n\n\n\nIn the computer science world, there are many types of parallelism, they untimately fall into two categories: data parallelism and task parallelism.\n\nData Parallelism (Simple Parallelism):\n\n“In data parallelism, each device processes a different subset of the data independently. This approach is ideal when:\n\nThe data can be evenly split across devices.\nEach subset can be processed without interaction with other devices.”\n\n“In cosmology, simple data parallelism could be used, for example, in parameter estimation tasks where each GPU computes the likelihood for different portions of the dataset independently.”\n\n\nNEXT\n\nData Parallelism with Collective Communication:\n\n“In data parallelism with collective communication, devices process data in parallel but need to share intermediate results periodically.”\n“For example, in distributed training of machine learning models, GPUs periodically exchange gradient information to keep models in sync.”\n“This approach is more complex because it requires synchronization between devices, adding communication overhead.”\n“Challenges arise in the form of bottlenecks during the communication phase, which can limit scalability and affect overall efficiency.”\n\n\nNEXT\n\nTask Parallelism:\n\n“In task parallelism, each device handles a unique part of the computation.”\n“This is useful when the computation can be split into discrete tasks that contribute independently to the final result.\n“Unlike data parallelism, task parallelism typically requires more complex coordination and may involve custom communication protocols between tasks.”\nThis is used extensively in large language models"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#when-should-you-use-parallelism",
    "href": "2024_11_CoPhy/index.html#when-should-you-use-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "When Should You Use Parallelism?",
    "text": "When Should You Use Parallelism?\n\nSimple cases\n\nData Parallelism (Simple) ✅\n\nIf your pipeline resembles simple data parallelism, then parallelism is a good idea.\n\nData Parallelism with Simple Collectives ✅\n\nSimple collectives (e.g., gradient averaging) can be easily expressed in JAX, allowing devices to share intermediate results.\n\n\n\nComplex cases\n\nNon-splittable Input (e.g., N-body Simulation Fields) ⚠️\n\nWhen the input is not easily batchable, like a field in an N-body simulation.\n\n\n\n\n\nTask Parallelism ⚠️\n\nUseful for long sequential cosmological pipelines where each device handles a unique task in the sequence.\nMore common in training complex models (e.g., LLMs like Gemini or ChatGPT).\n\n\n\n\n\nData Parallelism (Simple):\n\n“If your pipeline follows a simple data parallelism structure, where the dataset can be split across devices and each device processes a portion independently, parallelism is a good idea.”\nThis is usually a no-brainer, as it’s a straightforward way to speed up computations with minimal overhead.\n\n\nNEXT\n\nData Parallelism with Simple Collectives: ✅\n\n“When you need to share intermediate results (like gradient averaging) between devices, parallelism is still a great fit.”\nI most cases, this is a good idea as it allows devices to work independently but still communicate when needed.\n\n\nNEXT\n\nNon-splittable Input (e.g., N-body Simulation Fields): ⚠️\n\n“When the input data can’t be easily divided into independent chunks, parallelism can become challenging. For instance, in an N-body simulation, where the simulation field might be a continuous dataset that doesn’t naturally split into batches, you’ll need a more advanced approach.”\n“In such cases, we have to use data parallelism with more complex collectives, which can introduce overhead and reduce efficiency.”\n\n\nNEXT\n\nTask Parallelism: ⚠️\n\n“Task parallelism is often used when a computational pipeline is long and sequential, where each task performed by a device is distinct.”\n“This is common in large language model (LLM) training, like models such as Gemini or ChatGPT, and models that have billions of parameters.”\n“Task parallelism often requires significant restructuring of your pipeline and may involve custom communication protocols between tasks. It’s more complex than data parallelism, and it’s not always the best option unless your task demands it.”"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#when-not-to-use-parallelism",
    "href": "2024_11_CoPhy/index.html#when-not-to-use-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "When NOT to Use Parallelism",
    "text": "When NOT to Use Parallelism\n\n\nTo Keep in Mind\n\nData Fits on a Single GPU\n\n\n\nNeed for Complex Collectives\n\nAdditional GPUs can add complexity and may not yield enough performance improvement.\n\n\n\n\n\nTask Parallel Model\n\nChanging the pipeline or adapting to new devices often requires significant rewrites.\n\n\n\n\n\n\n\nNot fully used GPU\n\n\n\n\n\n\nConsider Scaling to multiple GPUs if:\n\n\n\nYou have a single-GPU prototype that’s working but needs significant runtime reduction.\nHas a significant impact on your results.\n\nUsing multiple GPUs can significantly decrease execution time.\nOR You have non-splittable input (e.g., fields in a cosmological simulation) that is crucial for your results.\n\n\n\n\n\n\n\nData Fits on a Single GPU: ❌\n\n“If your data fits comfortably on a single GPU and the computation can be done efficiently without needing to distribute the workload, then adding more GPUs won’t offer much benefit.”\n“In such cases, you’re better off sticking with a single GPU, as parallelism would just introduce unnecessary complexity.”\n\n\nNEXT\n\nNeed for Complex Collectives: ❌\n\n“When the task requires complex collective operations, adding more GPUs can actually increase the overhead and communication complexity.”\n“For example, if your workload demands frequent data synchronization between GPUs (such as gradient averaging), the added communication costs may outweigh the benefits of parallelism.”\n\n\nNEXT\n\nTask Parallel Model: ❌\n\n“When your computational pipeline is structured for task parallelism, it may require substantial changes to accommodate multiple GPUs.”\n“Adapting the pipeline to new devices can be a major task, often requiring significant rewrites of the codebase to handle the distributed nature of parallelism.”\n\n\nNEXT\n\nConsider Scaling to Multiple GPUs If:\n\n“You have a single-GPU prototype that’s working, but it requires a significant runtime reduction.”\n“If reducing execution time has a huge impact on your results, scaling to multiple GPUs can provide the needed speedup.”\n“Or, if you have non-splittable input (like fields in a cosmological simulation) that are critical to your results, adding GPUs could be essential for improving performance while handling larger, more complex datasets.”"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#how-to-measure-scaling-for-parallel-codes",
    "href": "2024_11_CoPhy/index.html#how-to-measure-scaling-for-parallel-codes",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "How to Measure Scaling for Parallel Codes",
    "text": "How to Measure Scaling for Parallel Codes\n\n\n\n\nStrong Scaling\n\nIncreasing the number of GPUs to reduce runtime for a fixed data size.\n\n\n\nAssesses performance as more GPUs are added to a fixed dataset. Danger Zone⚠️: Indicates the distributed code is not scaling efficiently.\n\n\n\n\nWeak Scaling\n\nIncreasing data size with a fixed number of GPUs.\n\n\n\nTests how the code handles increasing data sizes with a fixed number of GPUs. Danger Zone⚠️: Suggests underlying scaling issues with the code itself.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrong Scaling:\n\n“Strong scaling tests how your code performs when you add more GPUs to a fixed dataset, aiming to reduce runtime. If adding more GPUs doesn’t significantly decrease the runtime, it’s a sign that your code is not scaling efficiently.”\n“This is a critical metric for evaluating how well your code can handle the increasing parallelism and whether it benefits from more GPUs.”\n“Danger Zone ⚠️: If you see little to no reduction in runtime as more GPUs are added, this indicates that your parallel code is facing scalability issues.”\n\n\nNEXT\n\nWeak Scaling:\n\n“Weak scaling, on the other hand, tests how your code handles an increasing dataset with a fixed number of GPUs.”\n“This is useful for understanding how the code can handle larger data sizes and whether it can maintain performance as the workload increases.”\n“Danger Zone ⚠️: If the performance doesn’t improve with an increase in data size, this suggests there are underlying scaling issues in the code itself.”"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#environmental-impact-of-high-performance-computing",
    "href": "2024_11_CoPhy/index.html#environmental-impact-of-high-performance-computing",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Environmental Impact of High-Performance Computing",
    "text": "Environmental Impact of High-Performance Computing\n\n\n\n\n\nPerlmutter Supercomputer (NERSC)\n\nLocation: NERSC, Berkeley Lab, California, USA\nCompute Power: ~170 PFlops\nGPUs: 7,208 NVIDIA A100 GPUs\nPower Draw: ~ 3-4 MW\n\n\nJean Zay Supercomputer (IDRIS)\n\nLocation: IDRIS, France\nCompute Power: ~126 PFlops (FP64), 2.88 EFlops (BF/FP16)\nGPUs: 3,704 GPUs, including V100, A100, and H100\nPower Draw: ~1.4 MW on average (as of September, without full H100 usage), leveraging France’s renewable energy grid.\n\n\n\n\n\nPerlmutter Supercomputer\n\n\n\n\n\nJean Zay Supercomputer\n\n\n\n\n\n\n\n\n\nEnvironmental Benefits of Efficient Parallel Computing\n\n\n\n\nHigher throughput moves computations closer to peak FLOPS. \nOperating near peak FLOPS ensures more effective use of computational resources. \nMore computations are achieved per unit of energy, improving energy efficiency.\n\n\n\n\n\n\n\n\n\n\nPerlmutter Supercomputer (NERSC):\n\n“Perlmutter, located at NERSC in California, USA, is one of the most powerful supercomputers in the world, capable of 170 petaflops of compute power.” NEXT\n\nJean Zay Supercomputer (IDRIS):\n\n“The Jean Zay Supercomputer, located at IDRIS in France, is another impressive example, with a compute power of around 126 petaflops\n\nWhat are FLOPs?:\n\n“FLOPs stand for Floating Point Operations per Second, which is a measure of computational performance. Essentially, it refers to how many floating-point calculations a system can perform in one second.”\n\n\nHow does this impact the environment?\nNEXT\n\nEnvironmental Benefits of Efficient Parallel Computing:\n\n“Efficient parallel computing, when operated near peak FLOPS, maximizes the number of computations per unit of energy. This means that when we optimize the throughput of these supercomputers, we’re not just increasing speed, but also improving energy efficiency.”\n“By ensuring that we use computational resources more effectively, we can reduce the environmental footprint of these massive computational infrastructures.”\n\n\n\n\nCredit: Laurent Leger from IDRIS and Nestor Demeure from NERSC"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#why-jax-for-distributed-computing",
    "href": "2024_11_CoPhy/index.html#why-jax-for-distributed-computing",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Why JAX for Distributed Computing?",
    "text": "Why JAX for Distributed Computing?\n\nDistributed Computing Isn’t New:\n\nTools like MPI and OpenMP are used extensively.\nML frameworks like TensorFlow and PyTorch offer distributed training.\nDiffEqFlux.jl Horovod and Ray\n\n\n\n\nFamiliar and Accessible API:\n\nJAX offers a NumPy-like API that is both accessible and intuitive.\nPython users can leverage parallelism without needing in-depth knowledge of low-level parallel frameworks like MPI.\n\n\n\n\n\n\nKey Points\n\n\n\nPythonic Scalability: JAX allows you to write scalable, pythonic code that is compiled by XLA for performance.\nAutomatic Differentiation: JAX offers a trivial way to write diffrentiable distributed code.\nSame code runs on anything from a laptop to multi node supercomputer.\n\n\n\n\n\nWhy am i laser focused on JAX\n\nTraditional Distributed Computing: MPI and OpenMP have been essential tools for achieving high-performance distributed computing in fields like cosmology. These frameworks provide fine control but often require specific parallel programming expertise.\nAccessibility and Familiarity: JAX’s familiar, high-level syntax lowers the barrier to entry, bringing distributed computing within reach of Python users without the need to manage intricate MPI or OpenMP settings.\nJAX’s Unique Advantages: Through XLA compilation, JAX not only scales code efficiently but also integrates differentiability, crucial for machine learning and simulations that require backpropagation. This blend of performance and flexibility sets JAX apart for scientific and AI applications.\n\nTalking Points on Alternative Framework Limitations\n\nOpen MPI: Primarily optimized for CPU-based parallelism, making it less effective on GPUs where scientific workloads in JAX often run. This can limit its efficiency for cosmology applications that leverage GPU acceleration.\nML Frameworks (PyTorch, TensorFlow): While powerful for machine learning, these frameworks are ML-centric and don’t natively support the arbitrary scientific functions often needed in cosmology. Customizing these frameworks for scientific use cases requires significant additional effort.\nDiffEqFlux.jl (Julia): While Julia’s ecosystem is growing, it’s still limited compared to Python, particularly for scientific computing and distributed applications. This smaller ecosystem can make it harder to find compatible tools and libraries for complex cosmological simulations.\nHorovod and Ray: Neither is natively differentiable, which means they rely on external frameworks (e.g., PyTorch or TensorFlow) for differentiation. This lack of built-in differentiability adds overhead and complexity for workflows that require gradient-based optimization, a key feature that JAX integrates seamlessly.\n\nFor questions\nPyTorch Distributed\n\nComplex Setup: Requires more effort to configure distributed training, especially outside deep learning.\nPerformance Overhead: Lacks JAX’s XLA compilation, which can lead to inefficiencies in scientific applications.\nLimited Scientific Libraries: PyTorch’s ecosystem is growing, but it still lacks the depth of JAX for scientific and physics-based computing.\n\nTensorFlow Distributed\n\nComplex and Verbose: Distributed setup with tf.distribute.Strategy is often more cumbersome and requires multiple API layers.\nLess Flexibility with Gradients: Limited flexibility in complex gradient computations compared to JAX’s functional approach.\nUnder-Optimized for Scientific Workflows: XLA support is not as performant in scientific HPC compared to JAX.\n\nRay with Auto-Differentiation\n\nNot Natively Differentiable: Ray relies on external libraries (like PyTorch and TensorFlow) for differentiation, adding communication and synchronization overhead.\nFocus on General Purpose Computing: Lacks specific optimizations for HPC environments and scientific computing.\nLimited Low-Level Hardware Control: Ray abstracts device management, reducing optimization potential in specialized HPC setups.\n\nDiffEqFlux.jl (Julia)\n\nLimited Ecosystem: Julia’s ecosystem is smaller and less mature, especially for scientific computing.\nDeveloping Distributed Support: Distributed computing in Julia is still evolving and less robust than in Python.\nLearning Curve: Julia has a steeper learning curve for Python-based teams, and integration with Python infrastructure can be difficult.\n\nMesh TensorFlow\n\nSpecialized for Transformers: Primarily designed for partitioning large transformer models, limiting flexibility in other scientific applications.\nComplex Configuration: Mesh configuration is often challenging and may be a barrier for scientific users.\nTied to TensorFlow: Mesh TensorFlow’s dependency on TensorFlow makes it less intuitive for scientific computing compared to JAX’s NumPy-like API.\n\nHorovod (Multi-Framework)\n\nOptimized for Data Parallelism in ML: Primarily suited for data parallelism in ML, not as adaptable for complex scientific workflows.\nExternal Library Dependence: Requires frameworks like PyTorch or TensorFlow for auto-differentiation, adding performance overhead.\nLimited Scientific Integration: Does not integrate well with libraries focused on physical simulations, whereas JAX has a growing ecosystem for such applications.\n\nKey Advantages of JAX\n\nUnified, Pythonic API: JAX’s intuitive API combines ease of use with the power of distributed computing.\nXLA Compilation for Efficiency: Optimized for performance across devices, making it highly suitable for HPC environments.\nNative Differentiability: Differentiability is built-in and seamlessly integrated with distributed workflows, providing a smooth experience for scientific applications."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#expressing-parallelism-in-jax-simple-parallelism",
    "href": "2024_11_CoPhy/index.html#expressing-parallelism-in-jax-simple-parallelism",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Expressing Parallelism in JAX (Simple parallelism)",
    "text": "Expressing Parallelism in JAX (Simple parallelism)\nExample of computing a gaussian from data Points\nimport jax\nimport jax.numpy as jnp\nfrom jax.debug import visualize_array_sharding\n\ndef gaussian(x, mean, variance):\n  coefficient = 1.0 / jnp.sqrt(2 * jnp.pi * variance)\n  exponent = -((x - mean) ** 2) / (2 * variance)\n  return coefficient * jnp.exp(exponent)\n\nmean = 0.0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n\n  GPU 0  \n         \n\n\n  GPU 0  \n         \n\n\n\nObjective: Compute the Gaussian distribution for a range of points using data parallelism.\nThe function gaussian computes the value for each point in the range independently, which is perfect for parallelism.\nNEXT\nWe can use a very convinient tool from JAX to see where the data is stored on the GPU. This is called visualize_array_sharding.\nIn here we see that the data is stored on a the first and default GPU. which is to be expected since we have not specified any parallelism strategy."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#expressing-parallelism-in-jax-simple-parallelism-1",
    "href": "2024_11_CoPhy/index.html#expressing-parallelism-in-jax-simple-parallelism-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Expressing Parallelism in JAX (Simple parallelism)",
    "text": "Expressing Parallelism in JAX (Simple parallelism)\nExample of computing a gaussian from data Points\nassert jax.device_count() == 8\n\nfrom jax.sharding import PartitionSpec as P, NamedSharding\n\ndef gaussian(x, mean, variance):\n  coefficient = 1.0 / jnp.sqrt(2 * jnp.pi * variance)\n  exponent = -((x - mean) ** 2) / (2 * variance)\n  return coefficient * jnp.exp(exponent)\n\nmesh = jax.make_mesh((8,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\n\nmean = 0.0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding)\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n  GPU 0    GPU 1    GPU 2    GPU 3    GPU 4    GPU 5    GPU 6    GPU 7  \n                                                                        \n\n\n  GPU 0    GPU 1    GPU 2    GPU 3    GPU 4    GPU 5    GPU 6    GPU 7  \n                                                                        \n\n\nNow by using the sharding module from jax we can set up a strategy.\nTwo new JAX objects to learn:\n\nMesh: Represents the collection of devices (GPUs) available for computation. In this case, we have 8 GPUs in the mesh.\nSharding: Specifies how data is distributed across the mesh. In this case, we use NamedSharding to distribute the data across the mesh.\n\nYou might think that the sharding is redundant but we will see later how it can be used to express more than one dimension of parallelism.\nNEXT\nBy using jax.device_put we can put the data on the GPU. This is a very convinient way to move data to the GPU.\nNotice that the output of the function has the same sharding as the input data.\nNEXT\nMost importantly, I did not touch the original function, I let JAX do all the work"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#expressing-parallelism-in-jax-using-collectives",
    "href": "2024_11_CoPhy/index.html#expressing-parallelism-in-jax-using-collectives",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Expressing Parallelism in JAX (Using collectives)",
    "text": "Expressing Parallelism in JAX (Using collectives)\nExample of SGD with Gradient averaging (from Jean-Eric’s tutorial)\n\n\n@jax.jit  \ndef gradient_descent_step(p, xi, yi, lr=0.1):\n  gradients = jax.grad(loss_fun)(p, xi, yi)\n  return p - lr * gradients\n\ndef minimzer(loss_fun, x_data, y_data, par_init, method, verbose=True):\n  ...\n# Example usage\npar_mini_GD = minimzer(\n  loss_fun, \n  x_data=xin, \n  y_data=yin, \n  par_init=jnp.array([0., 0.5]), \n  method=partial(gradient_descent_step, lr=0.5), \n  verbose=True\n)\n\nParallel Execution: - The gradient descent step function is executed in parallel across multiple devices. Each device computes the gradients independently based on its subset of the data. This is achieved by using the @partial(shard_map, mesh=mesh, in_specs=P('x'), out_spec=P('x')) decorator, which ensures that the function is applied across devices with proper mapping for parallel execution.\nCompute Gradients per Device: - For each device, the gradients with respect to the loss function are computed using jax.grad(loss_fun)(p, xi, yi). This means each device will compute the gradients using only its local data, and the gradients will be different depending on the device’s data partition.\nCollective Communication: - Once the gradients are computed on each device, jax.lax.pmean() is used to average the gradients across all devices. This is the collective communication step, ensuring that all devices have synchronized, averaged gradients before any updates are made to the model parameters. This ensures the consistency of gradient updates across devices.\nSharding the Data: - The data (xin, yin) is distributed (or “sharded”) across the available devices using jax.device_put(). Each device gets a portion of the input data. This enables parallel execution by splitting the data into smaller chunks that can be processed in parallel, with each device working on its own data subset."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#expressing-parallelism-in-jax-using-collectives-1",
    "href": "2024_11_CoPhy/index.html#expressing-parallelism-in-jax-using-collectives-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Expressing Parallelism in JAX (Using collectives)",
    "text": "Expressing Parallelism in JAX (Using collectives)\nExample of SGD with Gradient averaging (from Jean-Eric’s tutorial)\nfrom jax.experimental.shard_map import shard_map\n\n@jax.jit \n@partial(shard_map, mesh=mesh , in_specs=P('x'), out_spec=P('x'))\ndef gradient_descent_step(p, xi, yi, lr=0.1):\n      per_device_gradients = jax.grad(loss_fun)(p, xi, yi)\n      avg_gradients = jax.lax.pmean(per_device_gradients, axis_name='x')\n      return p - lr * avg_gradients\n\ndef minimzer(loss_fun, x_data, y_data, par_init, method, verbose=True):\n     ...\n  # Example usage\nxin = jax.device_put(xin, sharding)\nyin = jax.device_put(yin, sharding)\npar_mini_GD = minimzer(\n        loss_fun, \n        x_data=xin, \n        y_data=yin, \n        par_init=jnp.array([0., 0.5]), \n        method=partial(gradient_descent_step, lr=0.5), \n        verbose=True\n    )\n\nParallel Execution: - The gradient descent step function is executed in parallel across multiple devices. Each device computes the gradients independently based on its subset of the data. This is achieved by using the @partial(shard_map, mesh=mesh, in_specs=P('x'), out_spec=P('x')) decorator, which ensures that the function is applied across devices with proper mapping for parallel execution.\nCompute Gradients per Device: - For each device, the gradients with respect to the loss function are computed using jax.grad(loss_fun)(p, xi, yi). This means each device will compute the gradients using only its local data, and the gradients will be different depending on the device’s data partition.\nCollective Communication: - Once the gradients are computed on each device, jax.lax.pmean() is used to average the gradients across all devices. This is the collective communication step, ensuring that all devices have synchronized, averaged gradients before any updates are made to the model parameters. This ensures the consistency of gradient updates across devices.\nSharding the Data: - The data (xin, yin) is distributed (or “sharded”) across the available devices using jax.device_put(). Each device gets a portion of the input data. This enables parallel execution by splitting the data into smaller chunks that can be processed in parallel, with each device working on its own data subset."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#jax-collective-operations-for-parallel-computing",
    "href": "2024_11_CoPhy/index.html#jax-collective-operations-for-parallel-computing",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "JAX Collective Operations for Parallel Computing",
    "text": "JAX Collective Operations for Parallel Computing\nOverview of JAX Collectives in jax.lax.p* Functions\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlax.pmean\nComputes the mean of arrays across devices. Useful for averaging gradients in distributed training.\n\n\nlax.ppermute\nPermutes data across devices in a specified order. Very useful in cosmological simulations.\n\n\nlax.all_to_all\nExchanges data between devices in a controlled manner. Useful for custom data exchange patterns in distributed computing.\n\n\nlax.pmax / lax.pmin\nComputes the element-wise maximum/minimum across devices. Often used in situations where you want to find the maximum or minimum of a distributed dataset.\n\n\nlax.psum\nSums arrays across devices. Commonly used for aggregating gradients or other values in distributed settings.\n\n\nlax.pall\nChecks if all values across devices are True. Often used for collective boolean checks across distributed data.\n\n\n\n\nJAX offers a range of collective operations through the jax.lax.p* functions, enabling efficient parallelism across multiple devices in distributed computing tasks. Here’s a breakdown of the most commonly used functions:\n\nGradient Aggregation: In distributed training, pmean is commonly used to average gradients from multiple devices, ensuring each device updates with the same gradient.\nLogical Collectives: Operators like pand, por, and pall allow distributed boolean logic operations, which can help in synchronization or conditional checks in parallel code.\nFlexible Data Distribution: ppermute allows data rearrangement across devices, making it useful in more complex parallelism setups or for rearranging distributed data for specific computations.\nData Exchange: all_to_all provides a controlled way to exchange data between devices, useful for custom data exchange patterns in distributed computing."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#a-node-vs-a-supercomputer",
    "href": "2024_11_CoPhy/index.html#a-node-vs-a-supercomputer",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "A Node vs a Supercomputer",
    "text": "A Node vs a Supercomputer\n\n\nDifferences in Scale\n\n\nSingle GPU:\n\nMaximum memory: 80 GB\n\n\n\n\n\nSingle Node (Octocore):\n\nMaximum memory: 640 GB\nContains multiple GPUs (e.g., 8 A100 GPUs) connected via high-speed interconnects.\n\n\n\n\n\nMulti-Node Cluster:\n\nInfinite Memory 🎉\nConnects multiple nodes, allowing scaling across potentially thousands of GPUs.\n\n\n\n\n\n\nMulti-Node scalability with Jean Zay\n\n\n\nUp to 30TB of memory using all 48 nodes of Jean Zay\nIs enough to run a 15 billion particle simulation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@credit: NVIDIA\n\n\n\n\n\n\n\n@credit: servethehome.com\n\n\n\n\n\n\nSo far, we’ve focused on scaling within a single, but we have not yet explored how to achieve truly infinite scalability across an unlimited number of nodes First, let’s understand the differences in scale between a single GPU, a single node, and a multi-node cluster.\nNEXT\n\nSingle GPU: GPUs have powerful cores but are limited by memory. With a max memory of 80 GB, they are ideal for tasks that fit within this memory constraint, often used for model training or inference. NEXT\nSingle Node (Octocore): An octocore node can host multiple GPUs (e.g., 8 GPUs) and has larger memory (up to 640 GB), enabling it to handle larger datasets. This setup is common in high-performance servers. NEXT\nMulti-Node Cluster: By connecting nodes in a distributed cluster, we achieve “infinite” scalability in terms of memory and compute. JAX can take advantage of this via distributed parallelism, making it ideal for cosmological simulations and other large-scale scientific computations."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup",
    "href": "2024_11_CoPhy/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Scaling JAX on a Single GPU vs. Multi-Host Setup",
    "text": "Scaling JAX on a Single GPU vs. Multi-Host Setup\n\n\nSingle GPU Code\nx = jnp.linspace(-5, 5, 128)\nmean = 0.0\nvariance = 1.0\nresult = gaussian(x, mean, variance)\n\nMulti-GPU Code\nmesh = jax.make_mesh((8,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding)\nmean = 0.0\nvariance = 1.0\nresult = gaussian(x, mean, variance)\n\nMulti-Host Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe all know now how to run JAX code on a single GPU\nWe learned how to distribute across GPUs on a single node\nBut how do we distribute across multiple nodes?"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-1",
    "href": "2024_11_CoPhy/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Scaling JAX on a Single GPU vs. Multi-Host Setup",
    "text": "Scaling JAX on a Single GPU vs. Multi-Host Setup\nA JAX process per GPU\n\n\n\nRequesting a slurm job\n$ salloc --gres=gpu:8 --ntasks-per-node=1 --nodes=1\n\n\n\nmulti-host-jax.py\n\nimport jax\n\nmesh = jax.make_mesh((4,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\n\ndef gaussian(x, mean, variance):\n    ...\nmean = 0.0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding)\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n\nRunning with srun\n$ srun python multi-host-jax.py\n\n\n\n\n\n\n\n\n\n\nIn this example, we are utilizing 8 GPUs within a single node.\nThe code we previously discussed runs seamlessly on this setup.\nHowever, the limitation here is that we are confined to only 8 GPUs.\nNEXT"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-2",
    "href": "2024_11_CoPhy/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-2",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Scaling JAX on a Single GPU vs. Multi-Host Setup",
    "text": "Scaling JAX on a Single GPU vs. Multi-Host Setup\nA JAX process per GPU\n\n\n\nRequesting a slurm job\n$ salloc --gres=gpu:8 --ntasks-per-node=8 --nodes=2\n\n\n\nmulti-host-jax.py\n\nimport jax\njax.distributed.initialize()\nmesh = jax.make_mesh((16,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\n\ndef gaussian(x, mean, variance):\n    ...\nmean = 0.0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding) ❌ # DOES NOT WORK\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n\nRunning with srun\n$ srun -n 8 python multi-host-jax.py\n\n\n\n\n\n\n\n\n\n\nIn this case, we’re requesting 8 GPUs per node, but now across 2 nodes.\nNotice that we had to set tasks per node to 8.\nWhen working with multiple hosts, we need to run multiple processes—we can’t simply have a single process span across two nodes.\nThe implication here is that we are running the same code multiple times, in this case, 16 times (for 16 GPUs).\nWe need to call jax.distributed.initialize() to inform JAX that multiple processes are now running, and we have access to 16 GPUs.\nNEXT\nHowever, simply allocating a JAX NumPy array and using device_put() does not work as expected in this multi-node setup."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-3",
    "href": "2024_11_CoPhy/index.html#scaling-jax-on-a-single-gpu-vs.-multi-host-setup-3",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Scaling JAX on a Single GPU vs. Multi-Host Setup",
    "text": "Scaling JAX on a Single GPU vs. Multi-Host Setup\nA JAX process per GPU\n\n\n\nRequesting a slurm job\n$ salloc --gres=gpu:8 --ntasks-per-node=8 --nodes=2\n\n\n\nmulti-host-jax.py\n\nimport jax\njax.distributed.initialize()\nmesh = jax.make_mesh((16,), ('x'))\nsharding = NamedSharding(mesh , P('x'))\n\ndef gaussian(x, mean, variance):\n    ...\nmean = 0\nvariance = 1.0\nx = jnp.linspace(-5, 5, 128)\nx = jax.device_put(x, sharding) ❌ # DOES NOT WORK\nresult = gaussian(x, mean, variance)\nvisualize_array_sharding(x)\nvisualize_array_sharding(result)\n\n\nRunning with srun\n$ srun -n 8 python multi-host-jax.py\n\n\n\n\n\n\n\n\n\n\nCAUTION ⚠️\n\n\n\njax.device_put does not work with multi-host setups.\nAllocating a jax numpy array does not have the same behavior as single node setups."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#loading-data-in-jax-in-a-multi-host-setup",
    "href": "2024_11_CoPhy/index.html#loading-data-in-jax-in-a-multi-host-setup",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Loading Data in JAX in a Multi-Host Setup",
    "text": "Loading Data in JAX in a Multi-Host Setup\nA JAX process per GPU\n\nimport jax\njax.distributed.initialize()\n\nassert jax.device_count() == 16\n\nx = jnp.linspace(-5, 5, 128)\nvisualize_array_sharding(x)\n\n\n\n  GPU 0  \n         \n\n  GPU 2  \n         \n\n  GPU 1  \n         \n\n  GPU 3  \n         \n\n  GPU 14  \n         \n\n  GPU 8  \n         \n\n  GPU 7  \n         \n\n\n  GPU 5  \n         \n\n  GPU 6  \n         \n\n  GPU 4  \n         \n\n  GPU 15  \n         \n\n  GPU 12  \n         \n\n  GPU 13  \n         \n\n  GPU 11  \n         \n\n\n\nTo undestand why we visualize_array_sharding the array that we allocated\nwe see that it has been allocated 16 times, one for each process. An identical array.\nWhich is not the behavior of a single node setup.\nNEXT"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#loading-data-in-jax-in-a-multi-host-setup-1",
    "href": "2024_11_CoPhy/index.html#loading-data-in-jax-in-a-multi-host-setup-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Loading Data in JAX in a Multi-Host Setup",
    "text": "Loading Data in JAX in a Multi-Host Setup\nA JAX process per GPU\n\n\n\nmulti-host-jax.py\n\nimport jax\njax.distributed.initialize()\n\nmesh = jax.make_mesh((16,) , ('x',))\nsharding = NamedSharding(mesh , P('x'))\n\ndef distributed_linspace(start, stop, num):\n    def local_linspace(indx):\n        return np.linspace(start, stop, num)[indx]\n    return jax.make_array_from_callback(shape=(num,), sharding=sharding,data_callback=local_linspace)\n\nx = distributed_linspace(-5, 5, 128)\nif jax.process_index() == 0:\n  visualize_array_sharding(x)\n\n\n  …    …    …    …    …    …    …    …    …    …   G…   G…   G…   G…   G…   G…  \n  0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15  \n                                                                                \n\n\nTo correctly load a distributed array in a multi-host setup, we need to define a function that tells JAX how to construct the array from slices.\nThis function takes a slicing index as an argument, allowing JAX to understand how to distribute the data across the different devices and nodes."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#loading-data-in-jax-in-a-multi-host-setup-2",
    "href": "2024_11_CoPhy/index.html#loading-data-in-jax-in-a-multi-host-setup-2",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Loading Data in JAX in a Multi-Host Setup",
    "text": "Loading Data in JAX in a Multi-Host Setup\nA JAX process per GPU\n\n\n\nmulti-host-jax.py\n\nimport jax\njax.distributed.initialize()\n\nmesh = jax.make_mesh((16,) , ('x',))\nsharding = NamedSharding(mesh , P('x'))\n\ndef distributed_linspace(start, stop, num):\n    def local_linspace(indx):\n        return np.linspace(start, stop, num)[indx]\n    return jax.make_array_from_callback(shape=(num,), sharding=sharding,data_callback=local_linspace)\n\nx = distributed_linspace(-5, 5, 128)\nif jax.process_index() == 0:\n  visualize_array_sharding(x)\nmean = 0.0\nvariance = 1.0\nresult = gaussian(x, mean, variance)\nif jax.process_index() == 0:\n  visualize_array_sharding(result)\n\n\n  …    …    …    …    …    …    …    …    …    …   G…   G…   G…   G…   G…   G…  \n  0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15  \n                                                                                \n\n\n  …    …    …    …    …    …    …    …    …    …   G…   G…   G…   G…   G…   G…  \n  0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15  \n                                                                                \n\n\nWe can now use the distributed array in a computation and visualize the result. We can use the exact same function as before All collectives work the same the only thing that changes is the way we load the data."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#forward-modeling-in-cosmology",
    "href": "2024_11_CoPhy/index.html#forward-modeling-in-cosmology",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Forward Modeling in Cosmology",
    "text": "Forward Modeling in Cosmology\n\n\nWeak Lensing Model\n\nPrediction:\n\nA simulator generates observations from initial conditions and cosmological parameters.\n\nInference:\n\nThe simulated results are compared with actual observations.\nOptimal initial conditions and parameters are inferred to closely match the observed data.\n\n\n\n\n\nScaling Challenges\n\n\n\nSoftware: Existing tools like JaxPM or PMWD already exist.\nResolution Today: these differentiable simulators currently support up to 130 million particles \\(512^3\\).\nIdeal Resolution: Billion-particle simulations are necessary for high accuracy \\(1024^3\\) and more.\n(See Hugo’s and Justine’s talks for more details)\nWe need to scale up to multiple GPUs and nodes to reach the required resolution.\n\n\n\n\n\n\n\n\n\n\nForward Modeling (Prediction)\n\n\n\n\n\n\n\nForward Modeling (Inference)\n\n\n\n\n\n\nSo before diving into multi-node tools for cosmology, let’s see how they can benefit forward modeling.\n- Forward modeling is a cornerstone of cosmological inference, linking theoretical predictions with observed data.\nIn forward modeling, the goal is to replace an explicit likelihood function with a simulator. The process involves:\n\nPrediction:\n\nThe simulator generates synthetic observables, such as convergence maps, using initial conditions and cosmological parameters.\nThese observables mimic the universe’s large-scale structure under specific physical assumptions.\n\nInference:\n\nSimulated results are compared to actual observations (e.g., from telescopes).\nThrough iterative refinement, we infer the parameters that best match the observed universe, like dark matter density or Hubble constant.\n\nResolution Today:\n\nSimulations operate with 250,000–130 million particles (512^3).\nThese scales capture broad features but miss finer details essential for precision cosmology.\n\nIdeal Resolution:\n\nBillion-particle simulations are critical for matching the accuracy demanded by modern cosmological surveys.\nThese simulations uncover small-scale phenomena like non-linear clustering.\n\nTools:\n\nTools like JaxPM and PMWD handle simulations up to 130 million particles on a single GPU.\nScaling beyond this requires multi-node, distributed approaches."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#jaxdecomp-components-for-distributed-particle-mesh-simulations",
    "href": "2024_11_CoPhy/index.html#jaxdecomp-components-for-distributed-particle-mesh-simulations",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "jaxDecomp : Components for Distributed Particle Mesh Simulations",
    "text": "jaxDecomp : Components for Distributed Particle Mesh Simulations\n\n\n\nKey Features\n\n\nDistributed 3D FFT\n\nEssential for force calculations in large-scale simulations.\n\n\n\n\n\nHalo Exchange for Boundary Conditions\n\nManages boundary conditions or particles leaving the simulation domain.\n\n\n\n\n\nFully Differentiable\n\nCan be used with differentiable simulations.\n\n\n\n\n\nMulti-Node Supports\n\nWorks seamlessly across multiple nodes.\n\nSupports Different Sharding strategies\nOpen-source and available on PyPI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to jaxDecomp:\n\nLet’s start with jaxDecomp, an open-source package designed specifically for distributed 3D Fast Fourier Transforms (FFT) using JAX.\n\nKey Features:\n\nDistributed 3D FFT: This core feature is essential for performing FFTs across distributed systems, making it highly effective for force calculations or solving partial differential equations in large-scale simulations.\nHalo Exchange for Boundary Conditions: A critical feature that manages boundary conditions in distributed simulations, ensuring proper handling of particles or data that leave or enter the simulation domain. This maintains the accuracy and continuity of simulations across multiple nodes.\nFully Differentiable: The package is fully differentiable, which means it integrates well with JAX’s automatic differentiation capabilities. This makes it ideal for simulations where optimization or gradient-based methods are required.\nMulti-Node Support: jaxDecomp scales across multiple nodes, allowing users to take full advantage of large-scale distributed systems, such as supercomputers or cloud clusters.\nSupports Different Sharding Strategies: jaxDecomp provides flexible data sharding options, which helps in distributing data efficiently across devices, ensuring optimal parallel computation.\nOpen-source: jaxDecomp is an open-source library available for anyone to use. You can easily install it via PyPI or access the code on GitHub."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#performance-benchmarks-of-pfft3d",
    "href": "2024_11_CoPhy/index.html#performance-benchmarks-of-pfft3d",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Performance benchmarks of PFFT3D",
    "text": "Performance benchmarks of PFFT3D\n \n\n\nStrong Scaling\n\n\n\n\n\n\nWeak scaling\n\n\n\n\n\n\n\n\nPerformance Benchmarks:\n\nNow, let’s take a look at the performance benchmarks for PFFT3D, the distributed 3D FFT implementation in jaxDecomp. These benchmarks focus on two types of scaling: strong scaling and weak scaling.\n\nKey Takeaway:\n\nWhile the scaling does not perfectly match the theoretical expectations, these results are still valuable as they reflect real-world conditions where network bandwidth and node interconnects impact performance. The bump observed is typical in distributed systems and is something to account for when planning the scaling of large simulations or computations.\n\nYou can see that I was able to fit a double precision 4096^3 FFT in 5 secondes on 256 GPUs. For info 4096^3 is 64 billion points and 1 TO of data."
  },
  {
    "objectID": "2024_11_CoPhy/index.html#halo-exchange-in-distributed-simulations",
    "href": "2024_11_CoPhy/index.html#halo-exchange-in-distributed-simulations",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Halo exchange in distributed simulations",
    "text": "Halo exchange in distributed simulations\n\n\n\n\n\n\n\n\n\n\n\nInitial Field\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\n\n\nLPT Field\n\n\n\n\n\nHalo exchange is essential in distributed simulations where we need to manage boundary conditions. In cosmological simulations, each node (or slice) of the simulation only has a portion of the data, so the boundary values at the edges need to be exchanged between neighboring nodes to maintain accuracy.\nWhere do all this fit in"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#jaxpm-2.0-distributed-particle-mesh-simulation",
    "href": "2024_11_CoPhy/index.html#jaxpm-2.0-distributed-particle-mesh-simulation",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "JaxPM 2.0 : Distributed Particle Mesh Simulation",
    "text": "JaxPM 2.0 : Distributed Particle Mesh Simulation\n\n\nBox size: 1G Mpc/h Resolution: \\(1024^3\\) Number of particles: 1 billion Number of snapshots: 10 Halo size: 128 Number of GPU used : 32 time taken : 45s\n\n     \n\n\n\nKey Features of JaxPM\n\n\n\nMulti-Node Performance: Optimized for efficient scaling across nodes.\nHigh Resolution: Capable of handling billions of particles for accurate simulations.\nDifferentiable: Compatible with JAX’s automatic differentiation (HMC, NUTS compatible).\nOpen Source:"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#conclusion-enabling-scalable-cosmology-with-distributed-jax",
    "href": "2024_11_CoPhy/index.html#conclusion-enabling-scalable-cosmology-with-distributed-jax",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Conclusion: Enabling Scalable Cosmology with Distributed JAX",
    "text": "Conclusion: Enabling Scalable Cosmology with Distributed JAX\n\n\nDistributed JAX: A Game-Changer for Cosmology\n\n\n\nThe future is bright for JAX in cosmology 🎉🎉!!\nJAX has transformed the landscape for scientific computing, enabling large-scale, distributed workflows in a Pythonic environment.\nRecent advancements (JAX 0.4.3x+) make it straightforward to scale computations across multiple GPUs and nodes.\nKey Advantages\n\nSimplicity: JAX makes it easier than ever to write high-performance code, allowing researchers to focus on science rather than infrastructure.\nDifferentiability: JAX allows seamless differentiation of code running across hundreds of GPUs, enabling advanced inference techniques.\n\nThe Future Ahead\n\nScaling Inference Models with Distributed jaxPM: By integrating the new distributed jaxPM into existing cosmological inference models, we can achieve unprecedented levels of detail and complexity.\nPaving the way to fully leverage large-scale survey data for deeper insights into the universe.\n\n\n\n\n\n\n\nTutorials and Exercises\n\n\nhttps://github.com/ASKabalan/Tutorials/blob/main/Cophy2024/Exercises/01_MultiDevice_With_JAX.ipynb\n\n\n\n\n\nIntroduction to JAX’s Impact on Cosmology:\n\nThe future of JAX in cosmology is incredibly promising! 🎉🎉 With its rapid advancements, JAX has revolutionized scientific computing, especially in the field of cosmology. It enables large-scale, distributed workflows, making it easier to scale computations across multiple GPUs and nodes without sacrificing ease of use or flexibility.\n\nKey Advantages:\n\nSimplicity: One of the standout features of JAX is its simplicity. The ability to write high-performance code in a Pythonic manner allows researchers to focus on the science and not the complexity of infrastructure. The learning curve for using JAX is significantly reduced, empowering scientists to leverage cutting-edge computational techniques without the steep overhead typically associated with parallelism or distributed computing.\nDifferentiability: Another key benefit is differentiability. JAX allows for automatic differentiation of complex models that run across hundreds or thousands of GPUs, making it easier to integrate advanced inference techniques. This is especially powerful in cosmology, where we need to optimize models across vast datasets and large-scale simulations.\n\nLooking to the Future:\n\nScaling Inference Models with Distributed jaxPM: Looking ahead, JAX’s ability to scale inference models through tools like distributed jaxPM will open new doors. By incorporating this into cosmological inference models, we can simulate more complex phenomena at much higher resolutions, giving us the power to explore more detailed and intricate patterns in the universe’s behavior. The integration of these tools promises to provide new insights into large-scale structures in the universe.\nLeveraging Large-Scale Survey Data: The future of cosmology also depends on fully harnessing the potential of large-scale survey data. With JAX, researchers will be able to process and analyze these massive datasets with unprecedented efficiency, leading to deeper insights and a better understanding of our universe’s fundamental properties.\n\nSummary:\n\nJAX is changing the landscape of cosmology by providing tools that allow us to easily scale our computational workflows, differentiate complex models, and unlock the potential of distributed computing. With continuous advancements and the growing support of large-scale computing systems, the future of cosmology looks brighter than ever! 🌌"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#using-shard_map-for-advanced-parallelism-in-jax",
    "href": "2024_11_CoPhy/index.html#using-shard_map-for-advanced-parallelism-in-jax",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Using shard_map for Advanced Parallelism in JAX",
    "text": "Using shard_map for Advanced Parallelism in JAX\nWhy shard_map instead of pmap?\n\nLimitations of pmap :\n\npmap is effective for simple data parallelism but lacks flexibility in more complex cases.\nNested Parallelism: pmap does not handle nested parallelism well.\nData Layout Control: pmap does not offer fine-grained control over data layout.\n\nAdvantages of shard_map:\n\nGreater Flexibility: shard_map allows custom parallelism patterns and fine control over data sharding.\nNested Parallelism Support: Suitable for complex workloads that require hierarchical parallelism.\nDirect Device Control: Allows fine-grained control over data distribution and parallel operations.\n\n\nJAX explaining the weakness of pmap"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#example-nested-parallelism-with-shard_map",
    "href": "2024_11_CoPhy/index.html#example-nested-parallelism-with-shard_map",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Example: Nested Parallelism with shard_map",
    "text": "Example: Nested Parallelism with shard_map\n\nmesh = jax.make_mesh((2,2), ('x', 'y'))\nsharding = NamedSharding(mesh , P('x', 'y'))\ndata = jnp.arange(16).reshape(4, 4) \nsharded_data = lax.with_sharding_constraint(data, sharding)\n\n@partial(jax.pmap, axis_name='x' , devices=mesh.devices[0])\n@partial(jax.pmap, axis_name='y', devices=mesh.devices[1])\ndef sum_and_avg_nested_pmap(x):\n      sum_across_x = lax.psum(x, axis_name='x')\n      avg_across_y = lax.pmean(sum_across_x, axis_name='y')  \n      return avg_across_y\n\ndef sum_and_avg_pmap(x):\n    sum_across_x = jax.pmap(lambda a: lax.psum(a, axis_name='x'),\n                            axis_name='x',\n                            devices=mesh.devices[0])(x.reshape(2, 2, 4))\n    avg_across_y = jax.pmap(lambda a: lax.pmean(a, axis_name='y'),\n                            axis_name='y',\n                            devices=mesh.devices[1])(sum_across_x.reshape(2, 4, 2))\n    return avg_across_y.reshape(4, 4)\n\n@partial(shard_map , mesh=mesh , in_specs=(P('x', 'y'),), out_specs=P('x'))\ndef sum_and_avg_shardmap(x):\n      sum_across_x = lax.psum(x, axis_name='x')\n      avg_across_y = lax.pmean(sum_across_x, axis_name='y')  \n      return avg_across_y"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#example-nested-parallelism-with-shard_map-1",
    "href": "2024_11_CoPhy/index.html#example-nested-parallelism-with-shard_map-1",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Example: Nested Parallelism with shard_map",
    "text": "Example: Nested Parallelism with shard_map\n\nmesh = jax.make_mesh((2,2), ('x', 'y'))\nsharding = NamedSharding(mesh , P('x', 'y'))\ndata = jnp.arange(16).reshape(4, 4) \nsharded_data = lax.with_sharding_constraint(data, sharding)\n\n@partial(jax.pmap, axis_name='x' , devices=mesh.devices[0])\n@partial(jax.pmap, axis_name='y', devices=mesh.devices[1])\ndef sum_and_avg_nested_pmap(x):\n      sum_across_x = lax.psum(x, axis_name='x')\n      avg_across_y = lax.pmean(sum_across_x, axis_name='y')  \n      return avg_across_y\n\ndef sum_and_avg_pmap(x):\n    sum_across_x = jax.pmap(lambda a: lax.psum(a, axis_name='x'),\n                            axis_name='x',\n                            devices=mesh.devices[0])(x.reshape(2, 2, 4))\n    avg_across_y = jax.pmap(lambda a: lax.pmean(a, axis_name='y'),\n                            axis_name='y',\n                            devices=mesh.devices[1])(sum_across_x.reshape(2, 4, 2))\n    return avg_across_y.reshape(4, 4)\n\n@partial(shard_map , mesh=mesh , in_specs=(P('x', 'y'),), out_specs=P('x'))\ndef sum_and_avg_shardmap(x):\n      sum_across_x = lax.psum(x, axis_name='x')\n      avg_across_y = lax.pmean(sum_across_x, axis_name='y')  \n      return avg_across_y"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#example-nested-parallelism-with-shard_map-2",
    "href": "2024_11_CoPhy/index.html#example-nested-parallelism-with-shard_map-2",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Example: Nested Parallelism with shard_map",
    "text": "Example: Nested Parallelism with shard_map\n\nmesh = jax.make_mesh((2,2), ('x', 'y'))\nsharding = NamedSharding(mesh , P('x', 'y'))\ndata = jnp.arange(16).reshape(4, 4) \nsharded_data = lax.with_sharding_constraint(data, sharding)\n\n@partial(jax.pmap, axis_name='x' , devices=mesh.devices[0])\n@partial(jax.pmap, axis_name='y', devices=mesh.devices[1])\ndef sum_and_avg_nested_pmap(x):\n      sum_across_x = lax.psum(x, axis_name='x')\n      avg_across_y = lax.pmean(sum_across_x, axis_name='y')  \n      return avg_across_y\n\ndef sum_and_avg_pmap(x):\n    sum_across_x = jax.pmap(lambda a: lax.psum(a, axis_name='x'),\n                            axis_name='x',\n                            devices=mesh.devices[0])(x.reshape(2, 2, 4))\n    avg_across_y = jax.pmap(lambda a: lax.pmean(a, axis_name='y'),\n                            axis_name='y',\n                            devices=mesh.devices[1])(sum_across_x.reshape(2, 4, 2))\n    return avg_across_y.reshape(4, 4)\n\n@partial(shard_map , mesh=mesh , in_specs=(P('x', 'y'),), out_specs=P('x'))\ndef sum_and_avg_shardmap(x):\n      sum_across_x = lax.psum(x, axis_name='x')\n      avg_across_y = lax.pmean(sum_across_x, axis_name='y')  \n      return avg_across_y"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#example-nested-parallelism-with-shard_map-3",
    "href": "2024_11_CoPhy/index.html#example-nested-parallelism-with-shard_map-3",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Example: Nested Parallelism with shard_map",
    "text": "Example: Nested Parallelism with shard_map\n\nmesh = jax.make_mesh((2,2), ('x', 'y'))\nsharding = NamedSharding(mesh , P('x', 'y'))\ndata = jnp.arange(16).reshape(4, 4) \nsharded_data = lax.with_sharding_constraint(data, sharding)\n\n@partial(jax.pmap, axis_name='x' , devices=mesh.devices[0])\n@partial(jax.pmap, axis_name='y', devices=mesh.devices[1])\ndef sum_and_avg_nested_pmap(x):\n      sum_across_x = lax.psum(x, axis_name='x')\n      avg_across_y = lax.pmean(sum_across_x, axis_name='y')  \n      return avg_across_y\n\ndef sum_and_avg_pmap(x):\n    sum_across_x = jax.pmap(lambda a: lax.psum(a, axis_name='x'),\n                            axis_name='x',\n                            devices=mesh.devices[0])(x.reshape(2, 2, 4))\n    avg_across_y = jax.pmap(lambda a: lax.pmean(a, axis_name='y'),\n                            axis_name='y',\n                            devices=mesh.devices[1])(sum_across_x.reshape(2, 4, 2))\n    return avg_across_y.reshape(4, 4)\n\n@partial(shard_map , mesh=mesh , in_specs=(P('x', 'y'),), out_specs=P('x'))\ndef sum_and_avg_shardmap(x):\n      sum_across_x = lax.psum(x, axis_name='x')\n      avg_across_y = lax.pmean(sum_across_x, axis_name='y')  \n      return avg_across_y"
  },
  {
    "objectID": "2024_11_CoPhy/index.html#motivation-cosmology-in-the-exascale-era",
    "href": "2024_11_CoPhy/index.html#motivation-cosmology-in-the-exascale-era",
    "title": "Massively Parallel Computing in Cosmology with JAX",
    "section": "Motivation: Cosmology in the Exascale Era ",
    "text": "Motivation: Cosmology in the Exascale Era \n\n\n\n\n\nUpcoming Surveys and Massive Data in Cosmology\n\n\n\nMassive Data Volume: LSST will generate 20 TB of raw data per night over 10 years, totaling 60 PB.\nCatalog Size: The processed LSST catalog database will reach 15 PB.\n\n\n\n\n\nCosmological Models and Pipelines\n\n\n\nCosmological simulations and forward modeling can easily reach multiple terabytes in size.\nWe need to scale up cosmological pipelines to handle these data volumes effectively.\n\n\n\n\n\nwill explain scaling in here"
  },
  {
    "objectID": "2025_05_BDL/index.html#outline-for-this-presentation",
    "href": "2025_05_BDL/index.html#outline-for-this-presentation",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Outline for This Presentation",
    "text": "Outline for This Presentation\n  \n\n\n\nBeyond Summary Statistics Inference in Cosmology  \nBuilding N-body Simulators for Cosmological Inference  \nModeling Observables: Weak Lensing & Lightcones  \nScaling Up: Distributed, Differentiable Simulations"
  },
  {
    "objectID": "2025_05_BDL/index.html#the-traditional-approach-to-cosmological-inference",
    "href": "2025_05_BDL/index.html#the-traditional-approach-to-cosmological-inference",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "The Traditional Approach to Cosmological Inference",
    "text": "The Traditional Approach to Cosmological Inference\n\n\n\n\n\n\n\n\n\n\n\n \n\ncosmological parameters (Ω): matter density, dark energy, etc.\nPredict observables: CMB, galaxies, lensing\nExtract summary statistics: \\(P(k)\\), \\(C_\\ell\\) , 2PCF\nCompute likelihood: \\(L(\\Omega \\vert data)\\)\nEstimate \\(\\hat{\\Omega}\\) via maximization (\\(\\chi^2\\) fitting)\n\n\n\n\n\nSummary Statistics Based Inference\n\n\n\nTraditional inference uses summary statistics to compress data.\nPower spectrum fitting: \\(P(k)\\), \\(C_\\ell\\)\nIt misses complex, non-linear structure in the data\n\n\n\n\n\n“Most of modern cosmological inference pipelines rely on summary statistics — things like the power spectrum P(k)P(k) or angular power spectrum CℓCℓ​.”\n“These work well under the assumption that most of the information is encoded in second-order statistics — basically, the correlations between pairs of points.”\n“But this approach ignores all the higher-order structure — the full non-linear complexity that emerges in the formation of cosmic structure.”\nEven higher-order statistics (like bispectrum or 3-point correlations) still reduce the data, and fail to capture the entire structure or allow for fully Bayesian inference over the field.\n“And that’s the motivation for moving beyond summary statistics…”"
  },
  {
    "objectID": "2025_05_BDL/index.html#the-traditional-approach-to-cosmological-inference-1",
    "href": "2025_05_BDL/index.html#the-traditional-approach-to-cosmological-inference-1",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "The Traditional Approach to Cosmological Inference",
    "text": "The Traditional Approach to Cosmological Inference\n\n\n\n\n\n\nCredit: Natalia Porqueres\n\n\n\n\n\n\n\nJeffrey et al. (2024)\n\n\n\n\n\n\n\nSummary statistics (e.g. P(k)) discard the non-Gaussian features.\nGradient-based curve fitting does not recover the true posterior shape."
  },
  {
    "objectID": "2025_05_BDL/index.html#from-summary-statistics-to-likelihood-free-inference",
    "href": "2025_05_BDL/index.html#from-summary-statistics-to-likelihood-free-inference",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "From Summary Statistics to Likelihood Free Inference",
    "text": "From Summary Statistics to Likelihood Free Inference\n\n\n\n\n\n\n\n\n\nBayes’ Theorem\n\\[\np(\\theta \\mid x_0) \\propto p(x_0 \\mid \\theta) \\cdot p(\\theta)\n\\]\n\nPrior: Encodes our assumptions about parameters \\(\\theta\\)\nLikelihood: How likely the data \\(x_0\\) is given \\(\\theta\\)\nPosterior: What we want to learn — how data updates our belief about \\(\\theta\\)\n\n\n\n\n\n\nSimulators become the bridge between cosmological parameters and observables.\nHow to use simulators allow us to go beyond summary statistics?"
  },
  {
    "objectID": "2025_05_BDL/index.html#from-summary-statistics-to-likelihood-free-inference-1",
    "href": "2025_05_BDL/index.html#from-summary-statistics-to-likelihood-free-inference-1",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "From Summary Statistics to Likelihood Free Inference",
    "text": "From Summary Statistics to Likelihood Free Inference\n\n\n\nImplicit Inference\n\nTreats the simulator as a black box — we only require the ability to simulate \\((\\theta, x)\\) pairs.\nNo need for an explicit likelihood — instead, use simulation-based inference (SBI) techniques\nOften relies on compression to summary statistics \\(t = f_\\phi(x)\\), then approximates \\(p(\\theta \\mid t)\\).\n\n\nExplicit Inference\n\nRequires a differentiable forward model or simulator.\nTreat the simulator as a probabilistic model and perform inference over the joint posterior \\(p(x \\mid \\theta, z)\\)\nComputationally demanding — but provides exact control over the statistical model.\n\n\n\n\n\n\n\nImplicit Inference\n\n\n\n\n\n\n\nExplicit Inference"
  },
  {
    "objectID": "2025_05_BDL/index.html#implicit-inference-1",
    "href": "2025_05_BDL/index.html#implicit-inference-1",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Implicit inference",
    "text": "Implicit inference\nSimulation-Based Inference Loop\n\nSample parameters \\(\\theta_i \\sim p(\\theta)\\)\nRun simulator \\(x_i = p(x \\vert \\theta_i)\\)\nCompress observables \\(t_i = f_\\phi(x_i)\\)\nTrain a density estimator \\(\\hat{p}_\\Phi(\\theta \\mid f_\\phi(x))\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Summarisation (Zeghal & Lanzieri et al 2025).\nNormalizing Flows (Zeghal et al. 2022).\n✅ Works with non-differentiable or stochastic simulators\n❌ Requires an optimal compression function \\(f_\\phi\\)"
  },
  {
    "objectID": "2025_05_BDL/index.html#explicit-inference-1",
    "href": "2025_05_BDL/index.html#explicit-inference-1",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Explicit inference",
    "text": "Explicit inference\n\n\nThe goal is to reconstruct the entire latent structure of the Universe — not just compress it into summary statistics. To do this, we jointly infer:\n\\[\np(\\theta, z \\mid x) \\propto p(x \\mid \\theta, z) \\, p(z \\mid \\theta) \\, p(\\theta)\n\\]\nWhere:\n\n\\(\\theta\\): cosmological parameters (e.g. matter density, dark energy, etc.)\n\\(z\\): latent fields (e.g. initial conditions of the density field)\n\\(x\\): observed data (e.g. convergence maps or galaxy fields)\n\n\n\n\nThe challenge of explicit inference\n\n\n\nThe latent variables \\(z\\) typically live in very high-dimensional spaces — with millions of degrees of freedom.\nSampling in this space is intractable using traditional inference techniques.\n\n\n\n\n\n\n\n\nWe need samplers that can scale efficiently to high-dimensional latent spaces and Exploit gradients from differentiable simulators\nThis makes differentiable simulators essential for modern cosmological inference.\nParticle Mesh (PM) simulations offer a scalable and differentiable solution.\n\n\n\n\n\n\n\n\n\nExplicit Inference"
  },
  {
    "objectID": "2025_05_BDL/index.html#particle-mesh-simulations",
    "href": "2025_05_BDL/index.html#particle-mesh-simulations",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Particle Mesh Simulations",
    "text": "Particle Mesh Simulations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompute Forces via PM method\n\nStart with particles \\(\\mathbf{x}_i, \\mathbf{p}_i\\)\n\nInterpolate to mesh: \\(\\rho(\\mathbf{x})\\)\n\nSolve Poisson’s Equation: \\[\n\\nabla^2 \\phi = -4\\pi G \\rho\n\\]\nIn Fourier space: \\[\n\\mathbf{f}(\\mathbf{k}) = i\\mathbf{k}k^{-2}\\rho(\\mathbf{k})\n\\]\n\n\nTime Evolution via ODE\n\nPM uses Kick-Drift-Kick (symplectic) scheme:\n\nDrift: \\(\\mathbf{x} \\leftarrow \\mathbf{x} + \\Delta a \\cdot \\mathbf{v}\\)\nKick: \\(\\mathbf{v} \\leftarrow \\mathbf{v} + \\Delta a \\cdot \\nabla \\phi\\)\n\n\n\n\n\n\n\n\nFast and scalable approximation to gravity.\nA cycle of FFTs and interpolations.\nSacrifices small-scale accuracy for speed and differentiability.\nCurrent implementations JAXPM v0.1, PMWD and BORG."
  },
  {
    "objectID": "2025_05_BDL/index.html#from-3d-structure-to-lensing-observables",
    "href": "2025_05_BDL/index.html#from-3d-structure-to-lensing-observables",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "From 3D Structure to Lensing Observables",
    "text": "From 3D Structure to Lensing Observables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulate structure formation over time, taking snapshots at key redshifts\nStitch these snapshots into a lightcone, mimicking the observer’s view of the universe\nCombine contributions from all slabs to form convergence maps\nUse the Born approximation to simplify the lensing calculation\n\n\n\n\n\nBorn Approximation for Convergence\n\n\n\\[\n\\kappa(\\boldsymbol{\\theta}) = \\int_0^{r_s} dr \\, W(r, r_s) \\, \\delta(\\boldsymbol{\\theta}, r)\n\\]\nWhere the lensing weight is:\n\\[\nW(r, r_s) = \\frac{3}{2} \\, \\Omega_m \\, \\left( \\frac{H_0}{c} \\right)^2 \\, \\frac{r}{a(r)} \\left(1 - \\frac{r}{r_s} \\right)\n\\]"
  },
  {
    "objectID": "2025_05_BDL/index.html#when-the-simulator-fails-the-model-fails",
    "href": "2025_05_BDL/index.html#when-the-simulator-fails-the-model-fails",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "When the Simulator Fails, the Model Fails",
    "text": "When the Simulator Fails, the Model Fails\nInference is only as good as the simulator it depends on.\n\n\n\n\nIf we want to model complex phenomena like galaxy painting, baryonic feedback, or non-linear structure formation, our simulator must be not only fast, but also physically accurate.\nA decent resolution for weak lensing requires about 1 grid cell per Mpc/h — both for angular resolution and structure fidelity."
  },
  {
    "objectID": "2025_05_BDL/index.html#scaling-up-the-simulation-volume-the-lsst-challenge",
    "href": "2025_05_BDL/index.html#scaling-up-the-simulation-volume-the-lsst-challenge",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Scaling Up the simulation volume: The LSST Challenge",
    "text": "Scaling Up the simulation volume: The LSST Challenge\n\n\nLSST Scan Range\n\nCovers ~18,000 deg² (~44% of the sky)\nRedshift reach: up to z ≈ 3\nArcminute-scale resolution\nRequires simulations spanning thousands of Mpc in depth and width\n\n\n\n\n\nLSST Survey Footprint\n\n\n\n\nSimulating even a (1 Gpc/h)³ subvolume at 1024³ mesh resolution requires:\n\n~54 GB of memory for a simulation with a single snapshot\nGradient-based inference and multi-step evolution push that beyond 100–200 GB\n\n\n\n\n\n\nTakeaway\n\n\n\nLSST-scale cosmological inference demands multiple (Gpc/h)³ simulations at high resolution.\nModern high-end GPUs cap at ~80 GB, so even a single box requires multi-GPU distributed simulation — both for memory and compute scalability.\n\n\n\n\n\n\n\nJean Zay HPC - IDRIS"
  },
  {
    "objectID": "2025_05_BDL/index.html#distributed-particle-mesh-simulation",
    "href": "2025_05_BDL/index.html#distributed-particle-mesh-simulation",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Distributed Particle Mesh Simulation",
    "text": "Distributed Particle Mesh Simulation\n\n\n\n\n\n\n\nParticle Mesh Simulation\n\n\n\n\n\n\n\nParticle Mesh Simulation\n\n\n\n\n\n\n\nParticle Mesh Simulation\n\n\n\n\n\n\nForce Computation is Easy to Parallelize\n\nPoisson’s equation in Fourier space:\n\\[\n\\nabla^2 \\phi = -4\\pi G \\rho\n\\]\nGravitational force in Fourier space:\n\\[\n\\mathbf{f}(\\mathbf{k}) = i\\mathbf{k}k^{-2}\\rho(\\mathbf{k})\n\\]\nEach Fourier mode \\(\\mathbf{k}\\) can be computed independently using JAX\n\nPerfect for large-scale, parallel GPU execution\n\n\n\n\n\n\n\n\n\n\nFourier Transform requires global communication"
  },
  {
    "objectID": "2025_05_BDL/index.html#jaxdecomp-distributed-3d-fft-and-halo-exchange",
    "href": "2025_05_BDL/index.html#jaxdecomp-distributed-3d-fft-and-halo-exchange",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "jaxDecomp: Distributed 3D FFT and Halo Exchange",
    "text": "jaxDecomp: Distributed 3D FFT and Halo Exchange\n\n\nDistributed 3D FFT using domain decomposition\n\nFully differentiable, runs on multi-GPU and multi-node setups\n\nDesigned as a drop-in replacement for jax.numpy.fft.fftn\n\nOpen source and available on PyPI \\(\\Rightarrow\\) pip install jaxdecomp\nHalo exchange for mass conservation across subdomains\n\n\n\n\n\n\n\nFFT\n\n\n\n\n\n\n\nHalo Exchange"
  },
  {
    "objectID": "2025_05_BDL/index.html#distributed-particle-mesh-simulation-1",
    "href": "2025_05_BDL/index.html#distributed-particle-mesh-simulation-1",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Distributed Particle Mesh Simulation",
    "text": "Distributed Particle Mesh Simulation\n\n\n\n\nParticle Mesh Simulation"
  },
  {
    "objectID": "2025_05_BDL/index.html#cloud-in-cell-cic-interpolation",
    "href": "2025_05_BDL/index.html#cloud-in-cell-cic-interpolation",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Cloud In Cell (CIC) Interpolation",
    "text": "Cloud In Cell (CIC) Interpolation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMass Assignment and Readout\nThe Cloud-In-Cell (CIC) scheme spreads particle mass to nearby grid points using a linear kernel:\n\n\n\nPaint to Grid (mass deposition): \\[\ng(\\mathbf{j}) = \\sum_i m_i \\prod_{d=1}^{D} \\left(1 - \\left|p_i^d - j_d\\right|\\right)\n\\]\n\n\n\nRead from Grid (force interpolation): \\[\nv_i = \\sum_{\\mathbf{j}} g(\\mathbf{j}) \\prod_{d=1}^{D} \\left(1 - \\left|p_i^d - j_d\\right|\\right)\n\\]"
  },
  {
    "objectID": "2025_05_BDL/index.html#distributed-cloud-in-cell-cic-interpolation",
    "href": "2025_05_BDL/index.html#distributed-cloud-in-cell-cic-interpolation",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Distributed Cloud In Cell (CIC) Interpolation",
    "text": "Distributed Cloud In Cell (CIC) Interpolation\n\nIn distributed simulations, each subdomain handles a portion of the global domain\nBoundary conditions are crucial to ensure physical continuity across subdomain edges\nCIC interpolation assigns and reads mass from nearby grid cells — potentially crossing subdomain boundaries\nTo avoid discontinuities or mass loss, we apply halo exchange:\n\nSubdomains share overlapping edge data with neighbors\nEnsures correct mass assignment and gradient flow across boundaries\n\n\n\n\nWithout Halo Exchange (Not Distributed)\n\n\n\n\n\n\nSub Domain 1 (Particles)\n\n\n\n\n\n\n\nSub Domain 1 (Grid)\n\n\n\n\n\n\n\nSub Domain 1 (Read out)\n\n\n\n\n\n\n\nWith Halo Exchange (Distributed)\n\n\n\n\n\n\nSub Domain 1 & Halo (Particles)\n\n\n\n\n\n\n\nSub Domain 1 & Halo (Grid)\n\n\n\n\n\n\n\nSub Domain 2 & Halo (Grid)"
  },
  {
    "objectID": "2025_05_BDL/index.html#why-halo-exchange-matters-in-distributed-simulations",
    "href": "2025_05_BDL/index.html#why-halo-exchange-matters-in-distributed-simulations",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Why Halo Exchange Matters in Distributed Simulations",
    "text": "Why Halo Exchange Matters in Distributed Simulations\n\n\n\n\n\n\nWithout halo exchange, subdomain boundaries introduce visible artifacts in the final field.\nThis breaks the smoothness of the result — even when each local computation is correct.\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nNo Halo Artifacts\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst slice\n\n\n\n\n\n\n\n\n\nSecond slice\n\n\n\n\n\n\n\n\n\nThird slice\n\n\n\n\n\n\n\n\n\nFourth slice\n\n\n\n\n\n\n\n\n\n\nWith Halo no Artifacts"
  },
  {
    "objectID": "2025_05_BDL/index.html#backpropagating-through-ode-integration",
    "href": "2025_05_BDL/index.html#backpropagating-through-ode-integration",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Backpropagating Through ODE Integration",
    "text": "Backpropagating Through ODE Integration\n \n\n\nWhy Gradients Are Costly\nTo compute gradients through a simulation, we need to track:\n\nAll intermediate positions: \\(d_i\\)\nAll velocities: \\(v_i\\)\nAnd their tangent vectors for backpropagation\n\nEven though each update is simple, autodiff requires storing the full history.\n\n\nExample: Kick-Drift Integration\nA typical update step looks like:\n\\[\n\\begin{aligned}\nd_{i+1} &= d_i + v_i \\, \\Delta t \\\\\\\\\nv_{i+1} &= v_i + F(d_{i+1}) \\, \\Delta t\n\\end{aligned}\n\\]\nOver many steps, the memory demand scales linearly — which becomes a bottleneck for large simulations.\n\n\n\n\n\nWhy This Is a Problem in Practice\n\n\n\nStoring intermediate states for autodiff causes memory to scale linearly with the number of steps.\nExample costs at full resolution:\n\n(1 Gpc/h)³, 10 steps → ~500 GB\n(2 Gpc/h)³, 10 steps → ~4.2 TB\n\nThis severely limits how many time steps or how large a volume we can afford — even with many GPUs."
  },
  {
    "objectID": "2025_05_BDL/index.html#reverse-adjoint-gradient-propagation-without-trajectory-storage-preliminary",
    "href": "2025_05_BDL/index.html#reverse-adjoint-gradient-propagation-without-trajectory-storage-preliminary",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Reverse Adjoint: Gradient Propagation Without Trajectory Storage (Preliminary)",
    "text": "Reverse Adjoint: Gradient Propagation Without Trajectory Storage (Preliminary)\n\n\n\n\nInstead of storing the full trajectory…\nWe use the reverse adjoint method:\n\nSave only the final state\nRe-integrate backward in time to compute gradients\n\n\n\nForward Pass (Kick-Drift)\n\\[\n\\begin{aligned}\nd_{i+1} &= d_i + v_i \\, \\Delta t \\\\\nv_{i+1} &= v_i + F(d_{i+1}) \\, \\Delta t\n\\end{aligned}\n\\]\nReverse Pass (Adjoint Method)\n\\[\n\\begin{aligned}\nv_i &= v_{i+1} - F(d_{i+1}) \\, \\Delta t \\\\\nd_i &= d_{i+1} - v_i \\, \\Delta t\n\\end{aligned}\n\\]\n\n\n\n\n\n\nMemory vs. Checkpoints\n\n\n\n\nCheckpointing saves intermediate simulation states periodically to reduce memory — but still grows with the number of steps.\nReverse Adjoint recomputes on demand, keeping memory constant.\n\n\n\n\n\n\n\nReverse Adjoint Method\n\n\n\nConstant memory regardless of number of steps\nRequires a second simulation pass for gradient computation\nIn a 10-step 1024³ Lightcone simulation, reverse adjoint uses 5× less memory than checkpointing (∼100 GB vs ∼500 GB)"
  },
  {
    "objectID": "2025_05_BDL/index.html#jaxpm-v0.1.5-differentiable-scalable-simulations",
    "href": "2025_05_BDL/index.html#jaxpm-v0.1.5-differentiable-scalable-simulations",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "JAXPM v0.1.5: Differentiable, Scalable Simulations ",
    "text": "JAXPM v0.1.5: Differentiable, Scalable Simulations \n\n  \n\n\nWhat JAXPM v0.1.5 Supports\n\n\n\nMulti-GPU and Multi-Node simulation with distributed domain decomposition (Successfully ran 2048³ on 256 GPUs)\nEnd-to-end differentiability, including force computation and interpolation\nCompatible with a custom JAX compatible Reverse Adjoint solver for memory-efficient gradients\nSupports full PM Lightcone Weak Lensing\nAvailable on PyPI: pip install jaxpm\nBuilt on top of jaxdecomp for distributed 3D FFT"
  },
  {
    "objectID": "2025_05_BDL/index.html#current-capabilities-road-ahead",
    "href": "2025_05_BDL/index.html#current-capabilities-road-ahead",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Current Capabilities & Road Ahead",
    "text": "Current Capabilities & Road Ahead\n\n\nWhat We’ve Achieved So Far\n\n\n\nBuilt a scalable, differentiable N-body simulation pipeline (JAXPM)\nEnables forward modeling and sampling in large cosmological volumes, paving the way toward full LSST-scale inference\nPreliminary performance:\n\n~20 s per 512×512×1024 simulation on 64×A100 GPUs\n&lt;1 TB memory for full 10-step lightcone\nStable gradients over 100-sample tests\n\n\n\n\n\n\n\nFor discussion\n\n\n\nUsing Scattering Transform to compress the data with SBI\nUsing Excplicit Inference for CMB r estimation"
  },
  {
    "objectID": "2025_05_BDL/index.html#approximating-the-small-scales",
    "href": "2025_05_BDL/index.html#approximating-the-small-scales",
    "title": "JAXPM: A JAX-Based Framework for Scalable and Differentiable Particle Mesh Simulations",
    "section": "Approximating the Small Scales",
    "text": "Approximating the Small Scales\n\n\n \nDynamic resultion grid\n\n We can use a dynamic resolution that automatically refines the grid to match the density regaions blabla\n  very difficult to differentialte and slow to compute\n\n\n\n\n\nDynamic Resolution Grid\n\n\n\n\n\nMultigrid Methods\n\n Multigrid solves the Poisson equation efficiently by combining coarse and fine grids\n It’s still an approximation — it does not match the accuracy of solving on a uniformly fine grid\n At high fidelity, fine-grid solvers outperform multigrid in recovering small-scale structure — critical for unbiased inference\n\n\n\n\n\n\nMultigrid"
  },
  {
    "objectID": "2025_05_BDL_School/index.html#outline-for-this-presentation",
    "href": "2025_05_BDL_School/index.html#outline-for-this-presentation",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Outline for This Presentation",
    "text": "Outline for This Presentation\n\n\n\nUnderstand Cosmological Inference: Learn how we go from observations to cosmological parameters. \nFrom χ² to Bayesian Inference: See how Bayesian modeling generalizes classical approaches. \nLearn Forward Modeling and Hierarchical Models: Understand generative models and field-level inference. \nExplore Modern Tools (JAX, NumPyro, BlackJAX): Use practical libraries for scalable inference. \nPrepare for Hands-On Notebooks: Apply Bayesian techniques in real examples using JAX.\n\n\n\n\n“Let me walk you through what we’re aiming to cover today.”\n\nFirst, we’ll build an understanding of cosmological inference — how we move from raw observational data to constraints on cosmological parameters. This includes both the intuition and the mathematical machinery behind it.\nSecond, we’ll see how Bayesian inference generalizes the classical approach. Instead of just optimizing a χ², we model uncertainty and latent structure more fully.\nThird, we’ll dive into forward modeling and hierarchical models. These are especially relevant in modern cosmology where we simulate the full data generation process and marginalize over latent variables.\nThen we’ll explore some of the modern tools that make all of this practical: JAX for fast, differentiable computing; NumPyro for probabilistic programming; and BlackJAX for flexible sampling.\nFinally, the goal is for you to leave prepared for the hands-on notebooks, where you’ll implement and explore real inference pipelines using JAX-based tools.\n\nThe goal of this is to able to start doing bayesian inference .. so if you have any questions, please ask them during the presentation."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#inference-in-cosmology-the-frequentist-pipeline",
    "href": "2025_05_BDL_School/index.html#inference-in-cosmology-the-frequentist-pipeline",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Inference in Cosmology: The Frequentist Pipeline",
    "text": "Inference in Cosmology: The Frequentist Pipeline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncosmological parameters (Ω): matter density, dark energy, etc.\n\n\n\n\nPredict observables: CMB, galaxies, lensing\n\n\n\n\nExtract summary statistics: \\(P(k)\\), \\(C_\\ell\\) , 2PCF\n\n\n\n\nCompute likelihood: \\(L(\\Omega \\vert data)\\)\n\n\n\n\nEstimate \\(\\hat{\\Omega}\\) via maximization (\\(\\chi^2\\) fitting)\n\n\n\n\n\nFrequentist Toolbox\n\n\n\nOptimizers/Gradient descent\n2-point correlation function (2PCF)\nPower spectrum fitting: \\(P(k)\\), \\(C_\\ell\\)\n\n\n\n\n\n\n“This is the traditional approach many of you are already familiar with — the frequentist pipeline.”\n\nWe start with a set of cosmological parameters, denoted here as Ω — this could include things like the matter density, dark energy equation of state, etc.\nThese parameters are used to predict observable quantities, such as the CMB power spectrum, galaxy clustering, or weak lensing shear.\nFrom the data, we extract summary statistics — typically things like 2-point correlation functions, power spectra (P(k), C_ℓ), or other reduced forms of the data.\nThen, we compute a likelihood — usually assuming a Gaussian form for the summary statistics — and estimate parameters by maximizing this likelihood. That gives us a point estimate Ω̂.\nThis pipeline works well when:\n\nYou have a reliable analytic likelihood,\nThe summary statistics are informative,\nAnd the assumptions (e.g. Gaussianity, linear regime) hold.\n\nThe box in the lower right shows what’s typically in the frequentist toolbox — χ² fitting, 2PCF, and so on.\n\n“This pipeline is efficient and interpretable — but as we’ll see, it has limitations when you move beyond simple, low-dimensional, or linear models.”"
  },
  {
    "objectID": "2025_05_BDL_School/index.html#inference-in-cosmology-the-bayesian-pipeline",
    "href": "2025_05_BDL_School/index.html#inference-in-cosmology-the-bayesian-pipeline",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Inference in Cosmology: The Bayesian Pipeline",
    "text": "Inference in Cosmology: The Bayesian Pipeline\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart from summary statistics: \\(P(k)\\), \\(C_\\ell\\) , 2PCF\n\n\n\n\nSample from a Prior \\(P(\\Omega)\\)\n\n\n\n\nCompute likelihood: \\(L(Obs \\vert \\Omega)\\)\n\n\n\n\nSampler from the Posterior \\(P(\\Omega \\vert Obs)\\)\n\n\n\n\n\nBayesian Toolbox\n\n\n\nPriors encode beliefs: \\(P(\\Omega)\\)\n\nHierarchical Bayesian Modeling (HBM)\nProbabilistic programming (e.g., NumPyro)\nGradient-based samplers: HMC, NUTS\n\n\n\n\n\n\nStart by emphasizing that, like the frequentist pipeline, the Bayesian version also begins with summary statistics like power spectra. But instead of finding a best-fit point estimate, the Bayesian approach defines a probabilistic model over cosmological parameters.\nNow show image 2:\n\nWe sample from a prior over parameters, combine it with a likelihood based on the observed data, and use this to compute the posterior.\nThis posterior captures all uncertainty, correlations, and degeneracies.\nThe Bayesian toolbox lets us extend models easily — hierarchical structure, latent fields, flexible priors — and perform inference using modern tools like NumPyro and gradient-based samplers (e.g. NUTS, HMC)."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#inference-in-cosmology-the-bayesian-pipeline-1",
    "href": "2025_05_BDL_School/index.html#inference-in-cosmology-the-bayesian-pipeline-1",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Inference in Cosmology: The Bayesian Pipeline",
    "text": "Inference in Cosmology: The Bayesian Pipeline\n\n\n\n\n\n\n\n\n\n\n\nPrior: Theory-driven assumptions \\(P(\\Omega)\\)\n\n\n\n\nLatent variables: Hidden/unobserved \\(z \\sim P(z \\mid \\Omega)\\)\n\n\n\n\nLikelihood: Generates observables \\(P(\\text{Obs} \\mid \\Omega, z)\\)\n\n\n\n\nPosterior: infer \\(P(\\Omega \\mid \\text{Obs})\\)"
  },
  {
    "objectID": "2025_05_BDL_School/index.html#inference-in-cosmology-the-bayesian-pipeline-2",
    "href": "2025_05_BDL_School/index.html#inference-in-cosmology-the-bayesian-pipeline-2",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Inference in Cosmology: The Bayesian Pipeline",
    "text": "Inference in Cosmology: The Bayesian Pipeline\n\n\n\n\n\n\n\n\n\nBayes’ Rule with all components:\n\nFull decomposition of the posterior. The denominator marginalizes over all possible parameters.\n\n\n\n\\[\n\\underbrace{P(\\Omega \\mid \\text{Obs})}_{\\text{Posterior}}\n= \\frac{\n    \\underbrace{P(\\text{Obs} \\mid \\Omega)}_{\\text{Likelihood}}\n    \\cdot\n    \\underbrace{P(\\Omega)}_{\\text{Prior}}\n}{\n    \\underbrace{\n        \\int P(\\text{Obs} \\mid \\Omega) P(\\Omega) \\, d\\Omega\n    }_{\\text{Evidence}}\n}\n\\]\n\n\n\\[\n\\underbrace{P(\\Omega \\mid \\text{Obs})}_{\\text{Posterior}}\n= \\frac{\n    \\underbrace{\\int P(\\text{Obs} \\mid \\Omega, z)\\, P(z \\mid \\Omega)\\, dz}_{\\text{Likelihood (marginalized over latent $z$)}}\n    \\cdot\n    \\underbrace{P(\\Omega)}_{\\text{Prior}}\n}{\n    \\underbrace{\n        \\int \\left[ \\int P(\\text{Obs} \\mid \\Omega, z)\\, P(z \\mid \\Omega)\\, dz \\right] P(\\Omega)\\, d\\Omega\n    }_{\\text{Evidence}}\n}\n\\]\n\n\n\n\n\n\n\nIn practice, we drop the evidence term when sampling — it’s a constant.\n\n\\[\nP(\\Omega \\mid \\text{Obs})\n\\propto\n\\underbrace{\\int P(\\text{Obs} \\mid \\Omega, z)\\, P(z \\mid \\Omega) \\, dz}_{\\text{Marginal Likelihood}}\n\\cdot\n\\underbrace{P(\\Omega)}_{\\text{Prior}}\n\\]\n\n\n\\[\n\\log P(\\Omega \\mid \\text{Obs})\n= \\log P(\\text{Obs} \\mid \\Omega) + \\log P(\\Omega)\n\\]\n\n\n\n\n\nBayes’ Rule in Practice\n\n\n\nThe posterior combines theory (prior) and data (likelihood) to infer cosmological parameters.\nLatent variables \\(z\\) encode hidden structure (e.g., initial fields, nuisance parameters).\nThe evidence is often ignored during sampling (it’s constant).\nModel comparison via the Bayes Factor:\n\\[\n\\text{Bayes Factor} = \\frac{P(\\text{Obs} \\mid \\mathcal{M}_1)}{P(\\text{Obs} \\mid \\mathcal{M}_2)}\n\\]\n\n\n\n\n\n\nSpeaker Notes for This Sequence:\n\nSTEP 1:\nWe now extend the Bayesian pipeline by introducing latent variables — denoted z. These are hidden, unobserved quantities such as initial conditions, noise fields, or instrumental effects.\n\nThe prior encodes our belief over cosmological parameters Ω. It’s unobserved, unknown, and is the target of inference.\nThe latent variables z are also unobserved and unknown, but they are conditional on the prior — they depend on Ω and are integrated out (marginalized) during inference.\nThe likelihood is a forward model that generates observables given both the prior and latent structure: \\(\\mathcal{L}(\\text{Obs} \\mid \\Omega, z)\\)\nThe posterior combines all of these: it tells us how probable different cosmological parameters are, given the data and the model structure: \\(P(\\Omega \\mid \\text{Obs}) \\propto \\int \\mathcal{L}(\\text{Obs} \\mid \\Omega, z) \\, P(z \\mid \\Omega) \\, dz \\cdot P(\\Omega)\\)\n\nThis hierarchical view is what powers modern cosmological inference.\nSTEP 2:\nHere’s a polished version of your speaker notes for that section:\n\nWe now move to the full Bayesian formula — starting without latent variables:\nThis gives us the posterior as the product of the likelihood and the prior, normalized by the evidence. The evidence \\(P(\\text{Obs})\\) ensures the posterior is a proper probability distribution.\nNow, when we introduce latent variables \\(z\\), the likelihood itself becomes an integral over those:\nThis marginal likelihood accounts for the full hidden structure.\nIn practice, we ignore the evidence term when sampling:\n\nIt’s constant for a given model — so it doesn’t affect posterior shape.\nIt’s computationally expensive to compute (requires full integration).\nBut: it’s very useful for comparing models.\n\n\nSummary:\n\nWe sample directly from the posterior, which combines prior and likelihood.\nLatent variables model hidden or uncertain structure — like initial conditions.\nThe evidence is dropped during sampling, but becomes important in model comparison using the Bayes Factor:\n\nThis tells us which model is better supported by the data."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#two-roads-to-inference-frequentist-and-bayesian",
    "href": "2025_05_BDL_School/index.html#two-roads-to-inference-frequentist-and-bayesian",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Two Roads to Inference: Frequentist and Bayesian",
    "text": "Two Roads to Inference: Frequentist and Bayesian\n\n\n\n\nConceptual Differences\n\n\n\n\n\n\n\n\n\n\nConcept\nFrequentist\nBayesian\n\n\n\n\nParameters\nFixed but unknown\nRandom variables with a prior\n\n\nGoal\nPoint estimate (e.g. MLE)\nFull distribution (posterior over parameters)\n\n\nUncertainty\nFrom data variability\nFrom parameter uncertainty (posterior)\n\n\nPrior Knowledge\nNot used\nExplicitly included via prior \\(P(\\Omega)\\)\n\n\nInterval Meaning\nConfidence interval: “95% of experiments contain truth”\nCredible interval: “95% chance truth is in this range”\n\n\nLikelihood Role\nCentral in \\(\\chi^2\\) minimization, fits\nCombined with prior to form posterior\n\n\nInference Output\nBest-fit estimate + error bars\nPosterior distribution\n\n\nTooling\nOptimization (e.g. χ², maximum likelihood)\nSampling (e.g. MCMC, HMC, NUTS)\n\n\n\n\n\n\n\nAlthough these approaches are often contrasted, they’re not mutually exclusive. Modern workflows — like causal inference in Statistical Rethinking — draw on both perspectives. Bayesian methods offer a formal way to combine theory and data, especially powerful when simulations are involved.\n\n\n\n\nStatistical Rethinking\n\n\n\n\n\nThis slide lays out a direct comparison between the frequentist and Bayesian approaches.\nWe’re highlighting not just the philosophical differences, but also the practical consequences.\nA few key contrasts to emphasize:\n\nParameters: In frequentist stats, parameters are fixed but unknown. In Bayesian stats, they’re random variables — we assign distributions to represent our uncertainty.\nGoal: Frequentists usually aim for a point estimate (like the MLE). Bayesians aim to recover the entire posterior distribution.\nUncertainty: Frequentists focus on data variability — uncertainty from random samples. Bayesians focus on parameter uncertainty — how uncertain we are about the parameters given the data.\nIntervals: The interpretations are totally different. A frequentist says, “95% of the time, this interval contains the truth.” A Bayesian says, “There’s a 95% chance the truth is in this interval.”\nTooling: Optimization vs. sampling. Frequentists often rely on curve fitting, minimization, etc. Bayesian workflows rely on sampling (MCMC, HMC, NUTS).\n\nMake sure the audience sees this isn’t about choosing sides. As the bottom note says — they’re not mutually exclusive. A lot of modern workflows combine both perspectives.\n\n\n@image credit:Wayne Stewart"
  },
  {
    "objectID": "2025_05_BDL_School/index.html#sampling-the-posterior-the-core-loop",
    "href": "2025_05_BDL_School/index.html#sampling-the-posterior-the-core-loop",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Sampling the Posterior: The Core Loop",
    "text": "Sampling the Posterior: The Core Loop\n\n\n\n\n\n\n\n\nThe Sampling Loop:\n\n\nStart from a sample \\((\\Omega^t, z^t)\\)\n\n\n\n\n\nPropose new sample \\((\\Omega', z')\\)\n\n\n\n\nCompute acceptance probability\n\n\n\n\nAccept or reject proposal\n\n\n\n\nRepeat and store accepted samples ⟶ posterior\n\n\n\nGoal: Explore the full shape of the posterior\n(even in high-dim, non-Gaussian spaces)\n\n\n\n\nKey Takeaways\n\n\n\nMost samplers follow this accept/reject loop\nDiffer by how they propose samples: – Random walk (e.g., MH) – Gradient-guided (e.g., HMC, NUTS)\nSome skip rejection (e.g., Langevin, VI)\n\n\n\n\n\n\nThis slide illustrates the core mechanism behind most MCMC samplers — the accept/reject loop.\nWe start with a current sample from the posterior, say \\((\\Omega^t, z^t)\\). The sampler then proposes a new point \\((\\Omega', z')\\), using some rule — it might be a random walk, or it might use gradients like in HMC or NUTS.\nNext, we compute the acceptance probability — this depends on how likely the new sample is under the posterior compared to the current one.\nThen we make a decision:\n\nIf the new sample is more likely (or meets some acceptance threshold), we accept it and add it to the chain.\nIf not, we reject it and store the current one again.\n\nThis process repeats to build a chain of samples. The accepted ones collectively approximate the posterior distribution.\nOn the right, the key takeaways:\n\nMost samplers use this loop.\nThe difference lies in how they generate proposals — basic methods use random walks, while advanced methods use gradients.\nSome newer algorithms avoid rejection altogether — like variational inference or some Langevin-based flows — but the classic accept/reject structure remains fundamental to many MCMC approaches."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#gradient-based-sampling-in-action",
    "href": "2025_05_BDL_School/index.html#gradient-based-sampling-in-action",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Gradient-Based Sampling in Action",
    "text": "Gradient-Based Sampling in Action\n\n\n\n\n\n\n\nHMC: Gaussian Posterior\n\n\n\n\n\n\n\nHMC: Gaussian Posterior\n\n\n\n\n\n\n\n\n\nHMC: Banana Posterior\n\n\n\n\n\n\n\nHMC: Banana Posterior\n\n\n\n\n\n\n\n\n\n\nMCMC: Gaussian Posterior\n\n\n\n\n\n\n\nMCMC: Gaussian Posterior\n\n\n\n\n\n\n\n\n\nMCMC: Banana Posterior\n\n\n\n\n\n\n\nMCMC: Banana Posterior"
  },
  {
    "objectID": "2025_05_BDL_School/index.html#gradient-based-sampling-in-action-1",
    "href": "2025_05_BDL_School/index.html#gradient-based-sampling-in-action-1",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Gradient-Based Sampling in Action",
    "text": "Gradient-Based Sampling in Action\n\n\n\n\n\nHMC: Gaussian Posterior\n\n\n\n\n\n\nHMC: Banana Posterior\n\n\n\n\n\n\nMCMC: Gaussian Posterior\n\n\n\n\n\n\nMCMC: Banana Posterior\n\n\n\n\nIn high dimensions, random walk proposals (MCMC) often land in low-probability regions ⟶ low acceptance.\nTo maintain acceptance, step size must shrink like \\(1/\\sqrt{d}\\) ⟶ very slow exploration.\nHMC uses gradients to follow high-probability paths ⟶ better samples, fewer steps.\n\n\n\n\n\n\n\nSampling Without Gradients\n\n\n\n\n\n\n\nSampling With Gradients\n\n\n\n\n\nIn this slide we compare HMC and traditional MCMC in two scenarios:\n\nTop row: Gaussian posterior\n\nHMC (left) aligns well with the true density contours — samples are well spread.\nMCMC (right) struggles a bit — it’s noisy, slightly distorted, and shows correlated samples.\nThat’s because MCMC does a random walk, which is inefficient even in simple geometries.\n\n\nBottom row: Banana-shaped posterior\n\nThis is a nonlinear, curved posterior — a much harder target.\nHMC (left) still tracks the true shape well using its gradient information.\nMCMC (right) again struggles: it oversamples in wrong regions and can’t explore the full space.\n\n\nKey Point:\nHMC shines when the geometry is tricky. Its gradients guide proposals along the posterior, unlike MCMC’s aimless wandering.\nThis motivates why we use HMC or NUTS in high-dimensional, curved, or strongly correlated problems — like cosmology.\nSlide: Sampling Without Gradients\n\nThis shows how traditional MCMC struggles when sampling from complex distributions.\nHere, proposals are based on random walks, which means they can easily jump into low-probability regions.\nAs a result, many proposals are rejected — leading to inefficient sampling.\nTo maintain high acceptance, samplers reduce step size — but this slows down exploration dramatically, especially in high dimensions.\nYou can see the samples (blue dots) under-sample the second peak and have poor coverage overall.\n\n\nSlide: Sampling With Gradients\n\nNow we add gradient information — this is what HMC uses.\nGradients give the sampler a sense of “direction,” pointing it toward high-probability areas.\nInstead of random jumps, we simulate trajectories that follow the shape of the distribution.\nThis enables much better exploration with fewer steps.\nAs a result, samples land more effectively across both peaks and better represent the target distribution.\nThis illustrates why gradient-based samplers like HMC or NUTS perform so well in high-dimensional, structured problems."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#differentiable-inference-with-jax",
    "href": "2025_05_BDL_School/index.html#differentiable-inference-with-jax",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Differentiable Inference with JAX",
    "text": "Differentiable Inference with JAX\nWhen it comes to gradients, always think of JAX.\n\n\n\nAn Easy pythonic API\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\ndef sample_prior(key):\n    return random.normal(key, shape=(3,))  # Ω ~ N(0, 1)\n\ndef log_prob(omega):\n    return -0.5 * jnp.sum(omega**2)  # log p(Ω) ∝ -Ω²\n\nlog_prob_jit = jax.jit(log_prob)\n\nEasily accessible gradients using GRAD\nomegas = ... # Sampled Ω\ngradients = jax.grad(log_prob_jit)(omegas)\n\n\nSupports vectorization using VMAP\ndef generate_samples(seeds):\n    key = jax.random.PRNGKey(seeds)\n    omega = sample_prior(key)\n    return omega\nseeds = jnp.arange(0, 1000)\nomegas = jax.vmap(generate_samples)(seeds)\n\n\n\n\n\n\n\n\n\n\nThis is why JAX is such a natural fit for inference: it’s fully differentiable and built around gradients.\nOn the left we show three core ideas that power modern inference.\n\nFirst block: JAX gives you a familiar NumPy-like API, with tools like jit to compile and optimize code. You define a prior, a log-prob function, and wrap it in jit — easy and fast.\nSecond block: With jax.grad, you can differentiate any function. That means you get gradients of the log-posterior “for free,” which is exactly what HMC or variational inference need.\nThird block: JAX scales easily — use vmap to vectorize your function across many seeds, particles, or chains. This is a huge win when doing amortized inference or simulation-based methods.\n\nAltogether, JAX provides the gradient plumbing for probabilistic inference — while remaining readable and fast."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#a-recipe-for-bayesian-inference",
    "href": "2025_05_BDL_School/index.html#a-recipe-for-bayesian-inference",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "A Recipe for Bayesian Inference",
    "text": "A Recipe for Bayesian Inference\n\n\n\n1. Probabilistic Programming Language (PPL) NumPyro:\nimport numpyro\nimport numpyro.distributions as dist\n\ndef model():\n    omega_m = numpyro.sample(\"Ωₘ\", dist.Uniform(0.1, 0.5))\n    sigma8 = numpyro.sample(\"σ₈\", dist.Normal(0.8, 0.1))\n\n\n2. Computing Likelihoods JAX-Cosmo:\nimport jax_cosmo as jc\ndef likelihood(cosmo_params):\n    mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(\n        cosmo_params, ell, probes\n    )\n    return jc.likelihood.gaussian_log_likelihood(data, mu, cov)\n\n\n3. Sampling the Posterior NumPyro & Blackjax:\nfrom numpyro.infer import MCMC, NUTS\n\nkernel = NUTS(model)\nmcmc = MCMC(kernel, num_warmup=500, num_samples=1000)\nmcmc.run(random.PRNGKey(0))\nsamples = mcmc.get_samples()\n\n\n4. Visualizing the Posterior ArviZ:\nimport arviz as az\nsamples = mcmc.get_samples()\naz.plot_pair(samples, marginals=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@credit: Zeghal et al. (2409.17975)\n\n\n\n\n\nHere are the speaker notes for the two slides:\n\nSlide 1: “A Recipe for Bayesian Inference”\n\nNow let’s break Bayesian inference into a practical workflow.\nFirst, we define our model using a probabilistic programming language — here, NumPyro. This is where we encode the prior and the structure of the model.\nSecond, we use a tool like JAX-Cosmo to compute the likelihood. This connects cosmological parameters to observable predictions — such as angular power spectra.\nThird, we use MCMC or BlackJAX to sample from the posterior.\n\nThis is where sampling happens, and where gradient-based methods like NUTS come into play.\n\nFinally, we extract posterior samples to analyze uncertainty and parameter correlations.\n\nThe graphic on the right summarizes this logic visually: from prior → likelihood → posterior → sample.\n\nSlide 2: “A Recipe for Bayesian Inference (Full Loop)”\n\nThis slide extends the recipe with the final step: visualization.\nAfter sampling, we can summarize and visualize the posterior with tools like ArviZ.\nThe corner plot on the right shows the joint and marginal distributions — a key diagnostic to assess whether inference worked and how parameters are correlated.\nThe dashed vs. solid lines show different inference strategies — possibly explicit (forward simulations) vs implicit likelihoods.\n\nThe takeaway is that with JAX, NumPyro, and JAX-Cosmo, we have a modular pipeline for Bayesian inference — from model definition to visual diagnostics."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#a-minimal-bayesian-linear-model",
    "href": "2025_05_BDL_School/index.html#a-minimal-bayesian-linear-model",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "A Minimal Bayesian Linear Model",
    "text": "A Minimal Bayesian Linear Model\nDefine a simple linear model:\ntrue_w = 2.0\ntrue_b = -1.0\nnum_points = 100\n\nrng_key = jax.random.PRNGKey(0)\nx_data = jnp.linspace(-3, 3, num_points)\nnoise = jax.random.normal(rng_key, shape=(num_points,)) * 0.3\ny_data = true_w * x_data + true_b + noise\n\ndef linear_regression(x, y=None):\n    w = numpyro.sample(\"w\", dist.Normal(1., 2.))\n    b = numpyro.sample(\"b\", dist.Normal(0., 2.))  # Fixed the second parameter\n    sigma = numpyro.sample(\"sigma\", dist.Exponential(1.0))\n\n    mean = w * x + b\n    numpyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n\nRun the model using NUTS:\nkernel = numpyro.infer.NUTS(linear_regression)\nmcmc = numpyro.infer.MCMC(kernel, num_warmup=500, num_samples=1000)\nmcmc.run(rng_key, x=x_data, y=y_data)\n\n\nPosterior corner plot using arviz + corner\nidata = az.from_numpyro(mcmc)\nposterior_array = az.extract(idata, var_names=[\"w\", \"b\", \"sigma\"]).to_array().values.T\n\nfig = corner.corner(\n    posterior_array,\n    labels=[\"w\", \"b\", \"σ\"],\n    truths=[true_w, true_b, None],\n    show_titles=True\n)\nplt.show()\n\n\nThis slide walks through the first notebook exercise — a minimal Bayesian linear regression model using NumPyro.\n\nFirst block defines synthetic data: we’re sampling noisy y values from a true line y = 2x - 1.\nThe linear_regression function defines a probabilistic model: priors on slope w, intercept b, and noise sigma.\nWe use NUTS from NumPyro to sample from the posterior — no likelihood math needed manually.\nFinally, the bottom cell visualizes the posterior using corner.py and ArviZ. You’ll see how the true parameters compare to the inferred distributions.\n\nThis is the foundation — you’ll implement and modify this directly during the hands-on."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#using-blackjax-and-numpyro",
    "href": "2025_05_BDL_School/index.html#using-blackjax-and-numpyro",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Using BlackJax and NumPyro",
    "text": "Using BlackJax and NumPyro\n\nBlackJax is NOT a PPL, so you need to combine it with a PPL like NumPyro or PyMC.\n\n\nInitialize model and extract the log-probability function\nrng_key, init_key = jax.random.split(rng_key)\ninit_params, potential_fn, *_ = initialize_model(\n    init_key, model, model_args=(x_data,), model_kwargs={\"y\": y_data}, dynamic_args=True\n)\n\nlogdensity_fn = lambda position: -potential_fn(x_data, y=y_data)(position)\ninitial_position = init_params.z\n\n\nRun warm-up to adapt step size and mass matrix using BlackJAX’s window adaptation\nnum_warmup = 2000\nadapt = blackjax.window_adaptation(blackjax.nuts, logdensity_fn, target_acceptance_rate=0.8)\nrng_key, warmup_key = jax.random.split(rng_key)\n(last_state, parameters), _ = adapt.run(warmup_key, initial_position, num_warmup)\nkernel = blackjax.nuts(logdensity_fn, **parameters).step\n\n\nRun BlackJAX NUTS sampling using lax.scan\ndef run_blackjax_sampling(rng_key, state, kernel, num_samples=1000):\n    def one_step(state, key):\n        state, info = kernel(key, state)\n        return state, state\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, samples = jax.lax.scan(one_step, state, keys)\n    return samples\n\nsamples = run_blackjax_sampling(rng_key, last_state, kernel)\n\n\nConvert BlackJAX output to ArviZ InferenceData\nidata = az.from_dict(posterior=samples.position)\n\n\nThis slide shows how to use BlackJAX for sampling when you already have a model defined in NumPyro.\n\nFirst, we extract the log-probability function from a NumPyro model using initialize_model. This lets us use BlackJAX with the same model.\nWe define a logdensity_fn, which BlackJAX expects — it just wraps the potential function with a negative sign.\nNext, we run adaptive warm-up with blackjax.window_adaptation. This tunes step size and mass matrix like NumPyro does.\nThen, we sample using blackjax.nuts with a loop written using jax.lax.scan for speed and JIT compatibility.\nFinally, we convert the raw samples to an ArviZ-friendly format with az.from_dict.\n\nKey point: BlackJAX is super fast and modular, but not a full PPL — so you bring your own model definition."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#sampler-comparison-table",
    "href": "2025_05_BDL_School/index.html#sampler-comparison-table",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Sampler Comparison Table",
    "text": "Sampler Comparison Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampler\nLibrary\nUses Gradient\nAuto-Tuning\nRejection\nBest For\nNotes\n\n\n\n\nMCMC (SA)\nNumPyro\n❌\n❌\n✅\nSimple low-dim models\nNo gradients; slow mixing\n\n\nHMC\nNumPyro / BlackJAX\n✅\n❌\n✅\nHigh-dim continuous posteriors\nNeeds tuned step size & trajectory\n\n\nNUTS\nNumPyro / BlackJAX\n✅\n✅\n✅\nGeneral-purpose inference\nAdaptive HMC\n\n\nMALA\nBlackJAX\n✅\n❌\n✅\nLocal proposals w/ gradients\nStochastic gradient steps\n\n\nMCLMC\nBlackJAX\n✅\n✅ (via L)\n❌\nLarge latent spaces\nUnadjusted Langevin dynamics\n\n\nAdj. MCLMC\nBlackJAX\n✅\nManual (L)\n✅\nBias-controlled Langevin sampler\nIncludes MH step\n\n\n\n\n\nFor more information check Simons et al. (2025), §2.2.3, arXiv:2504.20130\n\nSpeaker Notes – Sampler Comparison Table\nThis table gives a high-level overview of common samplers available in NumPyro and BlackJAX, organized by key features.\n\nMCMC (SA): This is standard Metropolis-Hastings — no gradients, no tuning, but it’s simple and useful for low-dimensional problems. Slow mixing is a drawback.\nHMC: Hamiltonian Monte Carlo improves mixing by using gradients to simulate physics-based trajectories. It requires tuning for step size and path length, so it’s more manual.\nNUTS: No-U-Turn Sampler is HMC with built-in auto-tuning. It adapts step size and path length during warm-up, making it the default choice for general-purpose inference in NumPyro and BlackJAX.\nMALA: Metropolis-Adjusted Langevin Algorithm uses gradients for local proposals. It’s a good middle ground — cheaper than full HMC, but more efficient than MH.\nMCLMC: Stochastic Langevin dynamics with no rejection step. Useful in large latent spaces, often in simulator-based models, but biased due to lack of correction.\nAdjusted MCLMC: Same as MCLMC, but now with a Metropolis-Hastings correction step. This helps reduce bias, at the cost of a little more computation.\n\nMain idea: Pick the right sampler based on:\n\nwhether you can compute gradients\ndimensionality of the posterior\nwhether you need auto-tuning or not\nwhether you want unbiased samples (with rejection) or not.\n\nFinal note: This is a simplified summary. For real applications, benchmarking is always best."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#power-spectrum-inference-with-jax-cosmo",
    "href": "2025_05_BDL_School/index.html#power-spectrum-inference-with-jax-cosmo",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Power Spectrum Inference with jax-cosmo",
    "text": "Power Spectrum Inference with jax-cosmo\n\nStep 1: Simulate Cosmological Data\nDefine a fiducial cosmology to generate synthetic observations\nfiducial_cosmo = jc.Planck15()\nell = jnp.logspace(1, 3)  # Multipole range for power spectrum\nSet up two redshift bins for galaxy populations\nnz1 = jc.redshift.smail_nz(1., 2., 1.)\nnz2 = jc.redshift.smail_nz(1., 2., 0.5)\nnzs = [nz1, nz2]\nDefine observational probes: weak lensing and number counts\nprobes = [\n    jc.probes.WeakLensing(nzs, sigma_e=0.26),\n    jc.probes.NumberCounts(nzs, jc.bias.constant_linear_bias(1.))\n]\nGenerate synthetic data using the fiducial cosmology\nmu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(fiducial_cosmo, ell, probes)\nrng_key = jax.random.PRNGKey(0)\nnoise = jax.random.multivariate_normal(rng_key, jnp.zeros_like(mu), cov)\ndata = mu + noise  # Fake observations\n\n\nStep 2: Define the NumPyro Model\n# Define a NumPyro probabilistic model to infer cosmological parameters\ndef model(data):\n    Omega_c = numpyro.sample(\"Omega_c\", dist.Uniform(0.1, 0.5))\n    sigma8 = numpyro.sample(\"sigma8\", dist.Uniform(0.6, 1.0))\n    \n    # Forward model: compute theoretical prediction given parameters\n    cosmo = jc.Planck15(Omega_c=Omega_c, sigma8=sigma8)\n    mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(cosmo, ell, probes)\n    \n    # Likelihood: multivariate Gaussian over angular power spectra\n    numpyro.sample(\"obs\", dist.MultivariateNormal(mu, cov), obs=data)\n\n\nThis slide introduces a basic end-to-end inference workflow using jax-cosmo and NumPyro.\nStep 1: Simulate synthetic cosmological data\n\nWe start with a fiducial cosmology using jc.Planck15().\nDefine a multipole range (ell) and two redshift distributions for tomographic bins.\nCreate weak lensing and number count probes.\nThen, using the angular power spectrum mean and covariance from jax_cosmo, we generate mock observations by adding Gaussian noise.\n\nStep 2: Define the NumPyro model\n\nThe model samples two parameters: Omega_c and sigma8.\nA new cosmology object is constructed with those parameters.\nThen the power spectrum prediction is computed and matched to data with a multivariate Gaussian likelihood.\n\nThis setup mirrors what we do in practice: forward-model observables, simulate noisy data, and recover parameters via inference."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#full-field-inference-with-forward-models",
    "href": "2025_05_BDL_School/index.html#full-field-inference-with-forward-models",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Full Field Inference with Forward Models",
    "text": "Full Field Inference with Forward Models\n\n\n\n\nBayesian Inference using power spectrum data:\n\n\nBayesian Inference using full field data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecap: Bayesian inference maps theory + data → posterior\n\n\n\n\nCosmological Forward models\n\nStart from cosmological + latent parameters\nSample initial conditions\nEvolve using N-body simulations\nPredict convergence maps in tomographic bins\n\n\n\n\n\nSimulation-Based Inference\n\nCompare predictions to real survey maps\nBuild a likelihood from the forward model\nInfer cosmological parameters from full field data\n\n\n\n\n\n\nFull Field vs. Summary Statistics\n\n\n\nPreserves non-Gaussian structure lost in summaries\nEnables tighter constraints in nonlinear regimes\nEspecially useful in high-dimensional inference problems\nSee: Zeghal et al. (2024), Leclercq et al. (2021)\n🔜 a talk on this topic this Thursday\n\n\n\n\n\n\nSpeaker Notes for Final Slide (Full Field Inference):\n\nWe now shift to the most general and flexible form of inference — using full field data.\nThis means we don’t extract summary statistics like \\(C_\\ell\\) — instead, we model the forward process end-to-end, including simulations.\nThe green box shows the core components of this forward model pipeline:\n\nStart by sampling cosmological + latent parameters (e.g., initial conditions)\nUse an N-body simulation to evolve structure\nPredict observables (e.g., weak lensing convergence maps)\nCompare the simulated maps to real observations to construct a likelihood\n\nThis approach preserves non-Gaussian information, which is critical in the nonlinear regime.\nWe’ll focus specifically on the contents of the green box in the hands-on notebook:\n\nSampling initial fields\nRunning a small-scale N-body simulation\nBuilding a likelihood from simulation output\n\nIt’s an extremely powerful method, and the frontier of cosmological inference."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#conclusion-why-bayesian-inference",
    "href": "2025_05_BDL_School/index.html#conclusion-why-bayesian-inference",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Conclusion: Why Bayesian Inference?",
    "text": "Conclusion: Why Bayesian Inference?\n  \nKey Takeaways\n\nBayesian modeling enables flexible, end-to-end inference pipelines — from analytical likelihoods to full forward simulations.\nThe JAX ecosystem (NumPyro, BlackJAX, jax-cosmo…) lets you focus on modeling, not low-level math.\nGradients + differentiable simulators make inference scalable — even in complex, high-dimensional models.\nThese tools are now mature, fast, and usable — and already applied to realistic cosmological settings.\n\nFuture Work\n\nDistributed, differentiable N-body simulations enable full-field inference at survey scale.\nWe look forward to applying these models to real survey data in upcoming projects.\n\n \nThank you for your attention!\n\nSpeaker Notes – Conclusion Slide\n\nLet’s wrap up with some key takeaways on why Bayesian inference matters, especially in cosmology:\n\nFirst, Bayesian modeling is modular and flexible. It gives us a way to go from simple analytic models to full simulator-based pipelines — all under one framework.\nThe JAX ecosystem — NumPyro, BlackJAX, jax-cosmo — gives you a complete toolchain. You don’t have to worry about math-heavy derivations or custom samplers. You can focus on the science and modeling.\nA major strength is the ability to leverage gradients and differentiable simulators. That means you can scale inference even for models with thousands or millions of parameters — like field-level inference.\nAnd importantly, the tools are mature and fast. These aren’t just research toys — they’re ready for real problems.\n\nNow it’s your turn. You’ll have access to two hands-on notebooks:\n\nOne walks you through Bayesian regression and cosmological inference with NumPyro.\nThe other dives into field-level inference and simulation-based modeling.\n\nThis is where things become tangible — you’ll code your own inference pipeline, simulate structure formation, and run real MCMC samplers.\nThat’s the power of combining Bayesian ideas with modern tools — and that’s where the future of cosmological inference is headed."
  },
  {
    "objectID": "2025_05_BDL_School/index.html#hands-on-notebooks",
    "href": "2025_05_BDL_School/index.html#hands-on-notebooks",
    "title": "Bayesian Inference for Cosmology with JAX",
    "section": "Hands on notebooks",
    "text": "Hands on notebooks\n \n \n\n\nHands-On Notebooks:\n\n\n\nBeginner Bayesian Inference with NumPyro & Blackjax here\nIntermediate Bayesian Inference with NumPyro & Blackjax here\nsome of the animation were made using this notebook"
  }
]